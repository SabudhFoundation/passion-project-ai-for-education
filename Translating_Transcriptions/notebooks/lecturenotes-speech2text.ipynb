{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9909250,"sourceType":"datasetVersion","datasetId":6088414},{"sourceId":9909267,"sourceType":"datasetVersion","datasetId":6088426}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:28:21.204430Z","iopub.execute_input":"2024-11-15T04:28:21.204920Z","iopub.status.idle":"2024-11-15T04:28:21.219856Z","shell.execute_reply.started":"2024-11-15T04:28:21.204880Z","shell.execute_reply":"2024-11-15T04:28:21.218999Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trial-song/09. Fuel.mp3\n/kaggle/input/nlp-lecture-2-sabudh/educollab.mp3\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install requests beautifulsoup4 pydub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:04:49.819454Z","iopub.execute_input":"2024-11-15T04:04:49.819949Z","iopub.status.idle":"2024-11-15T04:05:02.177957Z","shell.execute_reply.started":"2024-11-15T04:04:49.819910Z","shell.execute_reply":"2024-11-15T04:05:02.176812Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.3)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.8.30)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U openai-whisper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:05:02.179967Z","iopub.execute_input":"2024-11-15T04:05:02.180269Z","iopub.status.idle":"2024-11-15T04:05:33.924461Z","shell.execute_reply.started":"2024-11-15T04:05:02.180237Z","shell.execute_reply":"2024-11-15T04:05:33.923494Z"}},"outputs":[{"name":"stdout","text":"Collecting openai-whisper\n  Downloading openai-whisper-20240930.tar.gz (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (4.66.4)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (10.3.0)\nCollecting tiktoken (from openai-whisper)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting triton>=2.0.0 (from openai-whisper)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton>=2.0.0->openai-whisper) (3.15.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2.32.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (2024.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->openai-whisper) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->openai-whisper) (1.3.0)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=6227b5cf30e0f25521538430ed8e4ac48c0d5ecbfa5625b84883a676c96b0d6a\n  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\nSuccessfully built openai-whisper\nInstalling collected packages: triton, tiktoken, openai-whisper\nSuccessfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import requests\nfrom pydub import AudioSegment\n\ndef download_and_convert_to_audio(url):\n    # Download the video file\n    video_response = requests.get(url, stream=True)\n    if video_response.status_code == 200:\n        video_path = \"downloaded_video.mp4\"\n        with open(video_path, \"wb\") as file:\n            file.write(video_response.content)\n        print(\"Video downloaded successfully as 'downloaded_video.mp4'\")\n    else:\n        print(\"Failed to download video\")\n        print(video_response.status_code)\n        return\n\n    # Convert the downloaded video to audio\n    try:\n        audio = AudioSegment.from_file(video_path, format=\"mp4\")\n        output_audio_path = \"output_audio.mp3\"\n        audio.export(output_audio_path, format=\"mp3\")\n        print(\"Audio extracted and saved as\", output_audio_path)\n    except Exception as e:\n        print(\"Error during conversion:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:29:30.645535Z","iopub.execute_input":"2024-11-15T04:29:30.646128Z","iopub.status.idle":"2024-11-15T04:29:30.652997Z","shell.execute_reply.started":"2024-11-15T04:29:30.646088Z","shell.execute_reply":"2024-11-15T04:29:30.652081Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"download_and_convert_to_audio(\"https://educollab-data.s3.amazonaws.com/recorded-sessions/1731307787-92159043805.mp4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ6VB3Z4MJAQ4XAGS%2F20241115%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20241115T042201Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCmFwLXNvdXRoLTEiSDBGAiEApf3g21Mv4qQmAu5Ball55AHaku16dvkGBiG7eLN5eFcCIQDMC6KFBTfAferfIxgGVacBR1B6q9OnqSniE2TqmeTuDCr%2FAwj8%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAMaDDA2NTgzNzcxNTIyNCIMPiPwhKcc23tDyQyUKtMDEFAj9DJaDCTzQQslkge2UsdlpXg6AJ7uLIBpHUgSQLZAupALjlubDCeRE8mChjA%2Bp5sGy5Up5NbHbQjjk6slGgbUjMjPPuZmMYB6xzaKodSziE4Fh6o2BSrzqR0dRyRekS3o%2FBpM6zF5SIDFAwEVzP8vUNL80l68rPC3%2Fq3%2FGUiEHe2hfz%2FGzRCB7OoFNBFIq3OF9Gb17G%2BMlkFIExWK8vPDEgLKqN%2FMK7A8rV120yDmkVOqaowar6%2Bfqxhmp%2BLPLY47OIVfskWNRAnhwpEGC0QnNU9SmLZ%2BFPeAuHqortxtcCyxL7gsSp5AdZ0ndquflyxAWYdL2MpT4hP8AL0IwVExHEwKK%2B6wne7h3UFMNdis6SQ8iRf5h4cOegbxQF4jcPnt7zEtECy0rLm%2FFwzDnPWEhsyc1OccATBqpi%2FpkkgNf3qNELtBQH8SP%2FXH1ebBdwlQvp6wAZ%2BFrUH2Sxi4xW0RgImEmW%2FUdPd8vuUpX2NyKxEVO37lDMZNAdHtu8y5MhKnYxT%2BLw1VZfYtWvGpxTmu%2BE7xmFAMfcJ1pWtSzh1vMNFjWamXiLIPxiesWBqHtvxoCD7x7R6pDTjtEq%2F2cbz2%2BNuJeHP17nGHsGwg%2Byeuwp8w0OPauQY6pAFrbGskMmUp5CAVI6W9PdjYsZLngylQBGylSenXZwXZpITCZdLaiTlVp2xjkOsOZAgejmhXuQ6nOT7cT61YfkbXi4y7%2BpZP2pvKTNaxL3eUKu%2FdRlGzFF6wgsl%2F%2Bcq7bUt9j7mr2rLrhDqxs5Ba1%2FcWeJCImv10%2F4xPMoCfyodhdaisMq5UHJrjVuYA2yDhtMECZRB0EATb1qNiYA59wGxj9xH%2B0g%3D%3D&X-Amz-Signature=167181ba9c89dea7b4c41433da3906c1d36bef4254df4332f2e91afd8b2dc868\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:29:35.382210Z","iopub.execute_input":"2024-11-15T04:29:35.383115Z","iopub.status.idle":"2024-11-15T04:30:01.549415Z","shell.execute_reply.started":"2024-11-15T04:29:35.383069Z","shell.execute_reply":"2024-11-15T04:30:01.548465Z"}},"outputs":[{"name":"stdout","text":"Video downloaded successfully as 'downloaded_video.mp4'\nError during conversion: [Errno 30] Read-only file system: '/kaggle/input/output_audio.mp3'\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import whisper\nlarge = whisper.load_model(\"large-v3-turbo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:30:59.811506Z","iopub.execute_input":"2024-11-15T04:30:59.812195Z","iopub.status.idle":"2024-11-15T04:31:29.949295Z","shell.execute_reply.started":"2024-11-15T04:30:59.812151Z","shell.execute_reply":"2024-11-15T04:31:29.948310Z"}},"outputs":[{"name":"stderr","text":"100%|██████████████████████████████████████| 1.51G/1.51G [00:12<00:00, 128MiB/s]\n/opt/conda/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"reslarge = large.transcribe(\"/kaggle/working/output_audio.mp3\", verbose=True, initial_prompt=\"A lecture on Natural Language Processing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:31:29.951089Z","iopub.execute_input":"2024-11-15T04:31:29.951559Z","iopub.status.idle":"2024-11-15T04:35:28.776851Z","shell.execute_reply.started":"2024-11-15T04:31:29.951524Z","shell.execute_reply":"2024-11-15T04:35:28.775811Z"}},"outputs":[{"name":"stdout","text":"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\nDetected language: English\n[00:00.000 --> 00:29.980]  Thank you.\n[00:30.000 --> 00:40.960]  Good morning, everyone.\n[00:49.800 --> 00:58.860]  So we had started to discuss text analysis or natural language processing.\n[01:00.900 --> 01:07.300]  And we had looked at a number of examples, the kinds of things that we can do with text.\n[01:07.300 --> 01:20.340]  And today we're going to start with actually some of the techniques that we use to convert documents to vectors.\n[01:20.340 --> 01:32.140]  Now, hopefully, let's just recap very quickly what we have talked about up until now.\n[01:32.140 --> 01:38.060]  So we have as input a set of documents.\n[01:38.060 --> 01:41.940]  We are ignoring any hyperlinks now.\n[01:42.280 --> 01:47.060]  We have talked about how we can model hyperlinks as graphs.\n[01:47.060 --> 02:03.080]  And so if you ever had to analyze content that actually had hyperlinks as well, then you would actually have to combine content with what the graph-based algorithms were.\n[02:03.080 --> 02:07.480]  So we're going to focus on the content itself, right?\n[02:07.540 --> 02:09.120]  So we have a corpus.\n[02:09.120 --> 02:19.440]  What we want to do is convert that corpus into a tabular representation.\n[02:19.440 --> 02:34.540]  where we have a set of variables that we have extracted from this text.\n[02:34.540 --> 02:38.480]  And we call this process vectorization.\n[02:43.040 --> 02:51.720]  We talked about the fact that the columns here, the features that we extract here,\n[02:51.720 --> 02:59.840]  the simplest method, which is known as a syntactic approach to vectorization,\n[03:00.680 --> 03:05.240]  creates a vocabulary\n[03:05.240 --> 03:16.180]  and then uses that vocabulary, each of the individual elements of this vocabulary as features.\n[03:16.180 --> 03:24.940]  We talked about the fact that the basic vocabulary is all unique words\n[03:24.940 --> 03:30.180]  that appear within the corpus.\n[03:31.640 --> 03:35.220]  But then we also talked about key phrases\n[03:35.220 --> 03:41.040]  like the United Nations,\n[03:42.100 --> 03:46.060]  like the United States of America,\n[03:46.180 --> 03:50.240]  like the Republic of India.\n[03:52.240 --> 03:56.040]  Or in fact, we even used some examples like\n[03:56.040 --> 03:58.760]  Durga Puja.\n[04:00.020 --> 04:01.880]  These are all key phrases.\n[04:03.020 --> 04:06.620]  And these need to be extracted from the text\n[04:06.620 --> 04:10.380]  because we don't really know which sequence of words\n[04:10.380 --> 04:12.640]  creates a key phrase.\n[04:12.640 --> 04:16.100]  And in fact, key phrases are sometimes even dependent on\n[04:16.100 --> 04:17.860]  what are these documents about.\n[04:20.600 --> 04:24.620]  And then what we said was that actually some key phrases have a type.\n[04:24.620 --> 04:30.540]  And we refer to those as named entities.\n[04:30.540 --> 04:40.860]  where we had seen examples like the Bhartia Janta Party being a named entity.\n[04:43.400 --> 04:46.860]  The Congress would be another example of a named entity.\n[04:48.600 --> 04:49.640]  The Ahmadinejad Party,\n[04:49.640 --> 04:52.460]  like the Bhartia Janta Party,\n[04:52.460 --> 04:54.300]  like the Bhartia Janta Party would be a key phrase that has a type,\n[04:54.460 --> 04:57.280]  therefore it would be a named entity.\n[04:59.500 --> 05:02.800]  All three of these would be of type\n[05:02.800 --> 05:04.900]  political party.\n[05:04.900 --> 05:17.900]  And then the three examples that we gave are what we refer to as instances of this type.\n[05:21.900 --> 05:25.720]  So the unique words and the key phrases\n[05:25.720 --> 05:28.360]  create our vocabulary.\n[05:28.360 --> 05:37.160]  And we can then assign each one of these elements of the vocabulary\n[05:37.160 --> 05:40.120]  as columns.\n[05:40.860 --> 05:43.900]  And then we can either put a one or a zero\n[05:43.900 --> 05:48.920]  as the vector representation\n[05:48.920 --> 05:51.800]  of a single document.\n[05:53.820 --> 05:56.640]  Does this element of the vocabulary appear in this?\n[05:56.940 --> 05:57.900]  Does it not appear?\n[05:58.360 --> 06:01.320]  We also saw an alternative.\n[06:02.480 --> 06:03.340]  And so in this case,\n[06:03.400 --> 06:05.380]  this is a binary vector.\n[06:08.660 --> 06:10.740]  We also looked at an alternative\n[06:10.740 --> 06:12.760]  where we can put in the frequency\n[06:12.760 --> 06:14.180]  with which words appear\n[06:14.180 --> 06:15.720]  in the document.\n[06:17.800 --> 06:19.720]  And in Python\n[06:19.720 --> 06:22.600]  library vocabulary,\n[06:22.900 --> 06:25.320]  we call that the count vectorizer.\n[06:25.320 --> 06:31.480]  The vectorizer is what creates the vector.\n[06:32.480 --> 06:34.340]  You basically have a count vector\n[06:34.340 --> 06:37.120]  when you produce it through the count vectorizer.\n[06:37.120 --> 06:44.520]  And then we also looked at the TFI-DF vectorizer.\n[06:48.820 --> 06:50.240]  That balanced\n[06:50.240 --> 06:53.760]  the frequency with which\n[06:53.760 --> 06:55.820]  a word appeared in a document\n[06:55.820 --> 06:59.420]  and the frequency with which it appeared\n[06:59.420 --> 07:00.880]  across the corpus,\n[07:00.880 --> 07:01.980]  across documents.\n[07:01.980 --> 07:07.400]  So the more common the word that it appears in all documents within the corpus,\n[07:07.520 --> 07:09.780]  the less excited we are about it\n[07:09.780 --> 07:11.300]  as\n[07:11.300 --> 07:15.600]  representing what an individual document is about.\n[07:15.600 --> 07:19.960]  The higher the frequency of the word in the document,\n[07:19.960 --> 07:21.860]  the more encouraged we are\n[07:21.860 --> 07:25.480]  that this document is something to do with this element of the vocab.\n[07:26.800 --> 07:27.060]  Right?\n[07:27.120 --> 07:27.640]  And we will\n[07:27.640 --> 07:32.060]  look at this in a little more detail soon in any case.\n[07:32.060 --> 07:33.380]  But these are the three\n[07:33.380 --> 07:36.600]  syntactic vectorization approaches.\n[07:37.860 --> 07:39.340]  And then what we said was\n[07:39.340 --> 07:41.340]  the big problem with this,\n[07:41.580 --> 07:42.820]  there are a number of problems,\n[07:43.260 --> 07:47.620]  but the biggest problem is that it is vocabulary dependent.\n[07:49.440 --> 07:50.100]  And so\n[07:50.100 --> 07:53.420]  what we really want is a representation of a document\n[07:53.420 --> 07:56.700]  that is semantic in nature.\n[07:56.700 --> 08:02.520]  where these columns represent\n[08:02.520 --> 08:05.320]  semantic elements.\n[08:06.320 --> 08:08.180]  What is this document about?\n[08:09.580 --> 08:10.260]  Conceptually.\n[08:10.760 --> 08:13.320]  So at a higher level of abstraction,\n[08:13.980 --> 08:16.960]  we are wanting to have columns\n[08:16.960 --> 08:20.060]  that can represent\n[08:20.060 --> 08:22.640]  the semantics.\n[08:23.520 --> 08:24.920]  What is this document about?\n[08:24.920 --> 08:27.440]  And what we said there was\n[08:27.440 --> 08:29.820]  if we can find topics\n[08:29.820 --> 08:34.840]  where topics\n[08:34.840 --> 08:38.620]  drive the vocabulary that is used.\n[08:40.520 --> 08:41.440]  So\n[08:41.440 --> 08:43.000]  you could actually say\n[08:43.000 --> 08:45.580]  the author of a document\n[08:45.580 --> 08:49.200]  and the topic\n[08:49.200 --> 08:49.900]  together\n[08:49.900 --> 08:52.580]  drive the\n[08:52.580 --> 08:54.800]  words or vocabulary.\n[08:54.920 --> 08:57.000]  used in a topic.\n[08:59.440 --> 09:00.480]  And in fact,\n[09:00.600 --> 09:01.600]  typically a document\n[09:01.600 --> 09:02.460]  isn't\n[09:02.460 --> 09:05.100]  related to a single topic.\n[09:05.840 --> 09:06.720]  It is\n[09:06.720 --> 09:07.960]  a mixture\n[09:07.960 --> 09:09.480]  of topics.\n[09:12.480 --> 09:13.300]  And so\n[09:13.300 --> 09:15.340]  based on that mixture of topics,\n[09:15.700 --> 09:18.200]  the vocabulary within this document\n[09:18.200 --> 09:18.780]  is generated.\n[09:18.780 --> 09:21.360]  and we will look at\n[09:21.360 --> 09:23.620]  or we will model\n[09:23.620 --> 09:24.460]  these topics\n[09:24.460 --> 09:25.960]  as\n[09:25.960 --> 09:27.520]  hidden\n[09:27.520 --> 09:30.460]  variables.\n[09:32.740 --> 09:34.180]  And another word for them\n[09:34.180 --> 09:34.560]  is\n[09:34.560 --> 09:35.180]  latent.\n[09:35.180 --> 09:35.260]  Latent.\n[09:35.260 --> 09:35.700]  Latent.\n[09:35.700 --> 09:35.860]  Latent.\n[09:35.860 --> 09:35.880]  Latent.\n[09:35.880 --> 09:35.900]  Latent.\n[09:35.900 --> 09:35.920]  Latent.\n[09:35.920 --> 09:35.940]  Latent.\n[09:35.940 --> 09:36.220]  Latent.\n[09:38.800 --> 09:39.280]  Okay.\n[09:39.460 --> 09:39.820]  And so\n[09:39.820 --> 09:41.000]  this is what we are,\n[09:41.560 --> 09:43.000]  we have discussed to date.\n[09:43.300 --> 09:43.940]  We haven't gotten\n[09:43.940 --> 09:44.800]  into any depth.\n[09:45.000 --> 09:45.740]  Now we are going to go\n[09:45.740 --> 09:46.200]  into depth.\n[09:46.200 --> 09:48.640]  So the basic idea\n[09:48.640 --> 09:49.500]  here is\n[09:49.500 --> 09:51.100]  that documents\n[09:51.100 --> 09:52.780]  should be represented\n[09:52.780 --> 09:53.560]  as vectors\n[09:53.560 --> 09:56.200]  in what is referred to\n[09:56.200 --> 09:56.680]  as the\n[09:56.680 --> 09:58.220]  vector space model\n[09:58.220 --> 09:59.660]  where\n[09:59.660 --> 10:00.920]  every point\n[10:00.920 --> 10:02.300]  in this space\n[10:02.300 --> 10:03.100]  and of course\n[10:03.100 --> 10:03.900]  I'm only drawing\n[10:03.900 --> 10:04.720]  two dimensions\n[10:04.720 --> 10:05.820]  or three dimensions\n[10:05.820 --> 10:07.640]  but every point\n[10:07.640 --> 10:08.560]  in this space\n[10:08.560 --> 10:10.520]  is\n[10:10.520 --> 10:11.780]  now a document.\n[10:11.780 --> 10:14.500]  and as long\n[10:14.500 --> 10:15.460]  as we can have\n[10:15.460 --> 10:17.000]  all documents\n[10:17.000 --> 10:17.620]  represented\n[10:17.620 --> 10:18.640]  in this space\n[10:18.640 --> 10:19.500]  as vectors\n[10:19.500 --> 10:20.300]  we can now\n[10:20.300 --> 10:20.900]  calculate\n[10:20.900 --> 10:22.060]  similarity\n[10:22.060 --> 10:22.880]  between them.\n[10:23.600 --> 10:24.520]  We can therefore\n[10:24.520 --> 10:25.120]  apply\n[10:25.120 --> 10:26.420]  a lot of\n[10:26.420 --> 10:27.260]  the machine learning\n[10:27.260 --> 10:27.820]  algorithms\n[10:27.820 --> 10:28.300]  now\n[10:28.300 --> 10:29.680]  on documents\n[10:29.680 --> 10:31.160]  as we had done\n[10:31.160 --> 10:32.360]  on structured data.\n[10:32.580 --> 10:33.500]  So this idea\n[10:33.500 --> 10:34.440]  of a vector\n[10:34.440 --> 10:35.240]  representation\n[10:35.240 --> 10:37.160]  is really all\n[10:37.160 --> 10:37.700]  about\n[10:37.700 --> 10:38.880]  creating a\n[10:38.880 --> 10:39.480]  structured\n[10:39.480 --> 10:39.780]  representation\n[10:39.780 --> 10:41.880]  of documents.\n[10:44.180 --> 10:45.100]  If we think\n[10:45.100 --> 10:45.540]  of named\n[10:45.540 --> 10:46.040]  entities\n[10:46.040 --> 10:48.820]  we could also\n[10:48.820 --> 10:49.540]  instead\n[10:49.540 --> 10:51.000]  say forget\n[10:51.000 --> 10:51.420]  about the\n[10:51.420 --> 10:51.880]  vocabulary\n[10:51.880 --> 10:52.820]  that does not\n[10:52.820 --> 10:53.700]  belong to the\n[10:53.700 --> 10:54.340]  set of named\n[10:54.340 --> 10:54.840]  entities.\n[10:55.720 --> 10:56.760]  I am only\n[10:56.760 --> 10:57.760]  interested in\n[10:57.760 --> 10:59.800]  any political\n[10:59.800 --> 11:00.660]  party that is\n[11:00.660 --> 11:01.060]  mentioned\n[11:01.060 --> 11:03.740]  any person\n[11:03.740 --> 11:04.520]  that is\n[11:04.520 --> 11:05.060]  mentioned\n[11:05.060 --> 11:07.040]  and so on\n[11:07.040 --> 11:07.640]  and so forth.\n[11:08.820 --> 11:08.940]  Right?\n[11:09.020 --> 11:09.620]  And so out\n[11:09.620 --> 11:09.960]  here\n[11:09.960 --> 11:11.160]  if for a\n[11:11.160 --> 11:11.560]  document\n[11:11.560 --> 11:12.500]  I have\n[11:12.500 --> 11:13.080]  multiple\n[11:13.080 --> 11:14.320]  political\n[11:14.320 --> 11:14.820]  parties\n[11:14.820 --> 11:15.200]  that I've\n[11:15.200 --> 11:15.600]  mentioned\n[11:15.600 --> 11:16.960]  now what I\n[11:16.960 --> 11:17.360]  can do\n[11:17.360 --> 11:18.160]  is I can\n[11:18.160 --> 11:18.920]  store them\n[11:18.920 --> 11:20.980]  as an\n[11:20.980 --> 11:21.460]  array for\n[11:21.460 --> 11:21.880]  example.\n[11:23.180 --> 11:23.400]  Right?\n[11:23.620 --> 11:24.080]  And\n[11:24.080 --> 11:24.860]  similarly\n[11:24.860 --> 11:25.640]  people that\n[11:25.640 --> 11:26.140]  I've mentioned\n[11:26.140 --> 11:26.580]  I could\n[11:26.580 --> 11:27.340]  represent as\n[11:27.340 --> 11:27.680]  an array.\n[11:28.320 --> 11:28.780]  Now of\n[11:28.780 --> 11:29.220]  course the\n[11:29.220 --> 11:29.860]  array is not\n[11:29.860 --> 11:30.480]  great for\n[11:30.480 --> 11:31.500]  me because\n[11:31.500 --> 11:34.280]  I need\n[11:34.280 --> 11:35.540]  one value\n[11:35.540 --> 11:36.440]  in each\n[11:36.440 --> 11:37.200]  element and\n[11:37.200 --> 11:37.760]  so I could\n[11:37.760 --> 11:38.720]  actually represent\n[11:38.720 --> 11:40.400]  it by\n[11:40.400 --> 11:41.760]  doing the\n[11:41.760 --> 11:42.560]  equivalent of\n[11:42.560 --> 11:43.180]  one hot\n[11:43.180 --> 11:43.580]  encoding.\n[11:47.300 --> 11:48.560]  And so I\n[11:48.560 --> 11:48.880]  would\n[11:48.880 --> 11:51.340]  generate a\n[11:51.340 --> 11:52.160]  much larger\n[11:52.160 --> 11:53.100]  dimensionality\n[11:53.100 --> 11:55.460]  where I\n[11:55.460 --> 11:55.920]  would now\n[11:55.920 --> 11:58.040]  have the\n[11:58.040 --> 11:58.700]  BJP,\n[11:58.860 --> 11:59.600]  the Congress,\n[11:59.600 --> 12:01.800]  etc.\n[12:01.920 --> 12:02.220]  etc.\n[12:02.320 --> 12:02.920]  as columns\n[12:02.920 --> 12:04.320]  and I\n[12:04.320 --> 12:04.680]  would now\n[12:04.680 --> 12:05.400]  have a\n[12:05.400 --> 12:06.460]  one in\n[12:06.460 --> 12:06.880]  each of\n[12:06.880 --> 12:07.280]  the ones\n[12:07.280 --> 12:07.800]  that appear\n[12:07.800 --> 12:08.500]  and zeros\n[12:08.500 --> 12:08.800]  in the\n[12:08.800 --> 12:09.220]  ones that\n[12:09.220 --> 12:09.780]  don't appear.\n[12:11.980 --> 12:12.540]  Right?\n[12:12.660 --> 12:13.040]  So out\n[12:13.040 --> 12:13.900]  here what\n[12:13.900 --> 12:14.160]  am I\n[12:14.160 --> 12:14.460]  doing?\n[12:14.880 --> 12:15.360]  If I\n[12:15.360 --> 12:16.520]  limit myself\n[12:16.520 --> 12:17.040]  to only\n[12:17.040 --> 12:17.640]  using the\n[12:17.640 --> 12:17.900]  named\n[12:17.900 --> 12:18.580]  entities,\n[12:20.180 --> 12:21.060]  I am\n[12:21.060 --> 12:21.840]  ignoring all\n[12:21.840 --> 12:22.480]  words that\n[12:22.480 --> 12:22.920]  are not\n[12:22.920 --> 12:23.580]  entities,\n[12:23.940 --> 12:25.620]  I am\n[12:25.620 --> 12:26.420]  restricting\n[12:26.420 --> 12:26.780]  my\n[12:26.780 --> 12:27.340]  vocabulary.\n[12:29.600 --> 12:33.980]  I am\n[12:33.980 --> 12:34.280]  doing\n[12:34.280 --> 12:35.180]  dimensionality\n[12:35.180 --> 12:35.640]  reduction\n[12:35.640 --> 12:38.780]  and focusing\n[12:38.780 --> 12:40.500]  on just\n[12:40.500 --> 12:43.060]  the different\n[12:43.060 --> 12:44.100]  types of\n[12:44.100 --> 12:45.180]  data that\n[12:45.180 --> 12:46.320]  are represented\n[12:46.320 --> 12:46.960]  in the\n[12:46.960 --> 12:47.300]  text.\n[12:48.920 --> 12:49.720]  Now,\n[12:50.120 --> 12:51.500]  which one\n[12:51.500 --> 12:51.800]  is the\n[12:51.800 --> 12:52.500]  better approach\n[12:52.500 --> 12:53.200]  to use?\n[12:53.920 --> 12:54.580]  Ultimately,\n[12:54.780 --> 12:55.100]  you're going\n[12:55.100 --> 12:55.660]  to end up\n[12:55.660 --> 12:55.960]  with this\n[12:55.960 --> 12:56.260]  vector\n[12:56.260 --> 12:56.920]  representation.\n[12:56.920 --> 12:57.940]  all that's\n[12:57.940 --> 12:58.640]  happening is\n[12:58.640 --> 12:59.120]  that out\n[12:59.120 --> 12:59.520]  of the\n[12:59.520 --> 13:00.320]  complete\n[13:00.320 --> 13:00.860]  vocabulary,\n[13:01.940 --> 13:02.760]  which is\n[13:02.760 --> 13:03.400]  key phrases\n[13:03.400 --> 13:04.260]  and unique\n[13:04.260 --> 13:04.820]  words,\n[13:05.380 --> 13:06.080]  we are now\n[13:06.080 --> 13:06.700]  coming up\n[13:06.700 --> 13:06.940]  with a\n[13:06.940 --> 13:07.520]  representation\n[13:07.520 --> 13:08.280]  where we\n[13:08.280 --> 13:08.940]  have limited\n[13:08.940 --> 13:09.620]  it to be\n[13:09.620 --> 13:10.760]  only those\n[13:10.760 --> 13:11.460]  key phrases\n[13:11.460 --> 13:12.120]  and words\n[13:12.120 --> 13:12.480]  that are\n[13:12.480 --> 13:13.160]  instances\n[13:13.160 --> 13:14.320]  of some\n[13:14.320 --> 13:14.860]  entities.\n[13:14.860 --> 13:14.960]  things.\n[13:17.820 --> 13:18.220]  Now,\n[13:18.300 --> 13:18.600]  if there's\n[13:18.600 --> 13:18.940]  too much\n[13:18.940 --> 13:19.460]  loss of\n[13:19.460 --> 13:19.980]  information\n[13:19.980 --> 13:20.620]  by dropping\n[13:20.620 --> 13:23.360]  the rest\n[13:23.360 --> 13:23.620]  of the\n[13:23.620 --> 13:24.100]  vocabulary,\n[13:24.640 --> 13:25.080]  you're not\n[13:25.080 --> 13:25.360]  going to\n[13:25.360 --> 13:25.660]  get a\n[13:25.660 --> 13:26.000]  very good\n[13:26.000 --> 13:26.300]  model,\n[13:26.440 --> 13:26.580]  right?\n[13:26.600 --> 13:26.900]  But this\n[13:26.900 --> 13:27.380]  is really,\n[13:27.600 --> 13:27.920]  again,\n[13:28.860 --> 13:29.660]  a feature\n[13:29.660 --> 13:30.300]  engineering\n[13:30.300 --> 13:31.860]  problem.\n[13:36.200 --> 13:36.640]  So,\n[13:36.800 --> 13:37.320]  for now,\n[13:37.360 --> 13:37.620]  we will\n[13:37.620 --> 13:38.760]  assume that\n[13:38.760 --> 13:39.260]  we want\n[13:39.260 --> 13:40.160]  to use\n[13:40.160 --> 13:40.880]  all of\n[13:40.880 --> 13:41.380]  the unique\n[13:41.380 --> 13:42.460]  words and\n[13:42.460 --> 13:43.120]  key phrases\n[13:43.120 --> 13:43.480]  in our\n[13:43.480 --> 13:43.940]  vocabulary.\n[13:44.860 --> 13:47.240]  and that\n[13:47.240 --> 13:47.760]  would be\n[13:47.760 --> 13:48.160]  our\n[13:48.160 --> 13:49.080]  vector\n[13:49.080 --> 13:49.580]  space\n[13:49.580 --> 13:50.300]  model\n[13:50.300 --> 13:51.400]  for our\n[13:51.400 --> 13:52.340]  documents.\n[13:53.560 --> 13:54.220]  Now,\n[13:54.440 --> 13:55.000]  one of the\n[13:55.000 --> 13:55.840]  downsides\n[13:55.840 --> 13:56.760]  of doing\n[13:56.760 --> 13:57.440]  this\n[13:57.440 --> 13:59.180]  representation,\n[13:59.860 --> 14:00.160]  which is\n[14:00.160 --> 14:00.700]  also known\n[14:00.700 --> 14:01.080]  as the\n[14:01.080 --> 14:01.720]  bag of\n[14:01.720 --> 14:02.100]  words\n[14:02.100 --> 14:02.820]  representation,\n[14:03.600 --> 14:05.120]  is the\n[14:05.120 --> 14:05.800]  fact that\n[14:05.800 --> 14:06.620]  we ignore\n[14:06.620 --> 14:07.360]  the ordering\n[14:07.360 --> 14:08.540]  of words.\n[14:09.640 --> 14:09.860]  So,\n[14:09.960 --> 14:10.380]  the poor\n[14:10.380 --> 14:11.140]  man ate\n[14:11.140 --> 14:11.840]  the food\n[14:11.840 --> 14:13.300]  and the\n[14:13.300 --> 14:13.980]  man ate\n[14:13.980 --> 14:14.520]  the poor\n[14:14.520 --> 14:16.000]  food have\n[14:16.000 --> 14:16.620]  the same\n[14:16.620 --> 14:17.300]  bag of\n[14:17.300 --> 14:17.520]  words\n[14:17.520 --> 14:18.260]  representation\n[14:18.260 --> 14:19.080]  because we\n[14:19.080 --> 14:19.720]  are ignoring\n[14:19.720 --> 14:20.820]  the sequence\n[14:20.820 --> 14:21.420]  in which\n[14:21.420 --> 14:22.480]  the words\n[14:22.480 --> 14:22.680]  are\n[14:22.680 --> 14:22.960]  appeared.\n[14:24.060 --> 14:24.480]  Right?\n[14:24.560 --> 14:24.640]  So,\n[14:24.720 --> 14:25.020]  in a way,\n[14:25.060 --> 14:25.500]  this is a\n[14:25.500 --> 14:26.040]  simplification\n[14:26.040 --> 14:26.620]  of the\n[14:26.620 --> 14:27.020]  problem\n[14:27.020 --> 14:28.800]  where we\n[14:28.800 --> 14:29.180]  are saying,\n[14:29.320 --> 14:29.380]  well,\n[14:29.400 --> 14:29.900]  the sequence\n[14:29.900 --> 14:30.200]  in which\n[14:30.200 --> 14:31.800]  the words\n[14:31.800 --> 14:32.380]  appear\n[14:32.380 --> 14:33.420]  are not\n[14:33.420 --> 14:33.980]  important.\n[14:34.540 --> 14:35.320]  All that's\n[14:35.320 --> 14:35.920]  important is\n[14:35.920 --> 14:36.240]  that the\n[14:36.240 --> 14:36.580]  word\n[14:36.580 --> 14:37.160]  appeared.\n[14:37.160 --> 14:37.720]  and so\n[14:37.720 --> 14:38.420]  when we\n[14:38.420 --> 14:38.920]  were talking\n[14:38.920 --> 14:39.600]  about the\n[14:39.600 --> 14:40.160]  Bayesian\n[14:40.160 --> 14:41.500]  approach to\n[14:41.500 --> 14:42.980]  text,\n[14:43.380 --> 14:44.020]  and we\n[14:44.020 --> 14:44.440]  said,\n[14:45.180 --> 14:45.440]  you know,\n[14:45.500 --> 14:46.420]  they were\n[14:46.420 --> 14:47.580]  really representing\n[14:47.580 --> 14:48.400]  only statistical\n[14:48.400 --> 14:48.960]  properties,\n[14:49.640 --> 14:50.960]  and one of\n[14:50.960 --> 14:52.160]  the simplifying\n[14:52.160 --> 14:52.960]  assumptions they\n[14:52.960 --> 14:53.680]  made was that\n[14:53.680 --> 14:54.240]  the sequence\n[14:54.240 --> 14:54.620]  was not\n[14:54.620 --> 14:55.180]  important.\n[14:55.900 --> 14:56.740]  We look at\n[14:56.740 --> 14:57.620]  why that was\n[14:57.620 --> 14:58.180]  the case,\n[14:58.500 --> 14:59.140]  but you can\n[14:59.140 --> 15:00.260]  see that the\n[15:00.260 --> 15:01.880]  vector space\n[15:01.880 --> 15:04.140]  model was\n[15:04.140 --> 15:04.520]  kind of\n[15:04.520 --> 15:05.360]  influenced by\n[15:05.360 --> 15:05.840]  that thought\n[15:05.840 --> 15:06.380]  process,\n[15:06.520 --> 15:06.700]  that,\n[15:06.760 --> 15:07.020]  you know,\n[15:07.480 --> 15:09.340]  the order\n[15:09.340 --> 15:10.060]  of words\n[15:10.060 --> 15:10.520]  is not\n[15:10.520 --> 15:10.760]  important.\n[15:12.320 --> 15:12.620]  So,\n[15:12.780 --> 15:13.060]  then,\n[15:13.300 --> 15:13.860]  essentially,\n[15:14.080 --> 15:14.520]  from a\n[15:14.520 --> 15:15.780]  pre-processing\n[15:15.780 --> 15:16.520]  of text\n[15:16.520 --> 15:17.620]  is concerned,\n[15:18.000 --> 15:20.160]  the basic\n[15:20.160 --> 15:20.800]  steps are\n[15:20.800 --> 15:22.040]  we tokenize\n[15:22.040 --> 15:24.020]  the text,\n[15:24.160 --> 15:24.660]  which means\n[15:24.660 --> 15:25.580]  we break it\n[15:25.580 --> 15:26.240]  down to a\n[15:26.240 --> 15:27.140]  word representation.\n[15:27.540 --> 15:27.680]  Now,\n[15:27.760 --> 15:28.200]  of course,\n[15:28.680 --> 15:29.100]  you would\n[15:29.100 --> 15:29.440]  have,\n[15:29.540 --> 15:29.860]  for some\n[15:29.860 --> 15:30.160]  of you\n[15:30.160 --> 15:30.520]  who have\n[15:30.520 --> 15:31.180]  played with\n[15:31.180 --> 15:32.080]  JackGPT,\n[15:32.880 --> 15:33.920]  you know\n[15:33.920 --> 15:35.100]  that they\n[15:35.100 --> 15:35.660]  also talk\n[15:35.660 --> 15:36.160]  about the\n[15:36.160 --> 15:36.600]  number of\n[15:36.600 --> 15:37.060]  tokens,\n[15:37.260 --> 15:37.560]  they don't\n[15:37.560 --> 15:38.080]  talk about\n[15:38.080 --> 15:38.640]  words,\n[15:39.560 --> 15:39.860]  right?\n[15:40.280 --> 15:41.300]  And part\n[15:41.300 --> 15:41.520]  of the\n[15:41.520 --> 15:42.040]  reason why\n[15:42.040 --> 15:42.400]  they talk\n[15:42.400 --> 15:43.000]  about tokens\n[15:43.000 --> 15:43.480]  rather than\n[15:43.480 --> 15:44.180]  words is\n[15:44.180 --> 15:44.560]  that they\n[15:44.560 --> 15:45.100]  have actually\n[15:45.100 --> 15:46.660]  got a\n[15:46.660 --> 15:48.640]  subword\n[15:48.640 --> 15:49.640]  tokenization\n[15:49.640 --> 15:50.140]  method,\n[15:51.040 --> 15:51.260]  right?\n[15:51.340 --> 15:51.940]  And so,\n[15:52.580 --> 15:53.440]  while we\n[15:53.440 --> 15:53.960]  have talked\n[15:53.960 --> 15:54.560]  about taking\n[15:54.560 --> 15:55.600]  words as\n[15:55.600 --> 15:56.240]  the primitive\n[15:56.240 --> 15:58.060]  object and\n[15:58.060 --> 15:59.080]  finding key\n[15:59.080 --> 16:00.220]  phrases and\n[16:00.220 --> 16:01.380]  representing that\n[16:01.380 --> 16:02.180]  as a vector\n[16:02.180 --> 16:02.800]  representation,\n[16:03.120 --> 16:03.520]  which is the\n[16:03.520 --> 16:04.160]  traditional way\n[16:04.160 --> 16:04.540]  of doing\n[16:04.540 --> 16:04.820]  it,\n[16:05.520 --> 16:08.340]  the more\n[16:08.340 --> 16:09.040]  recent\n[16:09.040 --> 16:09.780]  innovations\n[16:09.780 --> 16:10.620]  have started\n[16:10.620 --> 16:11.120]  to say,\n[16:11.260 --> 16:11.700]  actually,\n[16:11.980 --> 16:12.340]  we are\n[16:12.340 --> 16:13.060]  better off\n[16:13.060 --> 16:14.160]  looking at\n[16:14.160 --> 16:14.880]  subword\n[16:14.880 --> 16:15.760]  tokenization.\n[16:16.920 --> 16:16.940]  So,\n[16:17.080 --> 16:17.600]  breaking it\n[16:17.600 --> 16:18.180]  by the\n[16:18.180 --> 16:18.360]  way,\n[16:18.420 --> 16:22.120]  in case\n[16:22.120 --> 16:22.480]  of\n[16:22.480 --> 16:23.900]  vector\n[16:23.900 --> 16:24.180]  space\n[16:24.180 --> 16:24.480]  model,\n[16:25.440 --> 16:25.960]  not in\n[16:25.960 --> 16:26.280]  case of\n[16:26.280 --> 16:26.780]  tokenization,\n[16:27.040 --> 16:27.460]  in case of\n[16:27.460 --> 16:27.860]  vector space\n[16:27.860 --> 16:28.120]  model,\n[16:28.260 --> 16:28.660]  are we\n[16:28.660 --> 16:29.300]  taking the\n[16:29.300 --> 16:29.860]  plurals of\n[16:29.860 --> 16:30.180]  words,\n[16:30.440 --> 16:30.960]  or separate\n[16:30.960 --> 16:32.640]  words or\n[16:32.640 --> 16:33.760]  using the\n[16:33.760 --> 16:34.520]  plurals and\n[16:34.520 --> 16:35.080]  the singulars\n[16:35.080 --> 16:35.580]  are same?\n[16:36.800 --> 16:38.120]  It depends\n[16:38.120 --> 16:38.620]  on what you\n[16:38.620 --> 16:38.840]  do,\n[16:38.920 --> 16:39.080]  right?\n[16:39.260 --> 16:39.500]  Here,\n[16:39.700 --> 16:40.100]  the second\n[16:40.100 --> 16:40.720]  step is\n[16:40.720 --> 16:41.940]  stemming and\n[16:41.940 --> 16:42.740]  lemmatization,\n[16:43.480 --> 16:43.760]  right?\n[16:43.760 --> 16:44.360]  and this\n[16:44.360 --> 16:44.840]  typically\n[16:44.840 --> 16:45.680]  removes\n[16:45.680 --> 16:47.840]  not only\n[16:47.840 --> 16:48.820]  plurals and\n[16:48.820 --> 16:49.620]  singular and\n[16:49.620 --> 16:50.220]  k makes them\n[16:50.220 --> 16:50.520]  one,\n[16:50.900 --> 16:51.440]  but it\n[16:51.440 --> 16:52.160]  actually goes\n[16:52.160 --> 16:52.540]  to the\n[16:52.540 --> 16:53.460]  root word,\n[16:54.160 --> 16:54.400]  right?\n[16:54.740 --> 16:55.080]  Like\n[16:55.080 --> 16:56.980]  organize without\n[16:56.980 --> 16:57.660]  the e,\n[16:58.460 --> 16:59.020]  replaces\n[16:59.020 --> 17:00.100]  organizing,\n[17:00.400 --> 17:01.300]  organizes,\n[17:01.440 --> 17:01.960]  and organize.\n[17:03.340 --> 17:03.940]  Okay.\n[17:03.940 --> 17:03.980]  Okay.\n[17:04.880 --> 17:05.400]  Okay.\n[17:05.520 --> 17:05.860]  So,\n[17:06.000 --> 17:06.360]  again,\n[17:06.700 --> 17:07.200]  the reason\n[17:07.200 --> 17:07.760]  for doing\n[17:07.760 --> 17:08.420]  that is\n[17:08.420 --> 17:09.300]  to reduce\n[17:09.300 --> 17:09.620]  the\n[17:09.620 --> 17:10.540]  dimensionality.\n[17:10.700 --> 17:11.060]  The curse\n[17:11.060 --> 17:11.840]  of dimensionality\n[17:11.840 --> 17:12.320]  is a real\n[17:12.320 --> 17:12.940]  problem when\n[17:12.940 --> 17:13.460]  it comes to\n[17:13.460 --> 17:13.820]  text,\n[17:13.920 --> 17:14.100]  right?\n[17:14.140 --> 17:14.740]  Because the\n[17:14.740 --> 17:15.100]  number of\n[17:15.100 --> 17:15.820]  unique words\n[17:15.820 --> 17:16.280]  and key\n[17:16.280 --> 17:17.980]  phrases often\n[17:17.980 --> 17:18.540]  goes into\n[17:18.540 --> 17:19.020]  millions.\n[17:23.200 --> 17:23.720]  So,\n[17:24.040 --> 17:24.860]  once we\n[17:24.860 --> 17:25.100]  have done\n[17:25.100 --> 17:25.960]  the tokenization,\n[17:26.480 --> 17:26.860]  again,\n[17:27.000 --> 17:27.660]  we may or\n[17:27.660 --> 17:28.040]  may not\n[17:28.040 --> 17:28.500]  want to\n[17:28.500 --> 17:29.160]  remove the\n[17:29.160 --> 17:29.940]  punctuation\n[17:29.940 --> 17:32.600]  because in\n[17:32.600 --> 17:32.960]  certain\n[17:32.960 --> 17:33.640]  applications,\n[17:33.940 --> 17:34.800]  the punctuation\n[17:34.800 --> 17:35.220]  may be\n[17:35.220 --> 17:35.660]  important.\n[17:36.640 --> 17:37.200]  Another thing\n[17:37.200 --> 17:37.800]  that people\n[17:37.800 --> 17:38.760]  say you\n[17:38.760 --> 17:39.260]  should do\n[17:39.260 --> 17:40.040]  is to\n[17:40.040 --> 17:43.240]  convert every\n[17:43.240 --> 17:44.020]  word to\n[17:44.020 --> 17:44.720]  lowercase.\n[17:45.700 --> 17:46.500]  That may\n[17:46.500 --> 17:47.040]  or may not\n[17:47.040 --> 17:47.660]  be the best\n[17:47.660 --> 17:48.300]  thing to do.\n[17:48.620 --> 17:49.060]  When you're\n[17:49.060 --> 17:49.760]  looking for\n[17:49.760 --> 17:50.620]  named entities,\n[17:50.800 --> 17:51.460]  for example,\n[17:52.300 --> 17:52.720]  there are\n[17:52.720 --> 17:53.520]  features that\n[17:53.520 --> 17:53.740]  say,\n[17:53.860 --> 17:54.020]  well,\n[17:54.140 --> 17:54.380]  you know,\n[17:54.420 --> 17:54.680]  if you\n[17:54.680 --> 17:57.360]  have some\n[17:57.360 --> 17:58.100]  capital\n[17:58.100 --> 17:59.300]  letters,\n[17:59.780 --> 18:00.360]  it's likely\n[18:00.360 --> 18:00.940]  to be a\n[18:00.940 --> 18:01.240]  noun,\n[18:01.400 --> 18:01.720]  which could\n[18:01.720 --> 18:02.000]  be a\n[18:02.000 --> 18:02.720]  person or\n[18:02.720 --> 18:02.920]  a,\n[18:02.920 --> 18:03.540]  you know,\n[18:03.540 --> 18:06.520]  a location\n[18:06.520 --> 18:07.180]  or whatever.\n[18:08.680 --> 18:10.260]  But in\n[18:10.260 --> 18:10.700]  general,\n[18:10.860 --> 18:11.160]  what we\n[18:11.160 --> 18:11.540]  would do\n[18:11.540 --> 18:12.040]  is to\n[18:12.040 --> 18:12.860]  minimize the\n[18:12.860 --> 18:13.320]  vocabulary\n[18:13.320 --> 18:15.100]  size after\n[18:15.100 --> 18:16.140]  tokenizing and\n[18:16.140 --> 18:16.500]  removing\n[18:16.500 --> 18:17.500]  punctuation,\n[18:18.500 --> 18:18.900]  we would\n[18:18.900 --> 18:22.440]  also change\n[18:22.440 --> 18:23.120]  the case\n[18:23.120 --> 18:24.620]  to lowercase\n[18:24.620 --> 18:25.020]  for all\n[18:25.020 --> 18:25.400]  words.\n[18:26.000 --> 18:26.480]  And then\n[18:26.480 --> 18:26.820]  we would\n[18:26.820 --> 18:27.600]  do stemming\n[18:27.600 --> 18:28.160]  and\n[18:28.160 --> 18:29.220]  lemmatization,\n[18:29.400 --> 18:29.840]  two different\n[18:29.840 --> 18:30.280]  approaches,\n[18:30.280 --> 18:32.780]  to mapping\n[18:32.780 --> 18:33.420]  multiple\n[18:33.420 --> 18:35.440]  words to\n[18:35.440 --> 18:36.000]  their root\n[18:36.000 --> 18:36.360]  form.\n[18:37.900 --> 18:38.720]  And then\n[18:38.720 --> 18:39.580]  we remove\n[18:39.580 --> 18:40.340]  stop words,\n[18:40.480 --> 18:41.080]  words like\n[18:41.080 --> 18:41.400]  a,\n[18:41.520 --> 18:41.920]  and,\n[18:42.160 --> 18:42.360]  the,\n[18:42.540 --> 18:42.880]  and,\n[18:43.020 --> 18:43.280]  but.\n[18:43.500 --> 18:43.900]  So there\n[18:43.900 --> 18:44.460]  is a list\n[18:44.460 --> 18:44.920]  of stop\n[18:44.920 --> 18:45.300]  words.\n[18:47.680 --> 18:48.500]  So this is\n[18:48.500 --> 18:49.300]  just based on\n[18:49.300 --> 18:49.940]  a lookup to\n[18:49.940 --> 18:50.120]  say,\n[18:50.220 --> 18:50.680]  is this word\n[18:50.680 --> 18:50.940]  in the\n[18:50.940 --> 18:51.460]  stop words?\n[18:51.460 --> 18:51.940]  if it is,\n[18:52.000 --> 18:52.280]  we throw\n[18:52.280 --> 18:52.580]  it out\n[18:52.580 --> 18:53.540]  and don't\n[18:53.540 --> 18:54.840]  add it to\n[18:54.840 --> 18:55.420]  our vector\n[18:55.420 --> 18:55.940]  representation.\n[18:57.540 --> 18:57.920]  Now,\n[18:58.780 --> 18:59.920]  stemming and\n[18:59.920 --> 19:00.700]  lemmatization.\n[19:04.280 --> 19:04.840]  Lemmatization\n[19:04.840 --> 19:05.900]  requires a\n[19:05.900 --> 19:06.460]  thesaurus.\n[19:07.920 --> 19:08.360]  Stemming\n[19:08.360 --> 19:09.740]  algorithmically\n[19:09.740 --> 19:12.500]  changes words.\n[19:13.580 --> 19:13.700]  Right?\n[19:13.800 --> 19:14.440]  So am\n[19:14.440 --> 19:16.000]  will become\n[19:16.000 --> 19:16.440]  be\n[19:16.440 --> 19:17.240]  in\n[19:17.240 --> 19:18.020]  lemmatization.\n[19:20.660 --> 19:21.340]  And\n[19:21.340 --> 19:22.220]  remains am\n[19:22.220 --> 19:23.100]  from stemming.\n[19:23.580 --> 19:23.900]  Going,\n[19:24.280 --> 19:25.460]  remains going\n[19:25.460 --> 19:26.140]  in stemming,\n[19:26.280 --> 19:27.300]  but becomes\n[19:27.300 --> 19:28.080]  go in,\n[19:28.980 --> 19:29.240]  sorry,\n[19:29.760 --> 19:30.040]  become,\n[19:30.340 --> 19:30.560]  going,\n[19:30.660 --> 19:31.340]  remains going\n[19:31.340 --> 19:32.420]  in lemmatization,\n[19:32.920 --> 19:33.520]  but becomes\n[19:33.520 --> 19:34.400]  go in stemming.\n[19:35.080 --> 19:35.880]  And having,\n[19:36.440 --> 19:37.580]  becomes have\n[19:37.580 --> 19:38.740]  in lemmatization,\n[19:38.740 --> 19:40.140]  where it\n[19:40.140 --> 19:40.540]  becomes\n[19:40.540 --> 19:41.480]  H-A-V\n[19:41.480 --> 19:42.460]  in stemming.\n[19:43.020 --> 19:43.240]  Now,\n[19:43.360 --> 19:43.800]  stemming\n[19:43.800 --> 19:44.660]  typically\n[19:44.660 --> 19:45.980]  can result\n[19:45.980 --> 19:46.520]  in\n[19:46.520 --> 19:47.880]  these kind\n[19:47.880 --> 19:48.060]  of\n[19:48.060 --> 19:49.020]  non-English\n[19:49.020 --> 19:49.620]  words\n[19:49.620 --> 19:50.320]  while\n[19:50.320 --> 19:51.440]  identifying\n[19:51.440 --> 19:52.000]  the root.\n[19:53.160 --> 19:53.560]  And so\n[19:53.560 --> 19:54.140]  it takes\n[19:54.140 --> 19:54.560]  away a\n[19:54.560 --> 19:55.000]  little bit\n[19:55.000 --> 19:55.400]  from\n[19:55.400 --> 19:55.900]  the\n[19:55.900 --> 19:56.600]  understanding\n[19:56.600 --> 19:57.120]  of\n[19:57.120 --> 19:58.880]  what is\n[19:58.880 --> 20:00.220]  the word\n[20:00.220 --> 20:00.880]  that is\n[20:00.880 --> 20:01.380]  important\n[20:01.380 --> 20:01.840]  within a\n[20:01.840 --> 20:02.200]  document\n[20:02.200 --> 20:02.660]  once you\n[20:02.660 --> 20:04.520]  stemmed\n[20:04.520 --> 20:05.100]  it.\n[20:05.680 --> 20:06.120]  So just\n[20:06.120 --> 20:06.600]  be aware\n[20:06.600 --> 20:07.060]  of this,\n[20:07.160 --> 20:07.580]  but these\n[20:07.580 --> 20:08.120]  are ways\n[20:08.120 --> 20:08.500]  that were\n[20:08.500 --> 20:09.100]  used\n[20:09.100 --> 20:09.880]  in the\n[20:09.880 --> 20:10.380]  past,\n[20:11.380 --> 20:12.140]  and so\n[20:12.140 --> 20:12.380]  it's\n[20:12.380 --> 20:12.680]  less\n[20:12.680 --> 20:13.060]  relevant\n[20:13.060 --> 20:13.500]  in any\n[20:13.500 --> 20:13.760]  case\n[20:13.760 --> 20:14.180]  at this\n[20:14.180 --> 20:14.580]  point\n[20:14.580 --> 20:17.520]  to\n[20:17.520 --> 20:18.300]  reduce\n[20:18.300 --> 20:18.580]  the\n[20:18.580 --> 20:19.220]  vocabulary\n[20:19.220 --> 20:19.660]  size.\n[20:19.740 --> 20:19.980]  And I'll\n[20:19.980 --> 20:20.240]  tell you\n[20:20.240 --> 20:20.760]  why it's\n[20:20.760 --> 20:21.000]  become\n[20:21.000 --> 20:21.260]  less\n[20:21.260 --> 20:21.580]  relevant\n[20:21.580 --> 20:22.240]  later on.\n[20:24.060 --> 20:24.380]  Okay.\n[20:31.380 --> 20:32.020]  Now,\n[20:32.300 --> 20:33.280]  we talked\n[20:33.280 --> 20:33.840]  about the\n[20:33.840 --> 20:34.320]  syntactic\n[20:34.320 --> 20:35.200]  representation,\n[20:35.580 --> 20:35.900]  and we've\n[20:35.900 --> 20:36.400]  talked about\n[20:36.400 --> 20:37.340]  TF-IDF.\n[20:37.340 --> 20:38.000]  so once\n[20:38.000 --> 20:38.400]  we have\n[20:38.400 --> 20:39.100]  identified\n[20:39.100 --> 20:39.620]  our\n[20:39.620 --> 20:40.240]  vocabulary,\n[20:40.500 --> 20:40.660]  whether\n[20:40.660 --> 20:41.040]  this is\n[20:41.040 --> 20:41.580]  a word\n[20:41.580 --> 20:42.940]  or a\n[20:42.940 --> 20:43.140]  key\n[20:43.140 --> 20:43.560]  phrase,\n[20:44.100 --> 20:46.000]  we need\n[20:46.000 --> 20:46.580]  to assign\n[20:46.580 --> 20:46.940]  it a\n[20:46.940 --> 20:47.260]  weight,\n[20:47.780 --> 20:48.220]  and that\n[20:48.220 --> 20:48.700]  one way\n[20:48.700 --> 20:49.220]  of assigning\n[20:49.220 --> 20:49.660]  the weight\n[20:49.660 --> 20:50.720]  is TF-IDF,\n[20:50.780 --> 20:51.280]  other than\n[20:51.280 --> 20:51.840]  just looking\n[20:51.840 --> 20:52.100]  at the\n[20:52.100 --> 20:52.440]  count,\n[20:52.680 --> 20:53.800]  or a\n[20:53.800 --> 20:54.160]  binary\n[20:54.160 --> 20:54.940]  representation,\n[20:55.340 --> 20:55.640]  or one\n[20:55.640 --> 20:56.020]  of zeros.\n[20:56.020 --> 20:56.200]  those.\n[20:59.080 --> 20:59.820]  So let's\n[20:59.820 --> 21:00.180]  look at a\n[21:00.180 --> 21:00.660]  very simple\n[21:00.660 --> 21:01.160]  example.\n[21:01.320 --> 21:01.660]  We have\n[21:01.660 --> 21:02.200]  these eight\n[21:02.200 --> 21:02.800]  documents\n[21:02.800 --> 21:03.240]  here.\n[21:03.860 --> 21:04.140]  We've\n[21:04.140 --> 21:04.600]  actually just\n[21:04.600 --> 21:04.960]  got the\n[21:04.960 --> 21:05.600]  titles of\n[21:05.600 --> 21:05.800]  these\n[21:05.800 --> 21:06.220]  documents,\n[21:06.220 --> 21:06.680]  and we\n[21:06.680 --> 21:07.260]  can see\n[21:07.260 --> 21:07.840]  by looking\n[21:07.840 --> 21:08.380]  at this\n[21:08.380 --> 21:10.780]  that the\n[21:10.780 --> 21:11.300]  first few\n[21:11.300 --> 21:12.000]  documents\n[21:12.000 --> 21:13.820]  seem to\n[21:13.820 --> 21:14.680]  be about\n[21:14.680 --> 21:18.520]  human-machine\n[21:18.520 --> 21:19.200]  interface.\n[21:19.200 --> 21:24.480]  the last\n[21:24.480 --> 21:24.720]  three\n[21:24.720 --> 21:25.240]  documents\n[21:25.240 --> 21:26.200]  here are\n[21:26.200 --> 21:26.580]  about\n[21:26.580 --> 21:27.060]  graph\n[21:27.060 --> 21:27.420]  theory.\n[21:31.200 --> 21:32.080]  We\n[21:32.080 --> 21:32.840]  studied how\n[21:32.840 --> 21:33.440]  to analyze\n[21:33.440 --> 21:33.960]  graphs,\n[21:34.560 --> 21:35.260]  so these\n[21:35.260 --> 21:35.840]  are related\n[21:35.840 --> 21:36.900]  to that\n[21:36.900 --> 21:37.220]  topic.\n[21:40.340 --> 21:41.020]  Now,\n[21:41.280 --> 21:41.700]  this is\n[21:41.700 --> 21:42.440]  our corpus,\n[21:42.740 --> 21:43.360]  all of it,\n[21:44.260 --> 21:44.860]  and so we\n[21:44.860 --> 21:45.560]  can convert\n[21:45.560 --> 21:46.400]  it into a\n[21:46.400 --> 21:47.000]  vectorized\n[21:47.000 --> 21:48.420]  form in\n[21:48.420 --> 21:48.860]  this way\n[21:48.860 --> 21:49.140]  here.\n[21:49.200 --> 21:52.760]  Now,\n[21:52.840 --> 21:53.200]  what I've\n[21:53.200 --> 21:53.640]  been talking\n[21:53.640 --> 21:54.220]  about is\n[21:54.220 --> 21:54.680]  that every\n[21:54.680 --> 21:56.000]  word becomes\n[21:56.000 --> 21:56.620]  a column.\n[21:56.960 --> 21:57.200]  I have\n[21:57.200 --> 21:57.680]  taken the\n[21:57.680 --> 21:58.600]  transpose of\n[21:58.600 --> 21:58.860]  that.\n[22:00.100 --> 22:00.740]  And so\n[22:00.740 --> 22:01.280]  it doesn't\n[22:01.280 --> 22:01.880]  really matter.\n[22:02.020 --> 22:02.280]  What we\n[22:02.280 --> 22:02.580]  are now\n[22:02.580 --> 22:03.140]  seeing is\n[22:03.140 --> 22:03.500]  that a\n[22:03.500 --> 22:04.080]  document\n[22:04.080 --> 22:05.820]  has a\n[22:05.820 --> 22:06.160]  vector\n[22:06.160 --> 22:07.100]  representation\n[22:07.100 --> 22:08.460]  that looks\n[22:08.460 --> 22:09.100]  like this.\n[22:10.380 --> 22:10.860]  Now,\n[22:10.920 --> 22:11.340]  to make it\n[22:11.340 --> 22:11.640]  easier,\n[22:11.720 --> 22:12.040]  I've taken\n[22:12.040 --> 22:12.600]  a subset\n[22:12.600 --> 22:13.020]  of the\n[22:13.020 --> 22:13.540]  vocabulary.\n[22:14.200 --> 22:14.460]  We are\n[22:14.460 --> 22:14.940]  taking the\n[22:14.940 --> 22:16.100]  vocabulary to\n[22:16.100 --> 22:17.000]  only be\n[22:17.000 --> 22:18.080]  words,\n[22:18.220 --> 22:18.660]  not key\n[22:18.660 --> 22:19.060]  phrases.\n[22:19.200 --> 22:19.940]  And so\n[22:19.940 --> 22:20.280]  we have\n[22:20.280 --> 22:20.520]  1,\n[22:20.600 --> 22:20.740]  2,\n[22:20.740 --> 22:21.100]  3,\n[22:21.220 --> 22:21.480]  4,\n[22:21.580 --> 22:21.880]  5,\n[22:21.880 --> 22:22.260]  6,\n[22:22.260 --> 22:22.640]  7,\n[22:22.640 --> 22:23.080]  8,\n[22:23.080 --> 22:23.440]  9,\n[22:23.440 --> 22:23.780]  10,\n[22:23.900 --> 22:24.140]  11,\n[22:24.240 --> 22:24.560]  12.\n[22:25.340 --> 22:26.000]  A vocabulary\n[22:26.000 --> 22:26.560]  of size\n[22:26.560 --> 22:27.280]  12 here,\n[22:27.880 --> 22:28.640]  and you\n[22:28.640 --> 22:29.120]  can see\n[22:29.120 --> 22:29.700]  that we\n[22:29.700 --> 22:30.040]  have\n[22:30.040 --> 22:31.640]  count\n[22:31.640 --> 22:32.480]  vectorized\n[22:32.480 --> 22:33.180]  each of\n[22:33.180 --> 22:33.360]  our\n[22:33.360 --> 22:33.840]  documents.\n[22:33.840 --> 22:33.940]  words.\n[22:35.140 --> 22:35.540]  Now,\n[22:35.540 --> 22:36.300]  if we\n[22:36.300 --> 22:36.900]  look at\n[22:36.900 --> 22:39.300]  the word\n[22:39.300 --> 22:39.840]  human\n[22:39.840 --> 22:43.940]  in\n[22:43.940 --> 22:44.380]  document\n[22:44.380 --> 22:44.840]  1,\n[22:45.380 --> 22:46.200]  if we\n[22:46.200 --> 22:46.340]  were\n[22:46.340 --> 22:46.840]  calculating\n[22:46.840 --> 22:47.160]  the\n[22:47.160 --> 22:48.080]  TF-IDF,\n[22:48.160 --> 22:48.440]  we would\n[22:48.440 --> 22:48.840]  say,\n[22:49.000 --> 22:49.300]  okay,\n[22:49.860 --> 22:50.940]  human has\n[22:50.940 --> 22:51.680]  a frequency\n[22:51.680 --> 22:52.420]  of 1.\n[22:53.800 --> 22:54.400]  There are\n[22:54.400 --> 22:54.780]  1,\n[22:54.960 --> 22:55.240]  2,\n[22:55.380 --> 22:55.820]  3,\n[22:55.820 --> 22:56.240]  4,\n[22:58.440 --> 22:58.900]  5,\n[22:58.900 --> 22:59.840]  6 words.\n[23:00.000 --> 23:00.480]  This is a\n[23:00.480 --> 23:01.080]  stock word.\n[23:03.140 --> 23:04.000]  So we have\n[23:04.000 --> 23:04.680]  removed it.\n[23:06.980 --> 23:08.000]  And I can only\n[23:08.000 --> 23:09.040]  see the 3 words\n[23:09.040 --> 23:09.260]  here.\n[23:09.340 --> 23:09.920]  The rest of the\n[23:09.920 --> 23:10.540]  words I have\n[23:10.540 --> 23:11.840]  kind of cut off\n[23:11.840 --> 23:12.480]  because I didn't\n[23:12.480 --> 23:13.240]  have enough space\n[23:13.240 --> 23:13.820]  to show it.\n[23:13.820 --> 23:15.560]  So 1 out\n[23:15.560 --> 23:16.920]  of 6\n[23:16.920 --> 23:18.020]  is the\n[23:18.020 --> 23:18.720]  frequency\n[23:18.720 --> 23:19.420]  part of\n[23:19.420 --> 23:19.640]  this.\n[23:19.720 --> 23:20.260]  So this is\n[23:20.260 --> 23:21.000]  the TF,\n[23:21.720 --> 23:22.280]  the term\n[23:22.280 --> 23:22.900]  frequency.\n[23:23.960 --> 23:24.940]  And here\n[23:24.940 --> 23:26.200]  8 is\n[23:26.200 --> 23:27.180]  the number\n[23:27.180 --> 23:28.100]  of documents.\n[23:30.920 --> 23:31.700]  And we\n[23:31.700 --> 23:32.200]  can see\n[23:32.200 --> 23:32.740]  human\n[23:32.740 --> 23:34.120]  appears in\n[23:34.120 --> 23:34.800]  document 1\n[23:34.800 --> 23:35.380]  and document\n[23:35.380 --> 23:35.860]  4.\n[23:36.040 --> 23:36.460]  So it's\n[23:36.460 --> 23:36.900]  document\n[23:36.900 --> 23:37.720]  frequency\n[23:37.720 --> 23:43.860]  is 2.\n[23:45.640 --> 23:46.240]  And so\n[23:46.240 --> 23:47.140]  the way\n[23:47.140 --> 23:47.680]  we calculate\n[23:47.680 --> 23:48.020]  TF,\n[23:48.060 --> 23:48.840]  IDF is\n[23:48.840 --> 23:49.220]  term\n[23:49.220 --> 23:50.000]  frequency\n[23:50.000 --> 23:52.580]  inverse\n[23:52.580 --> 23:53.120]  document\n[23:53.120 --> 23:53.740]  frequency.\n[23:53.900 --> 23:54.300]  So 2\n[23:54.300 --> 23:55.020]  by 8 is\n[23:55.020 --> 23:55.520]  the document\n[23:55.520 --> 23:56.240]  frequency.\n[23:56.420 --> 23:56.760]  We make\n[23:56.760 --> 23:57.160]  it 8\n[23:57.160 --> 23:57.740]  by 2.\n[23:58.840 --> 23:59.580]  And we\n[23:59.580 --> 24:00.020]  get this\n[24:00.020 --> 24:00.600]  value of\n[24:00.600 --> 24:01.500]  0.23.\n[24:01.780 --> 24:02.700]  So if\n[24:02.700 --> 24:03.380]  we wanted\n[24:03.380 --> 24:04.300]  to now\n[24:04.300 --> 24:04.940]  instead\n[24:04.940 --> 24:05.920]  represent\n[24:05.920 --> 24:07.380]  our documents\n[24:07.380 --> 24:08.160]  as vectors\n[24:08.160 --> 24:09.200]  of TF,\n[24:09.240 --> 24:09.680]  IDF\n[24:09.680 --> 24:10.140]  scores,\n[24:10.800 --> 24:11.320]  we can\n[24:11.320 --> 24:11.960]  calculate\n[24:11.960 --> 24:12.280]  them.\n[24:12.500 --> 24:12.820]  So the\n[24:12.820 --> 24:14.180]  first step\n[24:14.180 --> 24:14.700]  really of\n[24:14.700 --> 24:15.280]  calculating\n[24:15.280 --> 24:15.780]  the TF,\n[24:15.840 --> 24:16.600]  IDF is\n[24:16.600 --> 24:16.980]  to really\n[24:16.980 --> 24:17.280]  do a\n[24:17.280 --> 24:17.480]  counter\n[24:17.480 --> 24:18.360]  vectorization\n[24:18.360 --> 24:18.900]  of the\n[24:18.900 --> 24:19.260]  documents.\n[24:21.840 --> 24:22.900]  Now I'm\n[24:22.900 --> 24:23.120]  going to\n[24:23.120 --> 24:23.720]  skip this\n[24:23.720 --> 24:24.220]  because we\n[24:24.220 --> 24:24.800]  looked at\n[24:24.800 --> 24:25.900]  this in\n[24:25.900 --> 24:26.960]  principal\n[24:26.960 --> 24:27.440]  component\n[24:27.440 --> 24:28.100]  analysis.\n[24:28.800 --> 24:29.580]  But what\n[24:29.580 --> 24:29.900]  I'm going\n[24:29.900 --> 24:30.280]  to move\n[24:30.280 --> 24:30.840]  on to\n[24:30.840 --> 24:33.100]  is what\n[24:33.100 --> 24:34.000]  is singular\n[24:34.000 --> 24:34.520]  value\n[24:34.520 --> 24:35.560]  decomposition.\n[24:35.560 --> 24:37.560]  singular\n[24:37.560 --> 24:37.900]  value\n[24:37.900 --> 24:38.480]  decomposition\n[24:38.480 --> 24:39.680]  is a\n[24:39.680 --> 24:40.100]  linear\n[24:40.100 --> 24:41.340]  algebra\n[24:41.340 --> 24:43.960]  method\n[24:43.960 --> 24:52.340]  and is\n[24:52.340 --> 24:53.220]  based around\n[24:53.220 --> 24:53.980]  eigenvalue\n[24:53.980 --> 24:54.620]  decomposition.\n[24:55.900 --> 24:56.760]  And this\n[24:56.760 --> 24:57.460]  was the\n[24:57.460 --> 24:57.960]  first\n[24:57.960 --> 24:59.380]  semantic\n[24:59.380 --> 25:00.380]  vectorization\n[25:00.380 --> 25:00.920]  approach\n[25:00.920 --> 25:01.300]  that was\n[25:01.300 --> 25:01.760]  proposed.\n[25:01.760 --> 25:06.980]  Now\n[25:06.980 --> 25:07.500]  singular\n[25:07.500 --> 25:07.760]  value\n[25:07.760 --> 25:08.320]  decomposition\n[25:08.320 --> 25:09.420]  was the\n[25:09.420 --> 25:09.820]  method\n[25:09.820 --> 25:10.740]  used for\n[25:10.740 --> 25:11.100]  doing\n[25:11.100 --> 25:11.960]  semantic\n[25:11.960 --> 25:12.920]  vectorization.\n[25:13.420 --> 25:14.080]  When applied\n[25:14.080 --> 25:15.280]  to textual\n[25:15.280 --> 25:15.860]  documents,\n[25:16.040 --> 25:16.320]  it was\n[25:16.320 --> 25:16.920]  known as\n[25:16.920 --> 25:17.580]  latent\n[25:17.580 --> 25:20.160]  semantic\n[25:20.160 --> 25:21.080]  analysis.\n[25:21.080 --> 25:23.080]  singular\n[25:23.080 --> 25:23.420]  value\n[25:23.420 --> 25:24.080]  decomposition.\n[25:34.080 --> 25:34.960]  So what\n[25:34.960 --> 25:35.420]  was singular\n[25:35.420 --> 25:35.720]  value\n[25:35.720 --> 25:36.320]  decomposition\n[25:36.320 --> 25:37.020]  first of\n[25:37.020 --> 25:37.200]  all?\n[25:39.400 --> 25:40.040]  This\n[25:40.040 --> 25:41.060]  matrix\n[25:41.060 --> 25:41.440]  that I\n[25:41.440 --> 25:41.760]  had\n[25:41.760 --> 25:42.400]  X\n[25:42.400 --> 25:44.120]  where we\n[25:44.120 --> 25:44.560]  had\n[25:44.560 --> 25:45.160]  word\n[25:45.160 --> 25:45.520]  one,\n[25:45.880 --> 25:46.140]  word\n[25:46.140 --> 25:46.620]  two,\n[25:47.760 --> 25:47.940]  till\n[25:47.940 --> 25:48.420]  word\n[25:48.420 --> 25:49.460]  M,\n[25:50.260 --> 25:51.020]  and we\n[25:51.020 --> 25:51.340]  had\n[25:51.340 --> 25:52.020]  D1,\n[25:52.220 --> 25:52.740]  D2,\n[25:53.420 --> 25:53.640]  to\n[25:53.640 --> 25:54.400]  Dn.\n[25:56.540 --> 25:57.160]  We are\n[25:57.160 --> 25:57.460]  going to\n[25:57.460 --> 25:57.940]  refer to\n[25:57.940 --> 25:58.400]  this as\n[25:58.400 --> 25:59.140]  X.\n[26:01.220 --> 26:02.040]  And so\n[26:02.040 --> 26:02.400]  when we\n[26:02.400 --> 26:03.100]  go X\n[26:03.100 --> 26:03.960]  transpose,\n[26:04.360 --> 26:04.700]  X\n[26:04.700 --> 26:05.620]  transpose is\n[26:05.620 --> 26:06.000]  going to\n[26:06.000 --> 26:06.380]  be an\n[26:06.380 --> 26:07.060]  N by\n[26:07.060 --> 26:07.800]  M\n[26:07.800 --> 26:08.880]  matrix\n[26:08.880 --> 26:10.560]  multiplied\n[26:10.560 --> 26:11.820]  by an\n[26:11.820 --> 26:12.280]  M\n[26:12.280 --> 26:13.340]  by N\n[26:13.340 --> 26:13.800]  matrix.\n[26:13.960 --> 26:14.240]  So what\n[26:14.240 --> 26:14.440]  are we\n[26:14.440 --> 26:14.700]  going to\n[26:14.700 --> 26:15.140]  end up\n[26:15.140 --> 26:15.420]  with?\n[26:15.420 --> 26:16.160]  We are\n[26:16.160 --> 26:16.260]  going to\n[26:16.260 --> 26:16.640]  end up\n[26:16.640 --> 26:17.020]  with a\n[26:17.020 --> 26:17.840]  N by\n[26:17.840 --> 26:18.180]  N\n[26:18.180 --> 26:18.620]  matrix.\n[26:22.840 --> 26:23.580]  When we\n[26:23.580 --> 26:23.960]  were doing\n[26:23.960 --> 26:24.420]  principal\n[26:24.420 --> 26:24.940]  component\n[26:24.940 --> 26:25.540]  analysis,\n[26:26.380 --> 26:27.480]  what we\n[26:27.480 --> 26:28.100]  did was\n[26:28.100 --> 26:28.660]  we said\n[26:28.660 --> 26:29.940]  we have\n[26:29.940 --> 26:34.580]  X1,\n[26:34.640 --> 26:35.040]  X2,\n[26:35.140 --> 26:35.980]  to XN,\n[26:36.180 --> 26:36.620]  and we\n[26:36.620 --> 26:37.460]  had various\n[26:37.460 --> 26:38.040]  objects,\n[26:38.240 --> 26:38.660]  O1,\n[26:38.720 --> 26:38.880]  2,\n[26:38.960 --> 26:39.140]  O,\n[26:39.200 --> 26:39.460]  M.\n[26:42.900 --> 26:43.980]  And we\n[26:43.980 --> 26:44.560]  first\n[26:44.560 --> 26:45.160]  computed,\n[26:45.420 --> 26:45.940]  the\n[26:45.940 --> 26:46.720]  covariance\n[26:46.720 --> 26:47.160]  matrix.\n[26:51.600 --> 26:52.640]  And the\n[26:52.640 --> 26:53.220]  covariance\n[26:53.220 --> 26:54.240]  matrix was\n[26:54.240 --> 26:55.360]  an N\n[26:55.360 --> 26:56.040]  by N\n[26:56.040 --> 26:56.680]  matrix,\n[26:57.580 --> 27:00.500]  which,\n[27:01.340 --> 27:01.980]  given that\n[27:01.980 --> 27:02.560]  we have\n[27:02.560 --> 27:03.260]  these\n[27:03.260 --> 27:05.020]  variables\n[27:05.020 --> 27:05.780]  describing\n[27:05.780 --> 27:06.140]  each\n[27:06.140 --> 27:06.740]  object,\n[27:08.200 --> 27:09.040]  it\n[27:09.040 --> 27:09.540]  stored\n[27:09.540 --> 27:09.960]  the\n[27:09.960 --> 27:10.920]  variance\n[27:10.920 --> 27:11.380]  along\n[27:11.380 --> 27:11.620]  the\n[27:11.620 --> 27:12.080]  diagonal\n[27:12.080 --> 27:13.180]  of the\n[27:13.180 --> 27:13.700]  matrix,\n[27:13.700 --> 27:15.400]  and the\n[27:15.400 --> 27:16.360]  covariance\n[27:16.360 --> 27:18.740]  in the\n[27:18.740 --> 27:20.080]  non-diagonal\n[27:20.080 --> 27:21.240]  elements here.\n[27:22.720 --> 27:23.320]  Right?\n[27:23.400 --> 27:23.820]  So this\n[27:23.820 --> 27:24.640]  here was\n[27:24.640 --> 27:25.400]  the variance\n[27:25.400 --> 27:25.940]  of Xi,\n[27:26.440 --> 27:29.040]  and this,\n[27:30.080 --> 27:30.620]  depending on\n[27:30.620 --> 27:32.320]  which row\n[27:32.320 --> 27:32.740]  and column\n[27:32.740 --> 27:33.400]  it was,\n[27:34.000 --> 27:34.500]  was the\n[27:34.500 --> 27:35.520]  covariance\n[27:35.520 --> 27:36.560]  of Xi\n[27:36.560 --> 27:36.820]  and\n[27:36.820 --> 27:37.720]  Xi.\n[27:40.100 --> 27:41.020]  So here\n[27:41.020 --> 27:41.620]  also we've\n[27:41.620 --> 27:42.020]  got an\n[27:42.020 --> 27:42.700]  N by N\n[27:42.700 --> 27:43.140]  matrix.\n[27:44.140 --> 27:45.080]  So what\n[27:45.080 --> 27:45.600]  can we\n[27:45.600 --> 27:47.100]  kind of\n[27:47.100 --> 27:47.900]  visualize\n[27:47.900 --> 27:48.740]  this as?\n[27:49.560 --> 27:50.300]  We can\n[27:50.300 --> 27:51.480]  visualize it\n[27:51.480 --> 27:52.380]  that we\n[27:52.380 --> 27:53.020]  are treating\n[27:53.020 --> 27:54.020]  this matrix\n[27:54.020 --> 27:54.620]  X\n[27:54.620 --> 27:57.500]  as\n[27:57.500 --> 27:58.300]  each\n[27:58.300 --> 27:58.840]  word\n[27:58.840 --> 28:00.900]  being an\n[28:00.900 --> 28:01.380]  object\n[28:01.380 --> 28:03.620]  that is\n[28:03.620 --> 28:04.540]  represented\n[28:04.540 --> 28:05.760]  by its\n[28:05.760 --> 28:06.840]  occurrence\n[28:06.840 --> 28:07.480]  within\n[28:07.480 --> 28:08.640]  the N\n[28:08.640 --> 28:09.380]  document.\n[28:09.580 --> 28:10.300]  So the\n[28:10.300 --> 28:11.820]  documents\n[28:11.820 --> 28:12.760]  themselves\n[28:12.760 --> 28:14.160]  are now\n[28:14.160 --> 28:14.960]  our\n[28:14.960 --> 28:15.740]  variables\n[28:15.740 --> 28:18.260]  that\n[28:18.260 --> 28:19.140]  in some\n[28:19.140 --> 28:19.500]  way are\n[28:19.500 --> 28:20.200]  describing\n[28:20.200 --> 28:20.780]  our word.\n[28:24.280 --> 28:25.340]  And so\n[28:25.340 --> 28:26.220]  this\n[28:26.220 --> 28:27.780]  X\n[28:27.780 --> 28:28.540]  transpose\n[28:28.540 --> 28:29.280]  X,\n[28:29.280 --> 28:30.040]  if we\n[28:30.040 --> 28:30.440]  do an\n[28:30.440 --> 28:31.580]  eigenvector\n[28:31.580 --> 28:32.320]  decomposition\n[28:32.320 --> 28:32.920]  on it,\n[28:32.980 --> 28:33.320]  which is\n[28:33.320 --> 28:33.860]  essentially\n[28:33.860 --> 28:34.920]  the same\n[28:34.920 --> 28:35.440]  as what\n[28:35.440 --> 28:35.720]  we do\n[28:35.720 --> 28:35.880]  in\n[28:35.880 --> 28:36.200]  principal\n[28:36.200 --> 28:36.680]  component\n[28:36.680 --> 28:37.280]  analysis,\n[28:37.980 --> 28:39.500]  and we\n[28:39.500 --> 28:40.140]  have\n[28:40.140 --> 28:40.920]  our\n[28:40.920 --> 28:46.020]  eigenvectors.\n[28:47.300 --> 28:47.860]  Right?\n[28:50.100 --> 28:50.960]  What that\n[28:50.960 --> 28:51.520]  means is\n[28:51.520 --> 28:51.980]  for every\n[28:51.980 --> 28:52.440]  one of\n[28:52.440 --> 28:52.620]  these\n[28:52.620 --> 28:53.480]  eigenvectors,\n[28:53.700 --> 28:53.980]  X\n[28:53.980 --> 28:54.480]  transpose\n[28:54.480 --> 28:55.240]  X\n[28:55.240 --> 28:56.460]  times\n[28:56.460 --> 28:56.900]  VI\n[28:56.900 --> 28:57.820]  is equal\n[28:57.820 --> 28:58.180]  to\n[28:58.180 --> 28:58.640]  lambda\n[28:58.640 --> 28:59.080]  I.\n[28:59.280 --> 28:59.480]  VI.\n[28:59.640 --> 29:00.000]  This is\n[29:00.000 --> 29:00.500]  by just\n[29:00.500 --> 29:00.820]  the\n[29:00.820 --> 29:01.780]  definition\n[29:01.780 --> 29:02.260]  of\n[29:02.260 --> 29:04.980]  an\n[29:04.980 --> 29:05.760]  eigenvector\n[29:05.760 --> 29:08.960]  and\n[29:08.960 --> 29:11.080]  eigenvalue\n[29:11.080 --> 29:12.460]  which is\n[29:12.460 --> 29:13.180]  lambda I.\n[29:16.060 --> 29:17.060]  Now let\n[29:17.060 --> 29:17.440]  us\n[29:17.440 --> 29:19.020]  define\n[29:19.020 --> 29:19.960]  R\n[29:19.960 --> 29:20.860]  vectors\n[29:20.860 --> 29:21.940]  U1\n[29:21.940 --> 29:23.140]  to UR\n[29:23.140 --> 29:25.340]  as\n[29:25.340 --> 29:26.460]  this here.\n[29:26.460 --> 29:27.560]  So we\n[29:27.560 --> 29:28.180]  are taking\n[29:28.180 --> 29:29.560]  the\n[29:29.560 --> 29:30.160]  X\n[29:30.160 --> 29:31.280]  matrix,\n[29:31.280 --> 29:32.240]  which is\n[29:32.240 --> 29:32.520]  this\n[29:32.520 --> 29:33.060]  matrix\n[29:33.060 --> 29:33.420]  here.\n[29:34.380 --> 29:35.120]  We are\n[29:35.120 --> 29:35.720]  multiplying\n[29:35.720 --> 29:36.340]  by\n[29:36.340 --> 29:38.080]  VI,\n[29:38.740 --> 29:39.640]  which is\n[29:39.640 --> 29:40.040]  the\n[29:40.040 --> 29:40.920]  eigenvector\n[29:40.920 --> 29:41.860]  of X\n[29:41.860 --> 29:42.280]  transpose\n[29:42.280 --> 29:42.860]  X.\n[29:44.180 --> 29:45.280]  And we\n[29:45.280 --> 29:45.960]  are multiplying\n[29:45.960 --> 29:46.920]  by 1\n[29:46.920 --> 29:47.740]  divided by\n[29:47.740 --> 29:48.380]  the square\n[29:48.380 --> 29:49.040]  root of\n[29:49.040 --> 29:50.260]  lambda I.\n[29:50.260 --> 29:52.900]  it turns\n[29:52.900 --> 29:53.460]  out that\n[29:53.460 --> 29:53.860]  these\n[29:53.860 --> 29:54.680]  UIs are\n[29:54.680 --> 29:55.460]  also\n[29:55.460 --> 29:56.740]  orthonormal.\n[29:57.700 --> 29:58.620]  Why am I\n[29:58.620 --> 29:59.360]  saying also?\n[29:59.560 --> 30:00.580]  Because these\n[30:00.580 --> 30:01.680]  eigenvectors are\n[30:01.680 --> 30:02.380]  orthonormal.\n[30:02.560 --> 30:03.180]  So these\n[30:03.180 --> 30:03.940]  sets of\n[30:03.940 --> 30:04.820]  vectors are\n[30:04.820 --> 30:07.640]  orthonormal\n[30:07.640 --> 30:08.840]  as well.\n[30:08.840 --> 30:15.800]  So if\n[30:15.800 --> 30:16.460]  I now\n[30:16.460 --> 30:17.240]  say\n[30:17.240 --> 30:19.360]  X\n[30:19.360 --> 30:21.240]  and I\n[30:21.240 --> 30:22.160]  essentially\n[30:22.160 --> 30:23.100]  put B1\n[30:23.100 --> 30:23.600]  B2\n[30:23.600 --> 30:24.240]  to BR\n[30:24.240 --> 30:26.080]  as\n[30:26.080 --> 30:26.960]  columns\n[30:26.960 --> 30:27.840]  within a\n[30:27.840 --> 30:28.300]  matrix\n[30:28.300 --> 30:31.800]  and I\n[30:31.800 --> 30:32.300]  call this\n[30:32.300 --> 30:32.800]  matrix\n[30:32.800 --> 30:33.200]  V.\n[30:34.540 --> 30:35.540]  Then what\n[30:35.540 --> 30:36.080]  we have\n[30:36.080 --> 30:36.820]  is X\n[30:36.820 --> 30:37.720]  times V\n[30:37.720 --> 30:39.540]  is equal\n[30:39.540 --> 30:41.020]  to\n[30:41.020 --> 30:41.620]  U\n[30:41.620 --> 30:43.460]  lambda.\n[30:44.260 --> 30:44.840]  Where what\n[30:44.840 --> 30:45.320]  are U?\n[30:45.440 --> 30:46.020]  U R,\n[30:46.360 --> 30:46.880]  U 1,\n[30:47.100 --> 30:47.680]  U 2,\n[30:48.100 --> 30:48.360]  2,\n[30:48.560 --> 30:49.500]  U R.\n[30:53.080 --> 30:53.780]  And\n[30:53.780 --> 30:55.340]  lambda\n[30:55.340 --> 30:56.560]  here is\n[30:56.560 --> 30:57.540]  lambda 1,\n[30:57.680 --> 30:58.600]  lambda 2\n[30:58.600 --> 31:00.140]  to\n[31:00.140 --> 31:00.940]  lambda\n[31:00.940 --> 31:04.300]  R.\n[31:04.800 --> 31:05.440]  R.\n[31:07.720 --> 31:13.940]  right?\n[31:14.020 --> 31:14.240]  And it's\n[31:14.240 --> 31:14.700]  the square\n[31:14.700 --> 31:15.240]  root of\n[31:15.240 --> 31:15.860]  the lambda.\n[31:16.100 --> 31:16.300]  Sorry.\n[31:17.160 --> 31:17.460]  This should\n[31:17.460 --> 31:17.960]  be square\n[31:17.960 --> 31:18.300]  root of\n[31:18.300 --> 31:18.480]  that.\n[31:18.780 --> 31:19.120]  So we've\n[31:19.120 --> 31:19.740]  taken this\n[31:19.740 --> 31:21.240]  lambda I\n[31:21.240 --> 31:22.420]  and multiplied\n[31:22.420 --> 31:23.040]  it on this\n[31:23.040 --> 31:23.300]  side.\n[31:23.300 --> 31:23.340]  right?\n[31:27.340 --> 31:27.940]  Now,\n[31:28.340 --> 31:30.200]  if we\n[31:30.200 --> 31:30.960]  multiply\n[31:30.960 --> 31:31.860]  both sides\n[31:31.860 --> 31:32.440]  by the\n[31:32.440 --> 31:33.180]  inverse\n[31:33.180 --> 31:33.940]  of V,\n[31:34.340 --> 31:34.800]  we get\n[31:34.800 --> 31:35.720]  X V\n[31:35.720 --> 31:36.880]  V inverse\n[31:36.880 --> 31:37.760]  is equal\n[31:37.760 --> 31:38.780]  to\n[31:38.780 --> 31:39.980]  U\n[31:39.980 --> 31:40.760]  lambda\n[31:40.760 --> 31:42.220]  V\n[31:42.220 --> 31:43.080]  inverse.\n[31:45.960 --> 31:46.940]  So this\n[31:46.940 --> 31:47.380]  will become\n[31:47.380 --> 31:48.600]  the identity\n[31:48.600 --> 31:49.460]  matrix so we\n[31:49.460 --> 31:50.200]  can ignore it.\n[31:50.200 --> 31:51.820]  so this is\n[31:51.820 --> 31:52.620]  just X.\n[31:54.320 --> 31:54.980]  And\n[31:54.980 --> 31:57.180]  because the\n[31:57.180 --> 31:58.280]  vectors V\n[31:58.280 --> 31:59.060]  are\n[31:59.060 --> 32:00.060]  orthonormal,\n[32:02.900 --> 32:04.280]  V inverse\n[32:04.280 --> 32:05.200]  is actually\n[32:05.200 --> 32:05.680]  the same\n[32:05.680 --> 32:06.240]  as V\n[32:06.240 --> 32:06.800]  transpose.\n[32:10.800 --> 32:11.600]  Right?\n[32:11.600 --> 32:13.360]  because if\n[32:13.360 --> 32:13.680]  we are\n[32:13.680 --> 32:14.780]  looking at\n[32:14.780 --> 32:16.720]  a vector,\n[32:17.180 --> 32:17.940]  a matrix\n[32:17.940 --> 32:18.340]  V,\n[32:19.920 --> 32:21.160]  the inverse\n[32:21.160 --> 32:21.720]  is something\n[32:21.720 --> 32:22.400]  that produces\n[32:22.400 --> 32:23.220]  the identity\n[32:23.220 --> 32:23.760]  matrix.\n[32:25.160 --> 32:25.820]  Now,\n[32:25.920 --> 32:26.620]  each element\n[32:26.620 --> 32:27.400]  in the\n[32:27.400 --> 32:29.120]  product of\n[32:29.120 --> 32:29.640]  these two,\n[32:30.120 --> 32:30.300]  right?\n[32:30.400 --> 32:31.160]  So V,\n[32:31.280 --> 32:32.060]  V inverse,\n[32:32.760 --> 32:33.880]  each element\n[32:33.880 --> 32:34.400]  here,\n[32:35.700 --> 32:36.360]  any of these\n[32:36.360 --> 32:36.860]  elements,\n[32:37.820 --> 32:38.560]  are essentially\n[32:38.560 --> 32:39.620]  the dot product\n[32:39.620 --> 32:40.800]  of a vector\n[32:40.800 --> 32:41.700]  in V,\n[32:42.260 --> 32:42.860]  which is\n[32:42.860 --> 32:43.760]  a row,\n[32:44.760 --> 32:45.740]  and a\n[32:45.740 --> 32:46.420]  vector in\n[32:46.420 --> 32:47.060]  V inverse,\n[32:47.400 --> 32:47.800]  which is\n[32:47.800 --> 32:48.140]  a\n[32:48.140 --> 32:49.420]  column.\n[32:50.620 --> 32:51.540]  So if we\n[32:51.540 --> 32:51.980]  take the\n[32:51.980 --> 32:52.580]  transpose,\n[32:53.580 --> 32:55.060]  what are we\n[32:55.060 --> 32:55.620]  going to end\n[32:55.620 --> 32:56.080]  up with?\n[32:56.480 --> 32:57.420]  This element\n[32:57.420 --> 32:58.180]  here is going\n[32:58.180 --> 32:58.660]  to be\n[32:58.660 --> 33:00.080]  V1 dot\n[33:00.080 --> 33:01.500]  V1,\n[33:02.180 --> 33:03.800]  which is the\n[33:03.800 --> 33:04.480]  length of the\n[33:04.480 --> 33:04.780]  vector,\n[33:04.880 --> 33:05.480]  which is 1,\n[33:05.620 --> 33:06.200]  because it's\n[33:06.200 --> 33:06.860]  orthonormal.\n[33:06.860 --> 33:09.460]  And any\n[33:09.460 --> 33:10.440]  non-diagonal\n[33:10.440 --> 33:11.360]  element is\n[33:11.360 --> 33:11.920]  going to be\n[33:11.920 --> 33:12.760]  VI dot\n[33:12.760 --> 33:13.220]  VJ.\n[33:15.880 --> 33:16.560]  And of\n[33:16.560 --> 33:16.800]  course,\n[33:16.860 --> 33:17.380]  that is equal\n[33:17.380 --> 33:17.880]  to 0,\n[33:17.960 --> 33:18.460]  because they\n[33:18.460 --> 33:20.420]  are appendicular\n[33:20.420 --> 33:21.020]  to each other,\n[33:21.080 --> 33:21.240]  right?\n[33:21.280 --> 33:21.860]  That's the\n[33:21.860 --> 33:22.800]  orthogonal\n[33:22.800 --> 33:23.760]  part of this.\n[33:25.500 --> 33:25.880]  And so\n[33:25.880 --> 33:26.160]  V,\n[33:26.220 --> 33:26.900]  V transpose\n[33:26.900 --> 33:27.680]  is going to\n[33:27.680 --> 33:28.700]  be the\n[33:28.700 --> 33:29.200]  identity\n[33:29.200 --> 33:29.680]  matrix.\n[33:32.400 --> 33:32.880]  Oops,\n[33:33.380 --> 33:33.720]  apologies.\n[33:33.720 --> 33:35.380]  And so\n[33:35.380 --> 33:36.340]  the inverse\n[33:36.340 --> 33:38.120]  of a\n[33:38.120 --> 33:38.600]  matrix\n[33:38.600 --> 33:39.240]  where the\n[33:39.240 --> 33:40.080]  columns are\n[33:40.080 --> 33:40.980]  perpendicular to\n[33:40.980 --> 33:41.440]  each other\n[33:41.440 --> 33:44.980]  is going to\n[33:44.980 --> 33:45.380]  be the\n[33:45.380 --> 33:45.840]  transpose,\n[33:46.160 --> 33:46.360]  right?\n[33:46.400 --> 33:46.680]  And so\n[33:46.680 --> 33:47.040]  what we\n[33:47.040 --> 33:47.420]  are getting\n[33:47.420 --> 33:48.180]  here is\n[33:48.180 --> 33:49.580]  an\n[33:49.580 --> 33:50.220]  interesting\n[33:50.220 --> 33:51.660]  behavior.\n[33:55.160 --> 33:56.020]  That if\n[33:56.020 --> 33:56.740]  we take\n[33:56.740 --> 33:58.500]  the\n[33:58.500 --> 33:59.460]  eigenvectors\n[33:59.460 --> 34:00.280]  of X\n[34:00.280 --> 34:01.420]  transpose X,\n[34:01.420 --> 34:04.360]  we stack\n[34:04.360 --> 34:04.820]  them up\n[34:04.820 --> 34:05.300]  into\n[34:05.300 --> 34:09.000]  columns\n[34:09.000 --> 34:10.780]  and take\n[34:10.780 --> 34:10.920]  the\n[34:10.920 --> 34:11.340]  transpose\n[34:11.340 --> 34:11.780]  of that,\n[34:11.860 --> 34:12.080]  which is\n[34:12.080 --> 34:12.520]  the equivalent\n[34:12.520 --> 34:13.080]  of stacking\n[34:13.080 --> 34:13.480]  them as\n[34:13.480 --> 34:13.820]  rows,\n[34:17.820 --> 34:18.860]  and we\n[34:18.860 --> 34:20.180]  define a\n[34:20.180 --> 34:21.220]  new set\n[34:21.220 --> 34:21.840]  of vectors\n[34:21.840 --> 34:22.660]  as we\n[34:22.660 --> 34:23.060]  have done\n[34:23.060 --> 34:23.360]  here,\n[34:23.880 --> 34:25.460]  what we\n[34:25.460 --> 34:25.980]  are getting\n[34:25.980 --> 34:26.780]  is actually\n[34:26.780 --> 34:28.220]  a decomposition\n[34:28.220 --> 34:31.360]  or matrix,\n[34:31.420 --> 34:33.360]  factorization.\n[34:34.300 --> 34:34.980]  This is\n[34:34.980 --> 34:35.600]  called matrix\n[34:35.600 --> 34:36.540]  factorization,\n[34:37.060 --> 34:37.860]  where we\n[34:37.860 --> 34:38.280]  have a\n[34:38.280 --> 34:39.400]  matrix X\n[34:39.400 --> 34:40.760]  that can\n[34:40.760 --> 34:41.760]  be represented\n[34:41.760 --> 34:42.340]  as a\n[34:42.340 --> 34:43.180]  product of\n[34:43.180 --> 34:44.120]  two or\n[34:44.120 --> 34:44.360]  more\n[34:44.360 --> 34:44.820]  matrices.\n[34:47.640 --> 34:48.340]  Where have\n[34:48.340 --> 34:48.720]  we heard\n[34:48.720 --> 34:49.520]  factorization\n[34:49.520 --> 34:50.040]  before?\n[34:51.300 --> 34:51.620]  In\n[34:51.620 --> 34:52.020]  school,\n[34:52.200 --> 34:52.480]  we were\n[34:52.480 --> 34:53.720]  told factorize\n[34:53.720 --> 34:54.080]  12,\n[34:54.240 --> 34:54.480]  and we\n[34:54.480 --> 34:54.860]  would write\n[34:54.860 --> 34:55.600]  2 plus\n[34:55.600 --> 34:55.960]  2,\n[34:56.060 --> 34:56.640]  2 times\n[34:56.640 --> 34:57.280]  2 times\n[34:57.280 --> 34:57.600]  3.\n[34:58.300 --> 34:58.820]  These are\n[34:58.820 --> 34:59.880]  the prime\n[34:59.880 --> 35:00.420]  factors,\n[35:00.420 --> 35:01.500]  right?\n[35:05.100 --> 35:05.780]  So we\n[35:05.780 --> 35:06.060]  are doing\n[35:06.060 --> 35:06.480]  the same\n[35:06.480 --> 35:06.780]  here,\n[35:06.860 --> 35:07.080]  but we\n[35:07.080 --> 35:07.340]  are doing\n[35:07.340 --> 35:07.660]  it at a\n[35:07.660 --> 35:08.080]  matrix.\n[35:10.520 --> 35:11.040]  Now,\n[35:11.320 --> 35:15.020]  this here,\n[35:15.500 --> 35:16.220]  like we\n[35:16.220 --> 35:16.520]  said,\n[35:17.800 --> 35:19.120]  is a\n[35:19.120 --> 35:19.600]  diagonal\n[35:19.600 --> 35:20.900]  matrix with\n[35:20.900 --> 35:21.440]  the diagonal\n[35:21.440 --> 35:22.220]  elements being\n[35:22.220 --> 35:23.020]  the square\n[35:23.020 --> 35:23.540]  root of\n[35:23.540 --> 35:23.960]  lambda\n[35:23.960 --> 35:24.360]  i.\n[35:24.360 --> 35:25.360]  transpose.\n[35:25.360 --> 35:26.360]  Now,\n[35:27.520 --> 35:28.080]  if we\n[35:28.080 --> 35:28.800]  look to\n[35:28.800 --> 35:29.660]  calculate\n[35:29.660 --> 35:30.500]  XX\n[35:30.500 --> 35:31.200]  transpose\n[35:31.200 --> 35:31.560]  now,\n[35:31.960 --> 35:32.380]  we know\n[35:32.380 --> 35:32.840]  that X\n[35:32.840 --> 35:33.180]  can be\n[35:33.180 --> 35:33.720]  written as\n[35:33.720 --> 35:34.480]  U\n[35:34.480 --> 35:35.120]  lambda\n[35:35.120 --> 35:35.460]  V\n[35:35.460 --> 35:35.960]  transpose,\n[35:36.660 --> 35:37.420]  so we\n[35:37.420 --> 35:38.140]  plug those\n[35:38.140 --> 35:38.760]  in here,\n[35:39.300 --> 35:41.340]  and what\n[35:41.340 --> 35:41.620]  do we\n[35:41.620 --> 35:41.880]  get?\n[35:41.940 --> 35:42.380]  We get\n[35:42.380 --> 35:43.100]  U\n[35:43.100 --> 35:45.540]  lambda\n[35:45.540 --> 35:46.280]  square\n[35:46.280 --> 35:47.260]  U\n[35:47.260 --> 35:47.860]  transpose,\n[35:48.560 --> 35:50.960]  right?\n[35:50.960 --> 35:51.440]  because what\n[35:51.440 --> 35:51.960]  happens when\n[35:51.960 --> 35:52.500]  we open\n[35:52.500 --> 35:53.100]  this up,\n[35:53.840 --> 35:55.480]  it becomes\n[35:55.480 --> 35:55.760]  U\n[35:55.760 --> 35:56.580]  transpose,\n[35:57.420 --> 35:58.680]  lambda\n[35:58.680 --> 35:59.480]  transpose,\n[35:59.780 --> 35:59.920]  V\n[35:59.920 --> 36:00.580]  transpose of\n[36:00.580 --> 36:00.980]  transpose,\n[36:01.240 --> 36:01.480]  which is\n[36:01.480 --> 36:01.720]  V.\n[36:03.500 --> 36:04.100]  Oops,\n[36:04.320 --> 36:04.640]  the wrong\n[36:04.640 --> 36:04.840]  way.\n[36:06.320 --> 36:07.000]  It becomes\n[36:07.000 --> 36:08.140]  V\n[36:08.140 --> 36:09.400]  lambda\n[36:09.400 --> 36:10.260]  U\n[36:10.260 --> 36:10.980]  transpose.\n[36:11.600 --> 36:11.920]  That's\n[36:11.920 --> 36:12.440]  this part\n[36:12.440 --> 36:12.640]  here.\n[36:13.340 --> 36:14.020]  We multiply\n[36:14.020 --> 36:14.660]  it with\n[36:14.660 --> 36:15.600]  U\n[36:15.600 --> 36:17.360]  lambda\n[36:17.360 --> 36:17.880]  V\n[36:17.880 --> 36:18.420]  transpose,\n[36:18.700 --> 36:18.900]  V\n[36:18.900 --> 36:19.400]  transpose,\n[36:19.680 --> 36:20.020]  V is\n[36:20.020 --> 36:20.660]  the identity\n[36:20.660 --> 36:21.260]  matrix,\n[36:22.040 --> 36:23.120]  so it\n[36:23.120 --> 36:23.600]  disappears,\n[36:24.400 --> 36:24.860]  and we\n[36:24.860 --> 36:25.260]  end up\n[36:25.260 --> 36:25.600]  with\n[36:25.600 --> 36:26.800]  U\n[36:26.800 --> 36:27.400]  lambda\n[36:27.400 --> 36:27.940]  square\n[36:27.940 --> 36:29.020]  U\n[36:29.020 --> 36:29.520]  transpose.\n[36:33.420 --> 36:34.220]  The\n[36:34.220 --> 36:35.240]  square of\n[36:35.240 --> 36:35.760]  a diagonal\n[36:35.760 --> 36:36.560]  matrix is\n[36:36.560 --> 36:36.960]  really the\n[36:36.960 --> 36:37.500]  square of\n[36:37.500 --> 36:38.820]  the diagonal\n[36:38.820 --> 36:39.340]  element,\n[36:39.400 --> 36:41.200]  and so\n[36:41.200 --> 36:41.700]  you end\n[36:41.700 --> 36:42.300]  up with\n[36:42.300 --> 36:44.340]  essentially\n[36:44.340 --> 36:45.460]  this\n[36:45.460 --> 36:46.100]  representation\n[36:46.100 --> 36:46.780]  here,\n[36:46.900 --> 36:47.240]  which is\n[36:47.240 --> 36:47.740]  XX\n[36:47.740 --> 36:48.480]  transpose\n[36:48.480 --> 36:50.940]  is equal\n[36:50.940 --> 36:51.420]  to\n[36:51.420 --> 36:52.660]  U,\n[36:54.080 --> 36:54.700]  the\n[36:54.700 --> 36:55.200]  matrix\n[36:55.200 --> 36:55.740]  with all\n[36:55.740 --> 36:56.080]  lambda\n[36:56.080 --> 36:56.600]  1 to\n[36:56.600 --> 36:57.020]  lambda\n[36:57.020 --> 36:58.980]  R,\n[37:00.040 --> 37:00.380]  U\n[37:00.380 --> 37:01.000]  transpose,\n[37:03.080 --> 37:04.620]  and if\n[37:04.620 --> 37:05.240]  we now\n[37:05.240 --> 37:08.180]  take\n[37:08.180 --> 37:09.240]  this to\n[37:09.240 --> 37:09.900]  this side\n[37:09.900 --> 37:10.700]  by multiplying\n[37:10.700 --> 37:11.380]  by U,\n[37:13.120 --> 37:13.420]  we are\n[37:13.420 --> 37:14.140]  basically going\n[37:14.140 --> 37:14.680]  to get\n[37:14.680 --> 37:15.620]  XX\n[37:15.620 --> 37:16.220]  transpose\n[37:16.220 --> 37:16.940]  U\n[37:16.940 --> 37:19.080]  is equal\n[37:19.080 --> 37:20.060]  to 2\n[37:20.060 --> 37:21.820]  with\n[37:21.820 --> 37:22.280]  lambda\n[37:22.280 --> 37:23.360]  1 to\n[37:23.360 --> 37:23.720]  lambda\n[37:23.720 --> 37:24.820]  N,\n[37:25.760 --> 37:28.380]  which is\n[37:28.380 --> 37:28.860]  actually\n[37:28.860 --> 37:29.760]  what we\n[37:29.760 --> 37:30.200]  were seeing\n[37:30.200 --> 37:30.580]  here.\n[37:30.580 --> 37:33.160]  so while\n[37:33.160 --> 37:33.420]  the\n[37:33.420 --> 37:33.760]  V\n[37:33.760 --> 37:34.300]  is\n[37:34.300 --> 37:35.760]  are the\n[37:35.760 --> 37:36.740]  eigenvectors\n[37:36.740 --> 37:37.040]  of\n[37:37.040 --> 37:37.460]  X\n[37:37.460 --> 37:38.120]  transpose\n[37:38.120 --> 37:38.740]  X,\n[37:40.020 --> 37:40.800]  these\n[37:40.800 --> 37:41.200]  U\n[37:41.200 --> 37:41.780]  is\n[37:41.780 --> 37:43.920]  are\n[37:43.920 --> 37:45.800]  the\n[37:45.800 --> 37:46.780]  eigenvectors\n[37:46.780 --> 37:47.060]  of\n[37:47.060 --> 37:47.660]  XX\n[37:47.660 --> 37:48.340]  transpose\n[37:48.340 --> 37:49.800]  and both\n[37:49.800 --> 37:50.480]  have the\n[37:50.480 --> 37:50.980]  same\n[37:50.980 --> 37:52.700]  eigenvalues.\n[37:52.700 --> 37:55.240]  X\n[37:55.240 --> 37:57.160]  transpose\n[37:57.160 --> 37:57.440]  now let's\n[37:57.440 --> 37:57.720]  look at\n[37:57.720 --> 37:58.360]  what XX\n[37:58.360 --> 37:58.960]  transpose\n[37:58.960 --> 37:59.300]  is.\n[38:02.540 --> 38:03.180]  X\n[38:03.180 --> 38:04.540]  X\n[38:04.540 --> 38:05.360]  transpose\n[38:05.360 --> 38:06.400]  X\n[38:06.400 --> 38:07.180]  is M\n[38:07.180 --> 38:08.000]  by N\n[38:08.000 --> 38:09.120]  N\n[38:09.120 --> 38:09.580]  transpose\n[38:09.580 --> 38:10.120]  would be\n[38:10.120 --> 38:10.480]  N\n[38:10.480 --> 38:11.280]  by M\n[38:11.280 --> 38:12.660]  and so\n[38:12.660 --> 38:13.140]  this would\n[38:13.140 --> 38:13.520]  be an\n[38:13.520 --> 38:13.920]  M\n[38:13.920 --> 38:14.560]  by M\n[38:14.560 --> 38:15.040]  matrix.\n[38:15.040 --> 38:21.020]  So what\n[38:21.020 --> 38:21.320]  we are\n[38:21.320 --> 38:21.540]  doing\n[38:21.540 --> 38:22.120]  here is\n[38:22.120 --> 38:22.540]  just as\n[38:22.540 --> 38:23.000]  we said\n[38:23.000 --> 38:23.360]  that\n[38:23.360 --> 38:24.360]  X\n[38:24.360 --> 38:25.020]  transpose\n[38:25.020 --> 38:25.800]  X was\n[38:25.800 --> 38:26.120]  like\n[38:26.120 --> 38:26.680]  calculating\n[38:26.680 --> 38:26.980]  the\n[38:26.980 --> 38:28.060]  covariance\n[38:28.060 --> 38:29.620]  matrix\n[38:29.620 --> 38:31.000]  where we\n[38:31.000 --> 38:31.640]  treated our\n[38:31.640 --> 38:32.920]  documents as\n[38:32.920 --> 38:33.740]  our variables\n[38:33.740 --> 38:35.640]  out here\n[38:35.640 --> 38:36.540]  XX\n[38:36.540 --> 38:37.260]  transpose\n[38:37.260 --> 38:38.280]  on the\n[38:38.280 --> 38:38.480]  other\n[38:38.480 --> 38:38.900]  hand\n[38:38.900 --> 38:40.840]  is\n[38:40.840 --> 38:42.280]  the\n[38:42.280 --> 38:43.020]  covariance\n[38:43.020 --> 38:43.660]  matrix\n[38:43.660 --> 38:46.800]  for\n[38:46.800 --> 38:48.200]  the\n[38:48.200 --> 38:48.740]  representation\n[38:48.740 --> 38:49.160]  of\n[38:49.160 --> 38:49.780]  documents\n[38:49.780 --> 38:50.020]  as\n[38:50.020 --> 38:50.440]  words.\n[38:53.260 --> 38:53.860]  Right?\n[38:54.000 --> 38:54.320]  There are\n[38:54.320 --> 38:55.200]  M words\n[38:55.200 --> 38:55.460]  in our\n[38:55.460 --> 38:55.900]  vocabulary\n[38:55.900 --> 38:56.400]  here.\n[38:57.160 --> 38:57.860]  We are\n[38:57.860 --> 38:58.540]  representing\n[38:58.540 --> 38:58.980]  this as\n[38:58.980 --> 38:59.380]  an M\n[38:59.380 --> 38:59.860]  by N\n[38:59.860 --> 39:00.680]  matrix.\n[39:01.260 --> 39:01.440]  So we\n[39:01.440 --> 39:01.900]  are treating\n[39:01.900 --> 39:02.920]  every word\n[39:02.920 --> 39:03.240]  as a\n[39:03.240 --> 39:03.560]  variable\n[39:03.560 --> 39:04.000]  instead.\n[39:07.300 --> 39:07.900]  Right?\n[39:08.340 --> 39:09.060]  And so\n[39:09.060 --> 39:10.280]  in a\n[39:10.280 --> 39:11.060]  way what\n[39:11.060 --> 39:11.340]  we are\n[39:11.340 --> 39:12.100]  doing here\n[39:12.100 --> 39:13.480]  when we\n[39:13.480 --> 39:13.680]  do\n[39:13.680 --> 39:14.060]  singular\n[39:14.060 --> 39:14.380]  value\n[39:14.380 --> 39:15.080]  decomposition\n[39:15.080 --> 39:17.360]  and do\n[39:17.360 --> 39:17.920]  a matrix\n[39:17.920 --> 39:18.720]  factorization\n[39:18.720 --> 39:19.580]  of X\n[39:19.580 --> 39:20.460]  into a\n[39:20.460 --> 39:21.080]  product of\n[39:21.080 --> 39:21.540]  three\n[39:21.540 --> 39:22.820]  matrices\n[39:22.820 --> 39:25.380]  what we\n[39:25.380 --> 39:25.860]  end up\n[39:25.860 --> 39:26.860]  with is\n[39:26.860 --> 39:28.420]  a\n[39:28.420 --> 39:29.140]  parallel\n[39:29.140 --> 39:30.240]  eigenvalue\n[39:30.240 --> 39:31.080]  decomposition\n[39:31.080 --> 39:36.280]  of X\n[39:36.280 --> 39:36.840]  transpose\n[39:36.840 --> 39:37.360]  X\n[39:37.360 --> 39:39.180]  and X\n[39:39.180 --> 39:39.480]  X\n[39:39.480 --> 39:39.940]  transpose.\n[39:39.940 --> 39:42.320]  X\n[39:42.320 --> 39:42.660]  so we\n[39:42.660 --> 39:43.160]  have the\n[39:43.160 --> 39:45.200]  eigenvectors\n[39:45.200 --> 39:48.320]  and remember\n[39:48.320 --> 39:48.860]  what we\n[39:48.860 --> 39:49.320]  did here.\n[39:49.440 --> 39:49.560]  Right?\n[39:49.600 --> 39:49.900]  When we\n[39:49.900 --> 39:50.180]  did a\n[39:50.180 --> 39:50.540]  principal\n[39:50.540 --> 39:51.080]  component\n[39:51.080 --> 39:51.640]  analysis\n[39:51.640 --> 39:52.140]  and we\n[39:52.140 --> 39:52.640]  chose the\n[39:52.640 --> 39:53.720]  K eigenvectors\n[39:53.720 --> 39:54.220]  what did\n[39:54.220 --> 39:54.600]  we end\n[39:54.600 --> 39:55.080]  up with?\n[39:57.960 --> 39:58.480]  We\n[39:58.480 --> 39:59.120]  ended up\n[39:59.120 --> 39:59.560]  with\n[39:59.560 --> 40:01.880]  an\n[40:01.880 --> 40:02.440]  R\n[40:02.440 --> 40:03.000]  dimensional\n[40:03.000 --> 40:03.760]  representation\n[40:03.760 --> 40:04.520]  of each\n[40:04.520 --> 40:04.840]  of our\n[40:04.840 --> 40:05.360]  objects.\n[40:05.360 --> 40:09.080]  so what\n[40:09.080 --> 40:09.700]  has happened\n[40:09.700 --> 40:10.220]  here\n[40:10.220 --> 40:14.180]  is that\n[40:14.180 --> 40:14.680]  we have\n[40:14.680 --> 40:15.280]  taken\n[40:15.280 --> 40:17.340]  R\n[40:17.340 --> 40:19.560]  X\n[40:19.560 --> 40:20.340]  transpose\n[40:20.340 --> 40:21.560]  sorry\n[40:21.560 --> 40:22.160]  XX\n[40:22.160 --> 40:22.740]  transpose\n[40:22.740 --> 40:25.840]  where we\n[40:25.840 --> 40:26.220]  are saying\n[40:26.220 --> 40:27.180]  our words\n[40:27.180 --> 40:27.580]  are\n[40:27.580 --> 40:30.020]  variables\n[40:30.020 --> 40:32.900]  and as\n[40:32.900 --> 40:33.360]  a result\n[40:33.360 --> 40:33.760]  we have\n[40:33.760 --> 40:34.200]  got an\n[40:34.200 --> 40:34.660]  R\n[40:34.660 --> 40:35.260]  dimensional\n[40:35.260 --> 40:36.160]  representation\n[40:36.160 --> 40:37.840]  so the\n[40:37.840 --> 40:38.500]  U is\n[40:38.500 --> 40:39.100]  an R\n[40:39.100 --> 40:39.600]  dimensional\n[40:39.600 --> 40:40.520]  representation\n[40:40.520 --> 40:44.440]  of\n[40:44.440 --> 40:46.460]  documents\n[40:46.460 --> 40:51.980]  and our\n[40:51.980 --> 40:52.360]  V\n[40:52.360 --> 40:54.020]  is\n[40:54.020 --> 40:55.020]  our\n[40:55.020 --> 40:55.600]  R\n[40:55.600 --> 40:56.220]  dimensional\n[40:56.220 --> 40:57.220]  representation\n[40:57.220 --> 40:59.840]  of the\n[41:00.020 --> 41:02.000]  of\n[41:02.000 --> 41:03.100]  words.\n[41:08.660 --> 41:09.380]  Right?\n[41:09.520 --> 41:09.940]  These were\n[41:09.940 --> 41:10.140]  our\n[41:10.140 --> 41:10.660]  variables\n[41:10.660 --> 41:11.300]  therefore we\n[41:11.300 --> 41:11.700]  got an\n[41:11.700 --> 41:11.980]  R\n[41:11.980 --> 41:13.100]  dimensional\n[41:13.100 --> 41:13.960]  representation\n[41:13.960 --> 41:14.740]  for words.\n[41:16.060 --> 41:16.460]  In\n[41:16.460 --> 41:18.440]  the other\n[41:18.440 --> 41:19.060]  case we\n[41:19.060 --> 41:19.680]  were treating\n[41:19.680 --> 41:20.560]  each of\n[41:20.560 --> 41:21.120]  these\n[41:21.120 --> 41:22.880]  as\n[41:22.880 --> 41:25.540]  our\n[41:25.540 --> 41:27.340]  observations\n[41:27.340 --> 41:28.740]  and treating\n[41:28.740 --> 41:29.620]  words as\n[41:29.620 --> 41:29.800]  our\n[41:29.800 --> 41:30.540]  variables so\n[41:30.540 --> 41:31.020]  we got\n[41:31.020 --> 41:32.480]  a R\n[41:32.480 --> 41:33.100]  dimensional\n[41:33.100 --> 41:34.020]  representation\n[41:34.020 --> 41:36.300]  of\n[41:36.300 --> 41:37.680]  documents.\n[41:39.120 --> 41:39.420]  Right?\n[41:39.680 --> 41:39.960]  So\n[41:39.960 --> 41:43.000]  we get\n[41:43.000 --> 41:44.000]  not just\n[41:44.000 --> 41:44.900]  a representation\n[41:44.900 --> 41:46.380]  of\n[41:46.380 --> 41:47.100]  objects\n[41:47.100 --> 41:48.260]  in R\n[41:48.260 --> 41:48.620]  dimensional\n[41:48.620 --> 41:49.200]  space as\n[41:49.200 --> 41:49.500]  we get\n[41:49.500 --> 41:49.700]  with\n[41:49.700 --> 41:50.280]  principal\n[41:50.280 --> 41:50.780]  component\n[41:50.780 --> 41:51.300]  analysis\n[41:51.300 --> 41:51.800]  and\n[41:51.800 --> 41:52.300]  the\n[41:52.300 --> 41:56.560]  matrix is\n[41:56.560 --> 41:57.800]  with\n[41:57.800 --> 41:57.920]  of\n[41:57.920 --> 41:58.880]  documents\n[41:58.880 --> 42:00.060]  where the\n[42:00.060 --> 42:00.820]  matrix is\n[42:00.820 --> 42:01.560]  represented\n[42:01.560 --> 42:02.220]  as shown\n[42:02.220 --> 42:02.560]  here.\n[42:03.680 --> 42:04.680]  We end\n[42:04.680 --> 42:05.220]  up with\n[42:05.220 --> 42:06.700]  an R\n[42:06.700 --> 42:07.040]  dimensional\n[42:07.040 --> 42:07.780]  representation\n[42:07.780 --> 42:08.620]  for words\n[42:08.620 --> 42:09.560]  and an\n[42:09.560 --> 42:10.160]  R dimensional\n[42:10.160 --> 42:10.900]  representation\n[42:10.900 --> 42:11.720]  of documents\n[42:11.720 --> 42:12.120]  and that\n[42:12.120 --> 42:12.440]  is what\n[42:12.440 --> 42:12.860]  is referred\n[42:12.860 --> 42:13.460]  to as\n[42:13.460 --> 42:14.480]  latent\n[42:14.480 --> 42:14.960]  semantic\n[42:14.960 --> 42:15.500]  analysis.\n[42:16.500 --> 42:16.700]  Right?\n[42:16.700 --> 42:17.200]  So let's\n[42:17.200 --> 42:17.700]  look at\n[42:17.700 --> 42:19.200]  what happens\n[42:19.200 --> 42:19.520]  here.\n[42:20.260 --> 42:21.180]  We have\n[42:21.180 --> 42:21.520]  our\n[42:21.520 --> 42:22.060]  X\n[42:22.060 --> 42:25.700]  matrix\n[42:25.700 --> 42:28.300]  which is\n[42:28.300 --> 42:28.720]  this one\n[42:28.720 --> 42:29.080]  here.\n[42:30.080 --> 42:30.560]  Documents\n[42:30.560 --> 42:31.120]  as columns\n[42:31.120 --> 42:40.300]  and if\n[42:40.300 --> 42:40.680]  we look\n[42:40.680 --> 42:41.000]  at\n[42:41.000 --> 42:41.400]  sorry\n[42:41.400 --> 42:41.860]  let's\n[42:41.860 --> 42:42.140]  yeah\n[42:42.140 --> 42:42.740]  okay so\n[42:42.740 --> 42:43.400]  we are\n[42:43.400 --> 42:43.620]  saying\n[42:43.620 --> 42:44.280]  X is\n[42:44.280 --> 42:45.640]  equal to\n[42:45.640 --> 42:46.120]  U\n[42:46.120 --> 42:46.840]  S\n[42:46.840 --> 42:47.540]  V\n[42:47.540 --> 42:48.260]  transpose\n[42:48.260 --> 42:50.420]  X\n[42:50.420 --> 42:50.820]  here\n[42:50.820 --> 42:52.940]  was\n[42:52.940 --> 42:53.360]  something\n[42:53.360 --> 42:53.680]  by\n[42:53.680 --> 42:54.220]  8.\n[42:56.220 --> 42:56.980]  How many\n[42:56.980 --> 42:57.520]  words were\n[42:57.520 --> 42:57.740]  there?\n[42:58.660 --> 42:59.620]  12 words.\n[43:05.020 --> 43:06.180]  12 times\n[43:06.180 --> 43:06.480]  8.\n[43:07.240 --> 43:08.220]  This now\n[43:08.220 --> 43:08.700]  is going\n[43:08.700 --> 43:09.120]  to be\n[43:09.120 --> 43:10.060]  12 times\n[43:10.060 --> 43:10.600]  R\n[43:10.600 --> 43:11.420]  and becomes\n[43:11.420 --> 43:12.060]  our word\n[43:12.060 --> 43:12.740]  representation\n[43:12.740 --> 43:13.680]  in R\n[43:13.680 --> 43:14.120]  dimensional\n[43:14.120 --> 43:14.720]  space.\n[43:16.540 --> 43:16.820]  Our\n[43:16.820 --> 43:18.780]  S here\n[43:18.780 --> 43:20.360]  becomes\n[43:20.360 --> 43:20.960]  R\n[43:20.960 --> 43:21.980]  by R\n[43:21.980 --> 43:23.400]  and\n[43:23.400 --> 43:23.760]  our\n[43:23.760 --> 43:24.260]  V\n[43:24.260 --> 43:25.580]  transpose\n[43:25.580 --> 43:26.380]  will be\n[43:26.380 --> 43:26.840]  R\n[43:26.840 --> 43:27.260]  by\n[43:27.260 --> 43:28.280]  8.\n[43:31.200 --> 43:32.180]  Which\n[43:32.180 --> 43:32.660]  means\n[43:32.660 --> 43:33.560]  V\n[43:33.560 --> 43:34.120]  itself\n[43:34.120 --> 43:34.840]  is going\n[43:34.840 --> 43:35.380]  to be\n[43:35.380 --> 43:36.060]  8\n[43:36.060 --> 43:36.840]  by R.\n[43:37.280 --> 43:37.580]  Right?\n[43:37.640 --> 43:37.900]  And there\n[43:37.900 --> 43:38.400]  were 8\n[43:38.400 --> 43:38.940]  documents.\n[43:38.940 --> 43:41.720]  and there\n[43:41.720 --> 43:42.340]  were 12\n[43:42.340 --> 43:42.880]  words.\n[43:43.920 --> 43:44.260]  So we\n[43:44.260 --> 43:44.580]  have\n[43:44.580 --> 43:45.360]  words\n[43:45.360 --> 43:46.760]  as\n[43:46.760 --> 43:50.440]  represented\n[43:50.440 --> 43:50.900]  in R\n[43:50.900 --> 43:51.200]  dimensional\n[43:51.200 --> 43:51.780]  space\n[43:51.780 --> 43:52.380]  and we\n[43:52.380 --> 43:52.680]  have\n[43:52.680 --> 43:53.380]  documents\n[43:53.380 --> 43:54.080]  represented\n[43:54.080 --> 43:54.860]  in R\n[43:54.860 --> 43:55.260]  dimensional\n[43:55.260 --> 43:55.820]  space.\n[43:57.300 --> 43:57.980]  Now\n[43:57.980 --> 44:00.960]  just as\n[44:00.960 --> 44:01.280]  we did\n[44:01.280 --> 44:01.440]  in\n[44:01.440 --> 44:01.780]  principal\n[44:01.780 --> 44:02.260]  component\n[44:02.260 --> 44:02.880]  analysis\n[44:02.880 --> 44:05.360]  these\n[44:05.360 --> 44:05.800]  diagonal\n[44:05.800 --> 44:06.300]  elements\n[44:06.300 --> 44:06.540]  are\n[44:06.540 --> 44:07.200]  representing\n[44:07.200 --> 44:07.740]  the\n[44:07.740 --> 44:08.760]  variance.\n[44:11.000 --> 44:11.980]  But\n[44:11.980 --> 44:12.440]  remember\n[44:12.440 --> 44:14.680]  that the\n[44:14.680 --> 44:15.400]  variance\n[44:15.400 --> 44:17.380]  is\n[44:17.380 --> 44:18.040]  the\n[44:18.040 --> 44:18.780]  eigenvalues.\n[44:20.100 --> 44:20.860]  This\n[44:20.860 --> 44:22.600]  matrix\n[44:22.600 --> 44:23.080]  here\n[44:23.080 --> 44:24.220]  is the\n[44:24.220 --> 44:24.700]  square\n[44:24.700 --> 44:25.040]  root\n[44:25.040 --> 44:25.440]  of\n[44:25.440 --> 44:25.720]  the\n[44:25.720 --> 44:26.880]  eigenvalues.\n[44:29.120 --> 44:29.260]  Right?\n[44:29.360 --> 44:29.720]  And so\n[44:29.720 --> 44:30.220]  these are\n[44:30.220 --> 44:30.820]  referred to\n[44:30.820 --> 44:31.220]  as\n[44:31.220 --> 44:32.800]  singular\n[44:32.800 --> 44:33.440]  values.\n[44:38.100 --> 44:38.800]  And\n[44:38.800 --> 44:39.360]  just as\n[44:39.360 --> 44:39.660]  we\n[44:39.660 --> 44:41.320]  had a\n[44:41.320 --> 44:41.780]  descending\n[44:41.780 --> 44:42.620]  order to\n[44:42.620 --> 44:44.100]  the eigenvalues\n[44:44.100 --> 44:44.680]  after we\n[44:44.680 --> 44:45.340]  did principal\n[44:45.340 --> 44:45.800]  component\n[44:45.800 --> 44:46.440]  analysis\n[44:46.440 --> 44:47.960]  we have\n[44:47.960 --> 44:49.260]  a descending\n[44:49.260 --> 44:50.080]  order here.\n[44:50.380 --> 44:51.020]  The larger\n[44:51.020 --> 44:51.740]  the singular\n[44:51.740 --> 44:52.420]  value the\n[44:52.420 --> 44:52.920]  more of\n[44:52.920 --> 44:53.560]  the variance\n[44:53.560 --> 44:53.980]  is being\n[44:53.980 --> 44:54.440]  captured.\n[44:55.160 --> 44:55.540]  But the\n[44:55.540 --> 44:56.040]  variance\n[44:56.040 --> 44:56.600]  itself\n[44:56.600 --> 44:56.920]  being\n[44:56.920 --> 44:57.440]  captured\n[44:57.440 --> 44:58.680]  is the\n[44:58.680 --> 44:59.080]  square\n[44:59.080 --> 45:00.720]  of\n[45:00.720 --> 45:01.100]  these.\n[45:03.280 --> 45:03.800]  Right?\n[45:03.880 --> 45:04.120]  So in\n[45:04.120 --> 45:04.560]  this case\n[45:04.560 --> 45:04.860]  if I\n[45:04.860 --> 45:05.320]  want to\n[45:05.320 --> 45:05.620]  just\n[45:05.620 --> 45:06.240]  visualize\n[45:06.240 --> 45:06.680]  my\n[45:06.680 --> 45:07.080]  words\n[45:07.080 --> 45:07.320]  and\n[45:07.320 --> 45:07.960]  documents\n[45:07.960 --> 45:08.760]  I\n[45:08.760 --> 45:08.960]  will\n[45:08.960 --> 45:09.340]  choose\n[45:09.340 --> 45:09.560]  the\n[45:09.560 --> 45:09.800]  two\n[45:09.800 --> 45:10.220]  top\n[45:10.220 --> 45:12.960]  eigenvalues\n[45:12.960 --> 45:14.640]  as a\n[45:14.640 --> 45:15.240]  result of\n[45:15.240 --> 45:15.600]  choosing\n[45:15.600 --> 45:16.520]  this as\n[45:16.520 --> 45:16.860]  a 2\n[45:16.860 --> 45:17.380]  by 2\n[45:17.380 --> 45:17.740]  so I'm\n[45:17.740 --> 45:17.960]  saying\n[45:17.960 --> 45:18.380]  R\n[45:18.380 --> 45:18.900]  is 2\n[45:18.900 --> 45:19.480]  therefore\n[45:19.480 --> 45:20.060]  I must\n[45:20.060 --> 45:21.140]  look at\n[45:21.140 --> 45:22.240]  each\n[45:22.240 --> 45:22.860]  word\n[45:22.860 --> 45:23.260]  now\n[45:23.260 --> 45:25.160]  as\n[45:25.160 --> 45:25.660]  being\n[45:25.660 --> 45:27.340]  two\n[45:27.340 --> 45:27.820]  dimensional\n[45:27.820 --> 45:28.700]  with\n[45:28.700 --> 45:29.000]  these\n[45:29.000 --> 45:29.500]  coordinates\n[45:29.500 --> 45:31.780]  and\n[45:31.780 --> 45:32.180]  every\n[45:32.180 --> 45:32.900]  document\n[45:32.900 --> 45:33.580]  which is\n[45:33.580 --> 45:33.740]  a\n[45:33.740 --> 45:34.080]  column\n[45:34.080 --> 45:35.220]  is\n[45:35.220 --> 45:35.440]  being\n[45:35.440 --> 45:36.060]  represented\n[45:36.060 --> 45:36.600]  as a\n[45:36.600 --> 45:36.780]  two\n[45:36.780 --> 45:37.220]  dimensional\n[45:37.220 --> 45:38.080]  representation\n[45:38.080 --> 45:38.660]  so\n[45:38.660 --> 45:42.380]  now of\n[45:42.380 --> 45:42.720]  course I\n[45:42.720 --> 45:42.860]  can\n[45:42.860 --> 45:43.220]  visualize\n[45:43.220 --> 45:43.820]  those\n[45:43.820 --> 45:44.820]  like\n[45:44.820 --> 45:45.240]  this\n[45:45.240 --> 45:47.100]  so\n[45:47.100 --> 45:47.480]  each\n[45:47.480 --> 45:47.800]  dot\n[45:47.800 --> 45:48.040]  here\n[45:48.040 --> 45:48.340]  is a\n[45:48.340 --> 45:48.620]  word\n[45:48.620 --> 45:52.140]  and\n[45:52.140 --> 45:52.540]  each\n[45:52.540 --> 45:52.820]  dot\n[45:52.820 --> 45:53.260]  here\n[45:53.260 --> 45:53.840]  is a\n[45:53.840 --> 45:54.260]  document\n[45:54.260 --> 45:55.200]  and what\n[45:55.200 --> 45:55.540]  we can\n[45:55.540 --> 45:56.100]  see is\n[45:56.100 --> 45:56.380]  there are\n[45:56.380 --> 45:56.560]  these\n[45:56.560 --> 45:56.800]  three\n[45:56.800 --> 45:57.300]  documents\n[45:57.300 --> 45:57.600]  that are\n[45:57.600 --> 45:58.040]  close to\n[45:58.040 --> 45:58.240]  each\n[45:58.240 --> 45:58.460]  other\n[45:58.460 --> 46:00.500]  these\n[46:00.500 --> 46:00.900]  are quite\n[46:00.900 --> 46:01.540]  separated\n[46:01.540 --> 46:02.900]  from\n[46:02.900 --> 46:03.420]  these\n[46:03.420 --> 46:04.080]  documents\n[46:04.080 --> 46:06.080]  and then\n[46:06.080 --> 46:06.420]  you have\n[46:06.420 --> 46:07.000]  this one\n[46:07.000 --> 46:08.020]  document\n[46:08.020 --> 46:08.400]  that's\n[46:08.400 --> 46:08.600]  sitting\n[46:08.600 --> 46:08.820]  out\n[46:08.820 --> 46:09.100]  here\n[46:09.100 --> 46:11.640]  which\n[46:11.640 --> 46:12.060]  document\n[46:12.060 --> 46:12.300]  is\n[46:12.300 --> 46:12.540]  sitting\n[46:12.540 --> 46:12.980]  right out\n[46:12.980 --> 46:13.280]  here\n[46:13.280 --> 46:15.160]  it is\n[46:15.160 --> 46:15.580]  document\n[46:15.580 --> 46:16.080]  number\n[46:16.080 --> 46:16.680]  5\n[46:16.680 --> 46:18.060]  0\n[46:18.060 --> 46:18.340]  1\n[46:18.340 --> 46:18.580]  2\n[46:18.580 --> 46:18.940]  3\n[46:18.940 --> 46:19.300]  4\n[46:19.300 --> 46:19.800]  5\n[46:19.800 --> 46:23.800]  I\n[46:23.800 --> 46:24.140]  think I\n[46:24.140 --> 46:24.400]  messed\n[46:24.400 --> 46:24.600]  up\n[46:24.600 --> 46:24.760]  here\n[46:24.760 --> 46:24.960]  I've\n[46:24.960 --> 46:25.140]  added\n[46:25.140 --> 46:25.280]  a\n[46:25.280 --> 46:25.420]  new\n[46:25.420 --> 46:25.860]  document\n[46:25.860 --> 46:26.320]  but\n[46:26.320 --> 46:26.980]  essentially\n[46:26.980 --> 46:28.240]  if we\n[46:28.240 --> 46:28.720]  look at\n[46:28.720 --> 46:29.080]  what's\n[46:29.080 --> 46:29.440]  happening\n[46:29.440 --> 46:29.920]  here\n[46:29.920 --> 46:34.300]  not\n[46:34.300 --> 46:34.700]  sure why\n[46:34.700 --> 46:35.000]  I have\n[46:35.000 --> 46:35.300]  nine\n[46:35.300 --> 46:36.300]  documents\n[46:36.300 --> 46:36.540]  there\n[46:36.540 --> 46:36.700]  I'll\n[46:36.700 --> 46:36.960]  have to\n[46:36.960 --> 46:37.220]  look at\n[46:37.220 --> 46:37.480]  it\n[46:37.480 --> 46:38.380]  maybe\n[46:38.380 --> 46:38.560]  I\n[46:38.560 --> 46:38.680]  by\n[46:38.680 --> 46:39.100]  mistake\n[46:39.100 --> 46:39.960]  incorporated\n[46:39.960 --> 46:40.440]  this\n[46:40.440 --> 46:41.360]  but if\n[46:41.360 --> 46:41.680]  we look\n[46:41.680 --> 46:42.020]  at these\n[46:42.020 --> 46:42.620]  documents\n[46:42.620 --> 46:43.360]  the last\n[46:43.360 --> 46:43.600]  three\n[46:43.600 --> 46:44.120]  documents\n[46:44.120 --> 46:44.380]  were\n[46:44.380 --> 46:44.740]  supposed\n[46:44.740 --> 46:45.300]  to be\n[46:45.300 --> 46:46.760]  graph\n[46:46.760 --> 46:48.060]  related\n[46:48.060 --> 46:49.440]  and what\n[46:49.440 --> 46:49.680]  we are\n[46:49.680 --> 46:49.940]  saying\n[46:49.940 --> 46:50.360]  is we\n[46:50.360 --> 46:50.520]  have\n[46:50.520 --> 46:50.780]  one\n[46:50.780 --> 46:51.120]  two\n[46:51.120 --> 46:51.680]  three\n[46:51.680 --> 46:52.220]  four\n[46:52.220 --> 46:53.260]  at\n[46:53.260 --> 46:53.460]  least\n[46:53.460 --> 46:53.800]  these\n[46:53.800 --> 46:54.360]  documents\n[46:54.360 --> 46:55.140]  potentially\n[46:55.140 --> 46:55.600]  are\n[46:55.600 --> 46:56.160]  the\n[46:56.160 --> 46:56.980]  documents\n[46:56.980 --> 46:57.320]  that are\n[46:57.320 --> 46:57.580]  also\n[46:57.580 --> 46:58.060]  close to\n[46:58.060 --> 46:58.240]  each\n[46:58.240 --> 46:58.500]  other\n[46:58.500 --> 46:59.400]  and\n[46:59.400 --> 46:59.700]  this\n[46:59.700 --> 47:00.000]  one\n[47:00.000 --> 47:00.420]  may be\n[47:00.420 --> 47:00.600]  the\n[47:00.600 --> 47:00.860]  one\n[47:00.860 --> 47:01.240]  that\n[47:01.240 --> 47:01.420]  is\n[47:01.420 --> 47:01.540]  a\n[47:01.540 --> 47:01.700]  little\n[47:01.700 --> 47:02.080]  further\n[47:02.080 --> 47:02.400]  away\n[47:02.400 --> 47:02.620]  from\n[47:02.620 --> 47:03.440]  I'll\n[47:03.440 --> 47:03.580]  have\n[47:03.580 --> 47:03.740]  to\n[47:03.740 --> 47:04.380]  come\n[47:04.380 --> 47:04.620]  back\n[47:04.620 --> 47:04.780]  to\n[47:04.780 --> 47:04.920]  you\n[47:04.920 --> 47:07.120]  right\n[47:07.120 --> 47:09.720]  but here\n[47:09.720 --> 47:10.100]  we can\n[47:10.100 --> 47:10.420]  see\n[47:10.420 --> 47:10.800]  that\n[47:10.800 --> 47:11.180]  these\n[47:11.180 --> 47:11.460]  are\n[47:11.460 --> 47:12.000]  all\n[47:12.000 --> 47:12.700]  graph\n[47:12.700 --> 47:13.140]  theory\n[47:13.140 --> 47:14.200]  these\n[47:14.200 --> 47:14.420]  are\n[47:14.420 --> 47:14.860]  all\n[47:14.860 --> 47:15.780]  human\n[47:15.780 --> 47:16.520]  computer\n[47:16.520 --> 47:21.760]  interaction\n[47:21.760 --> 47:25.460]  and\n[47:25.460 --> 47:25.700]  maybe\n[47:25.700 --> 47:26.040]  this\n[47:26.040 --> 47:26.360]  here\n[47:26.360 --> 47:26.840]  is a\n[47:26.840 --> 47:27.000]  little\n[47:27.000 --> 47:27.420]  more\n[47:27.420 --> 47:27.940]  on\n[47:27.940 --> 47:29.800]  response\n[47:29.800 --> 47:30.260]  time\n[47:30.260 --> 47:31.980]  rather\n[47:31.980 --> 47:32.300]  than\n[47:32.300 --> 47:32.580]  human\n[47:32.580 --> 47:33.000]  computer\n[47:33.000 --> 47:33.600]  interaction\n[47:33.600 --> 47:34.080]  so\n[47:34.080 --> 47:34.460]  that's\n[47:34.460 --> 47:34.640]  why\n[47:34.640 --> 47:34.820]  it\n[47:34.820 --> 47:35.120]  is\n[47:35.120 --> 47:35.820]  separated\n[47:35.820 --> 47:36.880]  we\n[47:36.880 --> 47:37.220]  also\n[47:37.220 --> 47:37.680]  see\n[47:37.680 --> 47:39.300]  that\n[47:39.300 --> 47:39.600]  some\n[47:39.600 --> 47:40.120]  words\n[47:40.120 --> 47:40.500]  here\n[47:40.500 --> 47:42.180]  are\n[47:42.180 --> 47:42.600]  coming\n[47:42.600 --> 47:43.160]  out\n[47:43.160 --> 47:43.780]  as\n[47:43.780 --> 47:45.840]  similar\n[47:45.840 --> 47:46.100]  to\n[47:46.100 --> 47:46.320]  each\n[47:46.320 --> 47:46.600]  other\n[47:46.600 --> 47:47.460]  these\n[47:47.460 --> 47:47.680]  are\n[47:47.680 --> 47:48.120]  words\n[47:48.120 --> 47:48.400]  that\n[47:48.400 --> 47:48.940]  have\n[47:48.940 --> 47:49.520]  their\n[47:49.520 --> 47:50.340]  second\n[47:50.340 --> 47:51.020]  axis\n[47:51.020 --> 47:53.040]  okay\n[47:53.040 --> 47:53.220]  so\n[47:53.220 --> 47:53.540]  this\n[47:53.540 --> 47:53.820]  word\n[47:53.820 --> 47:54.140]  here\n[47:54.140 --> 47:54.320]  and\n[47:54.320 --> 47:54.560]  this\n[47:54.560 --> 47:54.820]  word\n[47:54.820 --> 47:55.160]  here\n[47:55.160 --> 47:56.480]  are\n[47:56.480 --> 47:57.180]  close\n[47:57.180 --> 47:57.360]  to\n[47:57.360 --> 47:57.560]  each\n[47:57.560 --> 47:57.860]  other\n[47:57.860 --> 47:59.200]  so\n[47:59.200 --> 47:59.480]  what\n[47:59.480 --> 47:59.700]  were\n[47:59.700 --> 47:59.960]  those\n[47:59.960 --> 48:00.540]  words\n[48:00.540 --> 48:05.480]  trees\n[48:05.480 --> 48:06.480]  and\n[48:06.480 --> 48:07.600]  trees\n[48:07.600 --> 48:07.960]  and\n[48:07.960 --> 48:08.360]  miners\n[48:08.360 --> 48:09.120]  and\n[48:09.120 --> 48:09.500]  so\n[48:09.500 --> 48:10.400]  trees\n[48:10.400 --> 48:10.800]  graphs\n[48:10.800 --> 48:11.020]  and\n[48:11.020 --> 48:11.380]  miners\n[48:11.380 --> 48:11.840]  these\n[48:11.840 --> 48:12.020]  are\n[48:12.020 --> 48:12.440]  definitely\n[48:12.440 --> 48:12.900]  to do\n[48:12.900 --> 48:13.180]  with\n[48:13.180 --> 48:13.620]  graph\n[48:13.620 --> 48:14.060]  theory\n[48:14.060 --> 48:14.920]  that's\n[48:14.920 --> 48:15.060]  why\n[48:15.060 --> 48:15.240]  they\n[48:15.240 --> 48:15.340]  are\n[48:15.340 --> 48:15.780]  appearing\n[48:15.780 --> 48:16.600]  close\n[48:16.600 --> 48:16.760]  to\n[48:16.760 --> 48:16.940]  each\n[48:16.940 --> 48:17.120]  other\n[48:17.120 --> 48:17.360]  on\n[48:17.360 --> 48:18.740]  the\n[48:18.740 --> 48:18.960]  other\n[48:18.960 --> 48:19.380]  hand\n[48:19.380 --> 48:20.120]  if\n[48:20.120 --> 48:20.260]  we\n[48:20.260 --> 48:20.520]  look\n[48:20.520 --> 48:20.860]  at\n[48:20.860 --> 48:21.880]  human\n[48:21.880 --> 48:22.360]  and\n[48:22.360 --> 48:22.860]  user\n[48:22.860 --> 48:23.120]  we\n[48:23.120 --> 48:23.240]  would\n[48:23.240 --> 48:23.640]  expect\n[48:23.640 --> 48:23.920]  these\n[48:23.920 --> 48:24.200]  to be\n[48:24.200 --> 48:24.440]  close\n[48:24.440 --> 48:24.600]  to\n[48:24.600 --> 48:24.760]  each\n[48:24.760 --> 48:25.020]  other\n[48:25.020 --> 48:25.460]  right\n[48:25.460 --> 48:25.640]  a\n[48:25.640 --> 48:26.020]  user\n[48:26.020 --> 48:26.220]  of\n[48:26.220 --> 48:26.360]  a\n[48:26.360 --> 48:26.740]  computer\n[48:26.740 --> 48:28.400]  is\n[48:28.400 --> 48:28.940]  as\n[48:28.940 --> 48:29.100]  of\n[48:29.100 --> 48:29.300]  now\n[48:29.300 --> 48:29.480]  at\n[48:29.480 --> 48:29.760]  least\n[48:29.760 --> 48:30.060]  a\n[48:30.060 --> 48:30.300]  human\n[48:30.300 --> 48:31.040]  so\n[48:31.040 --> 48:31.300]  that's\n[48:31.300 --> 48:31.420]  the\n[48:31.420 --> 48:31.680]  first\n[48:31.680 --> 48:31.860]  and\n[48:31.860 --> 48:32.340]  fourth\n[48:32.340 --> 48:32.820]  so\n[48:32.820 --> 48:33.240]  first\n[48:33.240 --> 48:33.740]  and\n[48:33.740 --> 48:34.100]  second\n[48:34.100 --> 48:34.520]  third\n[48:34.520 --> 48:35.020]  fourth\n[48:35.020 --> 48:37.180]  it's\n[48:37.180 --> 48:37.340]  not\n[48:37.340 --> 48:37.600]  coming\n[48:37.600 --> 48:37.760]  out\n[48:37.760 --> 48:37.980]  that\n[48:37.980 --> 48:38.360]  close\n[48:38.360 --> 48:38.900]  right\n[48:38.900 --> 48:39.100]  but\n[48:39.100 --> 48:39.740]  what\n[48:39.740 --> 48:39.880]  we\n[48:39.880 --> 48:40.040]  would\n[48:40.040 --> 48:40.540]  expect\n[48:40.540 --> 48:40.980]  is\n[48:40.980 --> 48:41.180]  that\n[48:41.180 --> 48:41.360]  the\n[48:41.360 --> 48:41.800]  coordinates\n[48:41.800 --> 48:42.140]  of\n[48:42.140 --> 48:42.480]  these\n[48:42.480 --> 48:42.700]  would\n[48:42.700 --> 48:42.940]  come\n[48:42.940 --> 48:43.240]  close\n[48:43.240 --> 48:43.460]  now\n[48:43.460 --> 48:43.640]  of\n[48:43.640 --> 48:43.900]  course\n[48:43.900 --> 48:44.720]  take\n[48:44.720 --> 48:44.940]  into\n[48:44.940 --> 48:45.360]  account\n[48:45.360 --> 48:45.820]  that\n[48:45.820 --> 48:46.880]  these\n[48:46.880 --> 48:47.200]  are\n[48:47.200 --> 48:48.120]  this\n[48:48.120 --> 48:48.300]  is a\n[48:48.300 --> 48:48.520]  very\n[48:48.520 --> 48:48.900]  small\n[48:48.900 --> 48:49.360]  corpus\n[48:49.360 --> 48:49.620]  that\n[48:49.620 --> 48:49.780]  we\n[48:49.780 --> 48:49.880]  are\n[48:49.880 --> 48:50.280]  using\n[48:50.280 --> 48:51.840]  look at\n[48:51.840 --> 48:52.120]  this\n[48:52.120 --> 48:52.420]  these\n[48:52.420 --> 48:52.580]  two\n[48:52.580 --> 48:52.700]  are\n[48:52.700 --> 48:53.780]  exactly\n[48:53.780 --> 48:54.280]  the\n[48:54.280 --> 48:54.540]  same\n[48:54.540 --> 48:54.740]  so\n[48:54.740 --> 48:54.900]  what\n[48:54.900 --> 48:55.020]  are\n[48:55.020 --> 48:55.220]  these\n[48:55.220 --> 48:55.540]  1\n[48:55.540 --> 48:55.880]  2\n[48:55.880 --> 48:56.340]  3\n[48:56.340 --> 48:56.840]  4\n[48:56.840 --> 48:57.340]  5\n[48:57.340 --> 48:57.720]  6\n[48:57.720 --> 48:58.000]  7\n[48:58.000 --> 48:59.680]  1\n[48:59.680 --> 49:00.040]  2\n[49:00.040 --> 49:00.400]  3\n[49:00.400 --> 49:00.840]  4\n[49:00.840 --> 49:01.360]  5\n[49:01.360 --> 49:01.860]  6\n[49:01.860 --> 49:02.220]  7\n[49:02.220 --> 49:03.100]  response\n[49:03.100 --> 49:03.420]  and\n[49:03.420 --> 49:03.740]  time\n[49:03.740 --> 49:05.300]  are\n[49:05.300 --> 49:05.940]  identical\n[49:05.940 --> 49:06.400]  in their\n[49:06.400 --> 49:07.020]  representation\n[49:07.020 --> 49:08.320]  right\n[49:08.320 --> 49:08.980]  so we\n[49:08.980 --> 49:09.440]  can see\n[49:09.440 --> 49:10.080]  that something\n[49:10.080 --> 49:10.680]  interesting\n[49:10.680 --> 49:11.340]  is happening\n[49:11.340 --> 49:11.740]  here\n[49:11.740 --> 49:13.140]  now\n[49:13.140 --> 49:14.080]  when we\n[49:14.080 --> 49:16.680]  get\n[49:16.680 --> 49:17.140]  this\n[49:17.140 --> 49:18.720]  12\n[49:18.720 --> 49:19.300]  by 2\n[49:19.300 --> 49:19.900]  matrix\n[49:19.900 --> 49:20.320]  here\n[49:20.320 --> 49:21.220]  multiplied\n[49:21.220 --> 49:21.700]  by a\n[49:21.700 --> 49:21.880]  2\n[49:21.880 --> 49:22.260]  by 2\n[49:22.260 --> 49:22.740]  matrix\n[49:22.740 --> 49:23.220]  multiplied\n[49:23.220 --> 49:23.800]  by\n[49:23.800 --> 49:25.120]  a 2\n[49:25.120 --> 49:25.520]  by\n[49:25.520 --> 49:26.420]  in this\n[49:26.420 --> 49:26.660]  case\n[49:26.660 --> 49:27.040]  9\n[49:27.040 --> 49:27.480]  but should\n[49:27.480 --> 49:28.040]  be 8\n[49:28.040 --> 49:28.640]  matrix\n[49:28.640 --> 49:30.720]  by\n[49:30.720 --> 49:31.460]  multiplying\n[49:31.460 --> 49:31.840]  just\n[49:31.840 --> 49:32.140]  these\n[49:32.140 --> 49:32.420]  red\n[49:32.420 --> 49:33.000]  portions\n[49:33.000 --> 49:34.840]  we\n[49:34.840 --> 49:35.320]  get a\n[49:35.320 --> 49:36.120]  reconstruction\n[49:36.120 --> 49:37.780]  of our\n[49:37.780 --> 49:38.280]  x\n[49:38.280 --> 49:43.840]  so this\n[49:43.840 --> 49:44.120]  is what\n[49:44.120 --> 49:44.600]  our x\n[49:44.600 --> 49:44.880]  looked\n[49:44.880 --> 49:45.260]  like\n[49:45.260 --> 49:49.960]  where\n[49:49.960 --> 49:50.780]  we can\n[49:50.780 --> 49:51.320]  see\n[49:51.320 --> 49:55.220]  that\n[49:55.220 --> 49:55.760]  clearly\n[49:55.760 --> 49:57.200]  the\n[49:57.200 --> 49:57.840]  vocabulary\n[49:57.840 --> 49:58.400]  that was\n[49:58.400 --> 49:59.200]  being used\n[49:59.200 --> 50:00.020]  in these\n[50:00.020 --> 50:00.740]  documents\n[50:00.740 --> 50:02.020]  were\n[50:02.020 --> 50:02.540]  different\n[50:02.540 --> 50:03.180]  from the\n[50:03.180 --> 50:03.760]  vocabulary\n[50:03.760 --> 50:04.740]  being used\n[50:04.740 --> 50:06.880]  in these\n[50:06.880 --> 50:08.300]  documents\n[50:08.300 --> 50:14.040]  right\n[50:14.040 --> 50:15.320]  but\n[50:15.320 --> 50:16.220]  but the\n[50:16.220 --> 50:17.020]  sparseness\n[50:17.020 --> 50:17.360]  here\n[50:17.360 --> 50:18.120]  the zeros\n[50:18.120 --> 50:18.600]  here\n[50:18.600 --> 50:19.720]  made it\n[50:19.720 --> 50:20.100]  look\n[50:20.100 --> 50:20.940]  like\n[50:20.940 --> 50:21.460]  maybe\n[50:21.460 --> 50:21.840]  these\n[50:21.840 --> 50:22.180]  weren't\n[50:22.180 --> 50:22.600]  as\n[50:22.600 --> 50:23.020]  similar\n[50:23.020 --> 50:23.460]  to each\n[50:23.460 --> 50:23.720]  other\n[50:23.720 --> 50:25.620]  but when\n[50:25.620 --> 50:25.920]  we\n[50:25.920 --> 50:26.720]  reconstruct\n[50:26.720 --> 50:28.120]  x\n[50:28.120 --> 50:30.320]  by taking\n[50:30.320 --> 50:30.840]  u\n[50:30.840 --> 50:32.720]  s\n[50:32.720 --> 50:33.400]  v\n[50:33.400 --> 50:34.100]  transpose\n[50:34.100 --> 50:35.880]  we are\n[50:35.880 --> 50:36.180]  now\n[50:36.180 --> 50:36.660]  getting\n[50:36.660 --> 50:39.480]  not a\n[50:39.480 --> 50:39.920]  sparse\n[50:39.920 --> 50:40.380]  matrix\n[50:40.380 --> 50:40.860]  but a\n[50:40.860 --> 50:41.220]  dense\n[50:41.220 --> 50:41.620]  matrix\n[50:41.620 --> 50:48.680]  and\n[50:48.680 --> 50:49.120]  words\n[50:49.120 --> 50:49.360]  that\n[50:49.360 --> 50:50.320]  previously\n[50:50.320 --> 50:51.740]  had the\n[50:51.740 --> 50:52.100]  value\n[50:52.100 --> 50:52.540]  zero\n[50:52.540 --> 50:53.140]  in here\n[50:53.140 --> 50:55.340]  now\n[50:55.340 --> 50:55.700]  don't\n[50:55.700 --> 50:56.060]  have a\n[50:56.060 --> 50:56.300]  zero\n[50:56.300 --> 50:56.620]  value\n[50:56.620 --> 50:59.280]  because\n[50:59.280 --> 50:59.780]  even\n[50:59.780 --> 51:00.020]  though\n[51:00.020 --> 51:00.220]  they\n[51:00.220 --> 51:00.560]  didn't\n[51:00.560 --> 51:01.000]  mention\n[51:01.000 --> 51:01.260]  the\n[51:01.260 --> 51:01.640]  value\n[51:01.640 --> 51:02.160]  the\n[51:02.160 --> 51:04.420]  word\n[51:04.420 --> 51:05.100]  system\n[51:05.100 --> 51:06.160]  they did\n[51:06.160 --> 51:06.680]  mention\n[51:06.680 --> 51:07.380]  computer\n[51:07.380 --> 51:08.420]  and computer\n[51:08.420 --> 51:09.040]  and system\n[51:09.040 --> 51:09.740]  are really\n[51:09.740 --> 51:10.440]  synonyms\n[51:10.440 --> 51:10.720]  in the\n[51:10.720 --> 51:11.200]  context\n[51:11.200 --> 51:11.620]  of what\n[51:11.620 --> 51:11.860]  we are\n[51:11.860 --> 51:12.040]  talking\n[51:12.040 --> 51:14.160]  and so\n[51:14.160 --> 51:14.460]  we are\n[51:14.460 --> 51:14.700]  now\n[51:14.700 --> 51:15.140]  filling\n[51:15.140 --> 51:15.720]  in these\n[51:15.720 --> 51:16.160]  missing\n[51:16.160 --> 51:17.080]  values\n[51:17.080 --> 51:19.640]  with\n[51:19.640 --> 51:21.480]  numbers\n[51:21.480 --> 51:22.100]  other than\n[51:22.100 --> 51:22.480]  zero\n[51:22.480 --> 51:24.040]  if there\n[51:24.040 --> 51:24.900]  are other\n[51:24.900 --> 51:26.480]  words in\n[51:26.480 --> 51:27.160]  that document\n[51:27.160 --> 51:28.220]  that really\n[51:28.220 --> 51:29.060]  are suggesting\n[51:29.060 --> 51:30.280]  that instead\n[51:30.280 --> 51:30.740]  of using\n[51:30.740 --> 51:31.120]  the word\n[51:31.120 --> 51:31.560]  system\n[51:31.560 --> 51:31.960]  we could\n[51:31.960 --> 51:32.420]  have used\n[51:32.420 --> 51:32.920]  computer\n[51:32.920 --> 51:35.280]  right\n[51:35.280 --> 51:35.740]  and we\n[51:35.740 --> 51:36.280]  are seeing\n[51:36.280 --> 51:37.240]  that a\n[51:37.240 --> 51:37.520]  lot of\n[51:37.520 --> 51:38.040]  this area\n[51:38.040 --> 51:38.480]  now\n[51:38.480 --> 51:40.340]  has larger\n[51:40.340 --> 51:41.040]  positive\n[51:41.040 --> 51:41.800]  values\n[51:41.800 --> 51:42.640]  across the\n[51:42.640 --> 51:42.940]  board\n[51:42.940 --> 51:45.220]  and similarly\n[51:45.220 --> 51:45.840]  here\n[51:45.840 --> 51:46.940]  we have\n[51:46.940 --> 51:47.700]  larger\n[51:47.700 --> 51:48.240]  positive\n[51:48.240 --> 51:48.800]  values\n[51:48.800 --> 51:50.320]  across the\n[51:50.320 --> 51:50.640]  board\n[51:50.640 --> 51:51.520]  compared to\n[51:51.520 --> 51:52.000]  what we\n[51:52.000 --> 51:52.700]  have in\n[51:52.700 --> 51:53.280]  this one\n[51:53.280 --> 51:57.080]  right\n[51:57.080 --> 51:57.680]  so this\n[51:57.680 --> 51:58.060]  shashi\n[51:58.060 --> 51:58.440]  tharoor\n[51:58.440 --> 51:58.940]  versus\n[51:58.940 --> 51:59.940]  my\n[51:59.940 --> 52:00.480]  lecture\n[52:00.480 --> 52:01.420]  problem\n[52:01.420 --> 52:02.700]  to some\n[52:02.700 --> 52:03.140]  extent\n[52:03.140 --> 52:03.580]  is being\n[52:03.580 --> 52:04.120]  resolved\n[52:04.120 --> 52:04.540]  by\n[52:04.540 --> 52:05.260]  latent\n[52:05.260 --> 52:05.760]  semantic\n[52:05.760 --> 52:06.280]  analysis\n[52:06.280 --> 52:09.420]  but\n[52:09.420 --> 52:10.960]  critics of\n[52:10.960 --> 52:11.520]  this method\n[52:11.520 --> 52:12.140]  said\n[52:12.140 --> 52:12.960]  what do you\n[52:12.960 --> 52:13.360]  mean by\n[52:13.360 --> 52:13.900]  a negative\n[52:13.900 --> 52:14.320]  value\n[52:14.320 --> 52:19.560]  by looking\n[52:19.560 --> 52:19.860]  at a\n[52:19.860 --> 52:20.200]  positive\n[52:20.200 --> 52:20.720]  value\n[52:20.720 --> 52:21.320]  we can\n[52:21.320 --> 52:21.760]  say\n[52:21.760 --> 52:22.460]  yeah\n[52:22.460 --> 52:22.740]  okay\n[52:22.740 --> 52:23.200]  so\n[52:23.200 --> 52:24.100]  you know\n[52:24.100 --> 52:24.860]  this\n[52:24.860 --> 52:25.940]  word\n[52:25.940 --> 52:28.760]  has some\n[52:28.760 --> 52:29.600]  importance\n[52:29.600 --> 52:30.680]  in\n[52:30.680 --> 52:33.360]  the\n[52:33.360 --> 52:33.800]  second\n[52:33.800 --> 52:34.700]  document\n[52:34.700 --> 52:39.020]  but\n[52:39.020 --> 52:39.460]  when we\n[52:39.460 --> 52:39.800]  have a\n[52:39.800 --> 52:40.160]  negative\n[52:40.160 --> 52:40.720]  value\n[52:40.720 --> 52:41.640]  what does\n[52:41.640 --> 52:42.120]  that really\n[52:42.120 --> 52:42.500]  mean\n[52:42.500 --> 52:43.680]  does it\n[52:43.680 --> 52:44.180]  mean that\n[52:44.180 --> 52:44.760]  this word\n[52:44.760 --> 52:45.220]  appearing\n[52:45.220 --> 52:46.240]  takes away\n[52:46.240 --> 52:48.940]  from the\n[52:48.940 --> 52:49.380]  meaning of\n[52:49.380 --> 52:50.060]  this document\n[52:50.060 --> 52:53.360]  or what\n[52:53.360 --> 52:53.680]  is that\n[52:53.680 --> 52:54.020]  negative\n[52:54.020 --> 52:54.360]  value\n[52:54.360 --> 52:55.040]  representing\n[52:55.040 --> 52:55.420]  right\n[52:55.420 --> 52:55.820]  so this\n[52:55.820 --> 52:56.400]  was the\n[52:56.400 --> 52:59.580]  what was\n[52:59.580 --> 53:00.220]  represented\n[53:00.220 --> 53:00.660]  as a\n[53:00.660 --> 53:01.300]  rebuttal\n[53:01.300 --> 53:02.540]  by the\n[53:02.540 --> 53:03.120]  Bayesians\n[53:03.120 --> 53:08.540]  who then\n[53:08.540 --> 53:09.040]  went on\n[53:09.040 --> 53:09.900]  to propose\n[53:09.900 --> 53:10.500]  a\n[53:10.500 --> 53:11.540]  probabilistic\n[53:11.540 --> 53:15.960]  latent\n[53:15.960 --> 53:17.180]  semantic\n[53:17.180 --> 53:17.880]  analysis\n[53:17.880 --> 53:24.360]  where\n[53:24.360 --> 53:25.640]  essentially\n[53:25.640 --> 53:27.040]  you ended\n[53:27.040 --> 53:27.680]  up with\n[53:27.680 --> 53:28.820]  probabilities\n[53:28.820 --> 53:29.660]  in here\n[53:29.660 --> 53:31.420]  and so\n[53:31.420 --> 53:31.780]  when you\n[53:31.780 --> 53:32.400]  looked at\n[53:32.400 --> 53:32.640]  the\n[53:32.640 --> 53:33.240]  coordinates\n[53:33.240 --> 53:34.220]  out here\n[53:34.220 --> 53:36.380]  where\n[53:36.380 --> 53:36.800]  how do\n[53:36.800 --> 53:36.940]  we\n[53:36.940 --> 53:37.360]  interpret\n[53:37.360 --> 53:38.000]  this\n[53:38.000 --> 53:39.240]  these are\n[53:39.240 --> 53:39.440]  called\n[53:39.440 --> 53:39.820]  topic\n[53:39.820 --> 53:40.260]  one and\n[53:40.260 --> 53:40.540]  topic\n[53:40.540 --> 53:40.860]  two\n[53:40.860 --> 53:43.180]  and what\n[53:43.180 --> 53:43.600]  was being\n[53:43.600 --> 53:44.240]  said here\n[53:44.240 --> 53:45.000]  is that\n[53:45.000 --> 53:45.420]  word\n[53:45.420 --> 53:45.860]  one\n[53:45.860 --> 53:48.460]  has a\n[53:48.460 --> 53:48.900]  membership\n[53:48.900 --> 53:49.460]  of this\n[53:49.460 --> 53:49.840]  topic\n[53:49.840 --> 53:50.100]  of\n[53:50.100 --> 53:50.400]  point\n[53:50.400 --> 53:50.680]  two\n[53:50.680 --> 53:50.940]  two\n[53:50.940 --> 53:52.480]  so this\n[53:52.480 --> 53:52.840]  word\n[53:52.840 --> 53:53.240]  here\n[53:53.240 --> 53:56.280]  is\n[53:56.280 --> 53:56.780]  much\n[53:56.780 --> 53:58.000]  stronger\n[53:58.000 --> 53:58.980]  in its\n[53:58.980 --> 53:59.600]  connection\n[53:59.600 --> 54:00.200]  to topic\n[54:00.200 --> 54:00.640]  one\n[54:00.640 --> 54:02.000]  compared\n[54:02.000 --> 54:02.680]  to\n[54:02.680 --> 54:04.400]  this\n[54:04.400 --> 54:04.720]  word\n[54:04.720 --> 54:05.020]  here\n[54:05.020 --> 54:08.880]  and this\n[54:08.880 --> 54:09.200]  word\n[54:09.200 --> 54:09.540]  here\n[54:09.540 --> 54:10.760]  was much\n[54:10.760 --> 54:11.020]  more\n[54:11.020 --> 54:11.440]  related\n[54:11.440 --> 54:12.080]  to topic\n[54:12.080 --> 54:12.480]  two\n[54:12.480 --> 54:13.640]  compared\n[54:13.640 --> 54:14.160]  to this\n[54:14.160 --> 54:14.440]  word\n[54:14.440 --> 54:14.740]  here\n[54:14.740 --> 54:16.460]  and we\n[54:16.460 --> 54:16.940]  can see\n[54:16.940 --> 54:17.380]  again\n[54:17.380 --> 54:18.560]  right\n[54:18.560 --> 54:19.320]  that these\n[54:19.320 --> 54:20.040]  last three\n[54:20.040 --> 54:20.600]  words\n[54:20.600 --> 54:21.900]  are associated\n[54:21.900 --> 54:22.580]  with topic\n[54:22.580 --> 54:22.940]  two\n[54:22.940 --> 54:25.820]  the first\n[54:25.820 --> 54:26.340]  bunch of\n[54:26.340 --> 54:26.820]  words\n[54:26.820 --> 54:29.700]  are much\n[54:29.700 --> 54:30.080]  more\n[54:30.080 --> 54:30.920]  related to\n[54:30.920 --> 54:31.280]  topic\n[54:31.280 --> 54:31.720]  one\n[54:31.720 --> 54:34.220]  and this\n[54:34.220 --> 54:34.560]  word\n[54:34.560 --> 54:35.060]  here\n[54:35.060 --> 54:37.140]  seems to\n[54:37.140 --> 54:37.660]  have\n[54:37.660 --> 54:39.140]  an equal\n[54:39.140 --> 54:40.040]  representation\n[54:40.040 --> 54:40.780]  across those\n[54:40.780 --> 54:41.180]  topics\n[54:41.180 --> 54:41.880]  and what\n[54:41.880 --> 54:42.240]  was that\n[54:42.240 --> 54:42.600]  word\n[54:42.600 --> 54:44.900]  survey\n[54:44.900 --> 54:49.640]  because\n[54:49.640 --> 54:50.220]  survey\n[54:50.220 --> 54:50.680]  appeared\n[54:50.680 --> 54:51.220]  once\n[54:51.220 --> 54:51.640]  here\n[54:51.640 --> 54:53.180]  in a\n[54:53.180 --> 54:53.660]  document\n[54:53.660 --> 54:54.480]  that had\n[54:54.480 --> 54:54.980]  more of\n[54:54.980 --> 54:55.220]  this\n[54:55.220 --> 54:55.740]  vocabulary\n[54:55.740 --> 54:57.700]  and also\n[54:57.700 --> 54:58.140]  appeared\n[54:58.140 --> 54:58.480]  here\n[54:58.480 --> 54:59.820]  once\n[54:59.820 --> 55:01.740]  in a\n[55:01.740 --> 55:02.100]  set of\n[55:02.100 --> 55:02.740]  documents\n[55:02.740 --> 55:04.580]  that\n[55:04.580 --> 55:04.980]  actually\n[55:04.980 --> 55:05.420]  had\n[55:05.420 --> 55:06.040]  this\n[55:06.040 --> 55:06.560]  vocabulary\n[55:06.560 --> 55:07.940]  right\n[55:07.940 --> 55:08.620]  so the\n[55:08.620 --> 55:09.420]  jury is\n[55:09.420 --> 55:09.720]  out\n[55:09.720 --> 55:10.700]  or maybe\n[55:10.700 --> 55:11.440]  this word\n[55:11.440 --> 55:12.420]  transcends\n[55:12.420 --> 55:12.760]  both\n[55:12.760 --> 55:13.320]  topics\n[55:13.320 --> 55:14.720]  and that\n[55:14.720 --> 55:15.060]  is what\n[55:15.060 --> 55:15.860]  is represented\n[55:15.860 --> 55:17.140]  out here\n[55:17.140 --> 55:24.640]  right\n[55:24.640 --> 55:25.780]  so you\n[55:25.780 --> 55:26.340]  can see\n[55:26.340 --> 55:26.780]  that what\n[55:26.780 --> 55:27.060]  we are\n[55:27.060 --> 55:27.400]  getting\n[55:27.400 --> 55:27.740]  is a\n[55:27.740 --> 55:28.320]  partitioning\n[55:28.320 --> 55:28.600]  of the\n[55:28.600 --> 55:29.100]  vocabulary\n[55:29.100 --> 55:29.700]  out here\n[55:29.700 --> 55:31.020]  but the\n[55:31.020 --> 55:31.660]  criticism\n[55:31.660 --> 55:32.400]  was\n[55:32.400 --> 55:33.860]  why\n[55:33.860 --> 55:34.360]  minus\n[55:34.360 --> 55:35.520]  what does\n[55:35.520 --> 55:36.000]  this minus\n[55:36.000 --> 55:36.520]  represent\n[55:36.520 --> 55:37.820]  and you\n[55:37.820 --> 55:38.320]  can see\n[55:38.320 --> 55:38.980]  how if\n[55:38.980 --> 55:39.500]  you instead\n[55:39.500 --> 55:39.840]  had\n[55:39.840 --> 55:40.940]  probabilities\n[55:40.940 --> 55:43.080]  where word\n[55:43.080 --> 55:43.920]  one was\n[55:43.920 --> 55:44.660]  looking like\n[55:44.660 --> 55:45.440]  this word\n[55:45.440 --> 55:46.220]  two and so\n[55:46.220 --> 55:47.300]  on and\n[55:47.300 --> 55:47.640]  then the\n[55:47.640 --> 55:48.580]  latter words\n[55:48.580 --> 55:49.140]  word\n[55:49.140 --> 55:51.260]  10 word\n[55:51.260 --> 55:51.860]  11 and\n[55:51.860 --> 55:52.740]  word 12\n[55:52.740 --> 55:54.920]  word 9\n[55:54.920 --> 55:56.240]  word 9\n[55:56.240 --> 55:56.820]  looked more\n[55:56.820 --> 55:58.900]  like 4\n[55:58.900 --> 56:00.160]  and 0.6\n[56:00.160 --> 56:01.880]  whereas these\n[56:01.880 --> 56:02.360]  were more\n[56:02.360 --> 56:03.480]  0.8\n[56:03.480 --> 56:04.480]  0.9\n[56:04.480 --> 56:05.620]  0.92\n[56:05.620 --> 56:12.380]  the interpretation\n[56:12.380 --> 56:13.160]  is so much\n[56:13.160 --> 56:13.540]  easier\n[56:13.540 --> 56:14.040]  right\n[56:14.040 --> 56:15.120]  what we are\n[56:15.120 --> 56:15.680]  saying is\n[56:15.680 --> 56:16.100]  that if a\n[56:16.100 --> 56:16.580]  topic\n[56:16.580 --> 56:17.620]  topic one\n[56:17.620 --> 56:19.660]  is what\n[56:19.660 --> 56:20.380]  is generating\n[56:20.380 --> 56:21.280]  a word\n[56:21.280 --> 56:24.400]  or if we\n[56:24.400 --> 56:25.000]  are seeing\n[56:25.000 --> 56:26.160]  a word\n[56:26.160 --> 56:26.480]  in a\n[56:26.480 --> 56:26.960]  document\n[56:26.960 --> 56:28.600]  the\n[56:28.600 --> 56:29.180]  likelihood\n[56:29.180 --> 56:31.040]  is that\n[56:31.040 --> 56:31.880]  topic one\n[56:31.880 --> 56:33.420]  generated\n[56:33.420 --> 56:34.200]  this word\n[56:34.200 --> 56:35.060]  much more\n[56:35.060 --> 56:35.780]  than the\n[56:35.780 --> 56:36.360]  likelihood of\n[56:36.360 --> 56:36.880]  topic two\n[56:36.880 --> 56:37.380]  generated\n[56:37.380 --> 56:39.440]  it's four\n[56:39.440 --> 56:40.100]  times more\n[56:40.100 --> 56:41.460]  likely that\n[56:41.460 --> 56:42.240]  topic one\n[56:42.240 --> 56:42.820]  would have\n[56:42.820 --> 56:43.320]  generated\n[56:43.320 --> 56:43.940]  this word\n[56:43.940 --> 56:45.800]  and so\n[56:45.800 --> 56:46.140]  when we\n[56:46.140 --> 56:46.740]  look across\n[56:46.740 --> 56:47.120]  all the\n[56:47.120 --> 56:48.060]  words that\n[56:48.060 --> 56:48.540]  appear in\n[56:48.540 --> 56:49.080]  a document\n[56:49.080 --> 56:49.560]  we can\n[56:49.560 --> 56:50.060]  then come\n[56:50.060 --> 56:50.540]  up with a\n[56:50.540 --> 56:50.940]  document\n[56:50.940 --> 56:51.800]  representation\n[56:51.800 --> 56:52.320]  saying that\n[56:52.320 --> 56:52.960]  the document\n[56:52.960 --> 56:55.120]  has maybe\n[56:55.120 --> 56:56.260]  60% of\n[56:56.260 --> 56:56.980]  its vocabulary\n[56:56.980 --> 56:57.680]  from topic\n[56:57.680 --> 56:59.260]  one and\n[56:59.260 --> 57:00.200]  40% of\n[57:00.200 --> 57:00.800]  its vocabulary\n[57:00.800 --> 57:01.380]  from topic\n[57:01.380 --> 57:01.940]  two and\n[57:01.940 --> 57:02.260]  that would\n[57:02.260 --> 57:02.740]  become the\n[57:02.740 --> 57:03.440]  document\n[57:03.440 --> 57:04.080]  representation\n[57:04.080 --> 57:07.460]  here again\n[57:07.460 --> 57:08.520]  what we\n[57:08.520 --> 57:09.020]  can see\n[57:09.020 --> 57:11.220]  is that\n[57:11.220 --> 57:11.720]  these\n[57:11.720 --> 57:12.800]  are\n[57:12.800 --> 57:14.920]  related\n[57:14.920 --> 57:15.200]  to\n[57:15.200 --> 57:16.780]  topic\n[57:16.780 --> 57:17.160]  one\n[57:17.160 --> 57:18.780]  and these\n[57:18.780 --> 57:19.120]  are\n[57:19.120 --> 57:20.540]  related\n[57:20.540 --> 57:21.140]  more to\n[57:21.140 --> 57:21.940]  topic\n[57:21.940 --> 57:22.220]  two\n[57:22.220 --> 57:25.460]  right so\n[57:25.460 --> 57:26.200]  the partitioning\n[57:26.200 --> 57:26.660]  is happening\n[57:26.660 --> 57:27.440]  of documents\n[57:27.440 --> 57:29.240]  but\n[57:29.240 --> 57:31.200]  what these\n[57:31.200 --> 57:32.560]  actually mean\n[57:32.560 --> 57:33.220]  especially the\n[57:33.220 --> 57:34.080]  negative values\n[57:34.080 --> 57:34.660]  became a\n[57:34.660 --> 57:35.020]  question\n[57:35.020 --> 57:35.440]  that was\n[57:35.440 --> 57:35.760]  asked\n[57:35.760 --> 57:36.860]  so\n[57:36.860 --> 57:38.540]  let's take\n[57:38.540 --> 57:38.880]  a break\n[57:38.880 --> 57:39.220]  from the\n[57:39.220 --> 57:39.660]  theory of\n[57:39.660 --> 57:39.840]  it\n[57:39.840 --> 57:42.380]  oh actually\n[57:42.380 --> 57:43.480]  maybe not\n[57:43.480 --> 57:44.360]  I was going\n[57:44.360 --> 57:44.820]  to go into\n[57:44.820 --> 57:45.660]  some text\n[57:45.660 --> 57:46.240]  but let me\n[57:46.240 --> 57:48.000]  now do\n[57:48.000 --> 57:48.540]  this next\n[57:48.540 --> 57:49.320]  topic before\n[57:49.320 --> 57:50.880]  I get\n[57:50.880 --> 57:51.240]  on to\n[57:51.240 --> 57:52.020]  some of\n[57:52.020 --> 57:53.780]  the code\n[57:53.780 --> 57:57.660]  so what\n[57:57.660 --> 57:58.000]  we have\n[57:58.000 --> 57:58.640]  seen is\n[57:58.640 --> 57:59.840]  three methods\n[57:59.840 --> 58:00.980]  for syntactic\n[58:00.980 --> 58:03.920]  vectorization\n[58:03.920 --> 58:06.700]  and as\n[58:06.700 --> 58:07.220]  of now\n[58:07.220 --> 58:07.620]  we have\n[58:07.620 --> 58:08.340]  seen one\n[58:08.340 --> 58:09.500]  method for\n[58:09.500 --> 58:10.040]  semantic\n[58:10.040 --> 58:10.880]  representation\n[58:10.880 --> 58:12.800]  or semantic\n[58:12.800 --> 58:13.620]  vectorization\n[58:13.620 --> 58:17.780]  where each\n[58:17.780 --> 58:18.900]  document now\n[58:18.900 --> 58:19.900]  is represented\n[58:19.900 --> 58:20.340]  in this\n[58:20.340 --> 58:21.000]  two-dimensional\n[58:21.000 --> 58:22.300]  topic space\n[58:22.300 --> 58:25.960]  either way\n[58:25.960 --> 58:27.000]  what we now\n[58:27.000 --> 58:27.620]  want to do\n[58:27.620 --> 58:29.620]  is apply\n[58:29.620 --> 58:30.120]  this to\n[58:30.120 --> 58:30.560]  something\n[58:30.560 --> 58:32.700]  and when\n[58:32.700 --> 58:33.340]  we started\n[58:33.340 --> 58:34.060]  this topic\n[58:34.060 --> 58:34.840]  we had\n[58:34.840 --> 58:35.580]  talked about\n[58:35.580 --> 58:36.180]  text\n[58:36.180 --> 58:37.140]  categorization\n[58:37.140 --> 58:44.940]  where we\n[58:44.940 --> 58:45.880]  have documents\n[58:45.880 --> 58:46.540]  that have\n[58:46.540 --> 58:46.980]  a particular\n[58:46.980 --> 58:48.020]  class associated\n[58:48.020 --> 58:49.540]  is this a\n[58:49.540 --> 58:50.300]  sports document\n[58:50.300 --> 58:51.040]  is this a\n[58:51.040 --> 58:51.840]  politics document\n[58:51.840 --> 58:52.460]  is it a\n[58:52.460 --> 58:52.720]  whatever\n[58:52.720 --> 58:56.400]  so now\n[58:56.400 --> 58:56.800]  that we\n[58:56.800 --> 58:57.220]  have been\n[58:57.220 --> 58:57.600]  able to\n[58:57.600 --> 58:58.220]  vectorize\n[58:58.220 --> 58:58.920]  documents\n[58:58.920 --> 59:01.400]  this\n[59:01.400 --> 59:02.020]  essentially\n[59:02.020 --> 59:02.700]  becomes\n[59:02.700 --> 59:03.840]  a standard\n[59:03.840 --> 59:04.220]  machine\n[59:04.220 --> 59:04.520]  learning\n[59:04.520 --> 59:05.000]  problem\n[59:05.000 --> 59:07.620]  but\n[59:07.620 --> 59:08.800]  before we\n[59:08.800 --> 59:09.020]  even\n[59:09.020 --> 59:09.780]  vectorize\n[59:09.780 --> 59:12.420]  the\n[59:12.420 --> 59:13.120]  documents\n[59:13.120 --> 59:14.780]  or\n[59:14.780 --> 59:15.440]  maybe\n[59:15.440 --> 59:16.180]  we could\n[59:16.180 --> 59:16.500]  look at\n[59:16.500 --> 59:17.160]  this as\n[59:17.160 --> 59:18.160]  justifying\n[59:18.160 --> 59:18.780]  why we\n[59:18.780 --> 59:19.260]  are doing\n[59:19.260 --> 59:19.700]  what we\n[59:19.700 --> 59:20.160]  are doing\n[59:20.160 --> 59:20.420]  in the\n[59:20.420 --> 59:21.180]  vectorization\n[59:21.180 --> 59:21.820]  especially\n[59:21.820 --> 59:22.900]  the syntactic\n[59:22.900 --> 59:25.220]  vectorization\n[59:25.220 --> 59:30.060]  we talked\n[59:30.060 --> 59:30.480]  about the\n[59:30.480 --> 59:31.240]  vector space\n[59:31.240 --> 59:31.800]  model for\n[59:31.800 --> 59:32.260]  documents\n[59:32.260 --> 59:32.740]  let's look\n[59:32.740 --> 59:33.560]  at some\n[59:33.560 --> 59:34.320]  probabilistic\n[59:34.320 --> 59:35.060]  models for\n[59:35.060 --> 59:36.020]  documents\n[59:36.020 --> 59:36.460]  instead\n[59:36.460 --> 59:38.920]  so the\n[59:38.920 --> 59:39.680]  idea here\n[59:39.680 --> 59:43.840]  is we\n[59:43.840 --> 59:44.200]  are talking\n[59:44.200 --> 59:44.680]  about a\n[59:44.680 --> 59:46.120]  probabilistic\n[59:46.120 --> 59:46.440]  model\n[59:46.440 --> 59:47.000]  therefore we\n[59:47.000 --> 59:47.380]  must be\n[59:47.380 --> 59:47.820]  talking about\n[59:47.820 --> 59:48.300]  generative\n[59:48.300 --> 59:48.760]  models\n[59:48.760 --> 59:52.320]  and what\n[59:52.320 --> 59:52.580]  we are\n[59:52.580 --> 59:53.120]  saying is\n[59:53.120 --> 59:53.420]  that we\n[59:53.420 --> 59:53.700]  have a\n[59:53.700 --> 59:54.160]  corpus\n[59:54.160 --> 59:56.880]  a corpus\n[59:56.880 --> 59:57.500]  consists of\n[59:57.500 --> 59:57.760]  a number\n[59:57.760 --> 59:58.560]  of documents\n[59:58.560 --> 01:00:00.480]  and each\n[01:00:00.480 --> 01:00:01.100]  document\n[01:00:01.100 --> 01:00:04.620]  is generated\n[01:00:04.620 --> 01:00:06.160]  from a\n[01:00:06.160 --> 01:00:06.580]  mixture\n[01:00:06.580 --> 01:00:07.080]  model\n[01:00:07.080 --> 01:00:08.540]  parametrized\n[01:00:08.540 --> 01:00:09.200]  by some\n[01:00:09.200 --> 01:00:10.640]  parameter\n[01:00:10.640 --> 01:00:11.140]  vector\n[01:00:11.140 --> 01:00:11.740]  term\n[01:00:11.740 --> 01:00:24.340]  now we've\n[01:00:24.340 --> 01:00:24.840]  come across\n[01:00:24.840 --> 01:00:25.500]  mixture models\n[01:00:25.500 --> 01:00:26.720]  before when\n[01:00:26.720 --> 01:00:27.040]  we were\n[01:00:27.040 --> 01:00:27.640]  looking at\n[01:00:27.640 --> 01:00:28.260]  the EM\n[01:00:28.260 --> 01:00:28.880]  algorithm\n[01:00:28.880 --> 01:00:29.920]  for\n[01:00:29.920 --> 01:00:30.960]  clustering\n[01:00:30.960 --> 01:00:31.680]  and\n[01:00:31.680 --> 01:00:34.480]  essentially a\n[01:00:34.480 --> 01:00:35.080]  mixture model\n[01:00:35.080 --> 01:00:35.420]  is a\n[01:00:35.420 --> 01:00:35.760]  probability\n[01:00:35.760 --> 01:00:36.460]  distribution\n[01:00:36.460 --> 01:00:37.480]  defined by\n[01:00:37.480 --> 01:00:38.160]  a linear\n[01:00:38.160 --> 01:00:39.220]  combination of\n[01:00:39.220 --> 01:00:40.000]  individual\n[01:00:40.000 --> 01:00:40.620]  probability\n[01:00:40.620 --> 01:00:41.320]  distribution\n[01:00:41.320 --> 01:00:42.040]  also known\n[01:00:42.040 --> 01:00:42.800]  as components\n[01:00:42.800 --> 01:00:44.640]  and when\n[01:00:44.640 --> 01:00:44.960]  we were\n[01:00:44.960 --> 01:00:45.540]  studying the\n[01:00:45.540 --> 01:00:45.900]  EM\n[01:00:45.900 --> 01:00:46.720]  algorithm we\n[01:00:46.720 --> 01:00:47.340]  had assumed\n[01:00:47.340 --> 01:00:48.160]  that these\n[01:00:48.160 --> 01:00:49.600]  components\n[01:00:49.600 --> 01:00:51.100]  are what\n[01:00:51.100 --> 01:00:57.480]  they are\n[01:00:57.480 --> 01:00:57.900]  Gaussian\n[01:00:57.900 --> 01:01:03.760]  right and\n[01:01:03.760 --> 01:01:04.540]  each Gaussian\n[01:01:04.540 --> 01:01:05.080]  had its\n[01:01:05.080 --> 01:01:05.260]  own\n[01:01:05.260 --> 01:01:05.860]  parameters\n[01:01:05.860 --> 01:01:08.720]  and there\n[01:01:08.720 --> 01:01:09.680]  was another\n[01:01:09.680 --> 01:01:10.440]  parameter\n[01:01:10.440 --> 01:01:12.140]  which was\n[01:01:12.140 --> 01:01:12.840]  the probability\n[01:01:12.840 --> 01:01:13.840]  of a component\n[01:01:13.840 --> 01:01:14.640]  being called\n[01:01:14.640 --> 01:01:15.600]  upon to\n[01:01:15.600 --> 01:01:16.140]  generate\n[01:01:16.140 --> 01:01:16.800]  data\n[01:01:16.800 --> 01:01:19.820]  so similarly\n[01:01:19.820 --> 01:01:21.480]  we can say\n[01:01:21.480 --> 01:01:22.340]  that a\n[01:01:22.340 --> 01:01:23.020]  document\n[01:01:23.020 --> 01:01:23.560]  DI\n[01:01:23.560 --> 01:01:28.540]  is generated\n[01:01:28.540 --> 01:01:29.120]  by\n[01:01:29.120 --> 01:01:32.800]  a number\n[01:01:32.800 --> 01:01:33.740]  of components\n[01:01:33.740 --> 01:01:40.340]  and each\n[01:01:40.340 --> 01:01:40.640]  of those\n[01:01:40.640 --> 01:01:41.120]  components\n[01:01:41.120 --> 01:01:41.800]  has a\n[01:01:41.800 --> 01:01:42.380]  probability\n[01:01:42.380 --> 01:01:42.860]  of\n[01:01:42.860 --> 01:01:46.560]  being asked\n[01:01:46.560 --> 01:01:47.320]  to generate\n[01:01:47.320 --> 01:01:49.720]  a part\n[01:01:49.720 --> 01:01:50.100]  of this\n[01:01:50.100 --> 01:01:50.520]  document\n[01:01:50.520 --> 01:01:51.720]  so when\n[01:01:51.720 --> 01:01:52.120]  we think\n[01:01:52.120 --> 01:01:52.400]  of a\n[01:01:52.400 --> 01:01:52.880]  document\n[01:01:52.880 --> 01:01:56.320]  we are\n[01:01:56.320 --> 01:01:56.840]  really saying\n[01:01:56.840 --> 01:01:57.140]  that a\n[01:01:57.140 --> 01:01:58.020]  document is\n[01:01:58.020 --> 01:02:00.100]  a bag\n[01:02:00.100 --> 01:02:00.800]  of words\n[01:02:00.800 --> 01:02:03.340]  why are\n[01:02:03.340 --> 01:02:03.760]  we using\n[01:02:03.760 --> 01:02:04.200]  the word\n[01:02:04.200 --> 01:02:04.700]  bag\n[01:02:04.700 --> 01:02:06.860]  we are\n[01:02:06.860 --> 01:02:07.280]  saying it's\n[01:02:07.280 --> 01:02:07.880]  a bag\n[01:02:07.880 --> 01:02:08.420]  not a\n[01:02:08.420 --> 01:02:08.760]  set\n[01:02:08.760 --> 01:02:09.900]  because a\n[01:02:09.900 --> 01:02:10.480]  bag can\n[01:02:10.480 --> 01:02:11.240]  have multiple\n[01:02:11.240 --> 01:02:11.940]  copies of\n[01:02:11.940 --> 01:02:12.340]  the same\n[01:02:12.340 --> 01:02:12.760]  word\n[01:02:12.760 --> 01:02:14.160]  a set\n[01:02:14.160 --> 01:02:16.860]  has only\n[01:02:16.860 --> 01:02:17.280]  unique\n[01:02:17.280 --> 01:02:17.860]  elements\n[01:02:17.860 --> 01:02:21.180]  right\n[01:02:21.180 --> 01:02:22.320]  so what\n[01:02:22.320 --> 01:02:22.560]  we are\n[01:02:22.560 --> 01:02:23.120]  saying is\n[01:02:23.120 --> 01:02:23.440]  that this\n[01:02:23.440 --> 01:02:24.340]  document has\n[01:02:24.340 --> 01:02:26.340]  words in\n[01:02:26.340 --> 01:02:27.480]  it and\n[01:02:27.480 --> 01:02:28.300]  those words\n[01:02:28.300 --> 01:02:29.320]  are generated\n[01:02:29.320 --> 01:02:31.740]  by calling\n[01:02:31.740 --> 01:02:32.600]  upon these\n[01:02:32.600 --> 01:02:33.220]  components\n[01:02:33.220 --> 01:02:35.720]  so the\n[01:02:35.720 --> 01:02:36.700]  question is\n[01:02:36.700 --> 01:02:39.520]  what kind\n[01:02:39.520 --> 01:02:40.160]  of probability\n[01:02:40.160 --> 01:02:40.940]  distribution\n[01:02:40.940 --> 01:02:41.420]  are these\n[01:02:41.420 --> 01:02:41.860]  components\n[01:02:41.860 --> 01:02:46.960]  and there\n[01:02:46.960 --> 01:02:47.280]  are two\n[01:02:47.280 --> 01:02:47.920]  models\n[01:02:47.920 --> 01:02:49.560]  probabilistic\n[01:02:49.560 --> 01:02:50.080]  models of\n[01:02:50.080 --> 01:02:50.640]  documents\n[01:02:50.640 --> 01:02:52.400]  one is the\n[01:02:52.400 --> 01:02:53.340]  multivariate\n[01:02:53.340 --> 01:02:54.180]  Bern-Lowli\n[01:02:54.180 --> 01:02:55.640]  and the\n[01:02:55.640 --> 01:02:56.220]  second is\n[01:02:56.220 --> 01:02:56.440]  the\n[01:02:56.440 --> 01:02:57.260]  multinomial\n[01:02:57.260 --> 01:02:58.560]  distribution\n[01:02:58.560 --> 01:02:58.980]  model\n[01:02:58.980 --> 01:03:03.960]  so we\n[01:03:03.960 --> 01:03:04.080]  are going\n[01:03:04.080 --> 01:03:04.280]  to look\n[01:03:04.280 --> 01:03:04.620]  at both\n[01:03:04.620 --> 01:03:05.000]  of those\n[01:03:05.000 --> 01:03:08.180]  the\n[01:03:08.180 --> 01:03:09.300]  multivariate\n[01:03:09.300 --> 01:03:10.740]  Bern-Lowli\n[01:03:10.740 --> 01:03:11.080]  model\n[01:03:11.080 --> 01:03:13.000]  what does\n[01:03:13.000 --> 01:03:13.460]  that tell\n[01:03:13.460 --> 01:03:13.780]  you there\n[01:03:13.780 --> 01:03:14.100]  are two\n[01:03:14.100 --> 01:03:14.880]  really important\n[01:03:14.880 --> 01:03:15.440]  words here\n[01:03:15.440 --> 01:03:16.380]  multivariate\n[01:03:16.380 --> 01:03:18.460]  what is\n[01:03:18.460 --> 01:03:19.220]  multivariate\n[01:03:19.220 --> 01:03:25.360]  jay kishan\n[01:03:25.360 --> 01:03:29.420]  any idea\n[01:03:29.420 --> 01:03:29.820]  what it\n[01:03:29.820 --> 01:03:30.080]  is\n[01:03:30.080 --> 01:03:31.040]  multivariate\n[01:03:31.040 --> 01:03:32.920]  we say\n[01:03:32.920 --> 01:03:33.620]  multivariate\n[01:03:33.620 --> 01:03:34.120]  statistics\n[01:03:34.120 --> 01:03:34.620]  also\n[01:03:34.620 --> 01:03:37.940]  no sir\n[01:03:37.940 --> 01:03:39.640]  okay\n[01:03:39.640 --> 01:03:40.140]  sumit\n[01:03:40.140 --> 01:03:42.120]  any idea\n[01:03:42.120 --> 01:03:43.000]  what multivariate\n[01:03:43.000 --> 01:03:43.500]  stands for\n[01:03:43.500 --> 01:03:43.880]  this word\n[01:03:43.880 --> 01:03:50.900]  multiple\n[01:03:50.900 --> 01:03:53.300]  variables\n[01:03:53.300 --> 01:03:54.200]  it's as\n[01:03:54.200 --> 01:03:54.660]  simple as\n[01:03:54.660 --> 01:03:55.420]  that\n[01:03:55.420 --> 01:03:57.200]  multivariate\n[01:03:57.200 --> 01:03:57.480]  means\n[01:03:57.480 --> 01:03:57.880]  multiple\n[01:03:57.880 --> 01:03:58.260]  variables\n[01:03:58.260 --> 01:03:59.300]  so we\n[01:03:59.300 --> 01:03:59.680]  are saying\n[01:03:59.680 --> 01:04:00.240]  we have\n[01:04:00.240 --> 01:04:01.300]  x1\n[01:04:01.300 --> 01:04:01.960]  x2\n[01:04:01.960 --> 01:04:02.800]  x3\n[01:04:02.800 --> 01:04:04.340]  till xn\n[01:04:04.340 --> 01:04:05.920]  what are\n[01:04:05.920 --> 01:04:06.760]  these x1\n[01:04:06.760 --> 01:04:07.540]  to xn\n[01:04:07.540 --> 01:04:07.900]  in our\n[01:04:07.900 --> 01:04:08.200]  case\n[01:04:08.200 --> 01:04:08.640]  because we\n[01:04:08.640 --> 01:04:08.940]  are looking\n[01:04:08.940 --> 01:04:09.940]  at documents\n[01:04:09.940 --> 01:04:11.100]  these are\n[01:04:11.100 --> 01:04:11.880]  elements of\n[01:04:11.880 --> 01:04:12.520]  our vocabulary\n[01:04:12.520 --> 01:04:19.180]  so they\n[01:04:19.180 --> 01:04:19.560]  could be\n[01:04:19.560 --> 01:04:20.140]  words\n[01:04:20.140 --> 01:04:21.960]  they could\n[01:04:21.960 --> 01:04:22.660]  be keywords\n[01:04:22.660 --> 01:04:23.460]  key phrases\n[01:04:23.460 --> 01:04:28.600]  and of course\n[01:04:28.600 --> 01:04:29.440]  Bernoulli we\n[01:04:29.440 --> 01:04:30.120]  understand\n[01:04:30.120 --> 01:04:31.840]  in that they\n[01:04:31.840 --> 01:04:32.580]  either occur\n[01:04:32.580 --> 01:04:33.520]  or don't\n[01:04:33.520 --> 01:04:33.820]  occur\n[01:04:33.820 --> 01:04:35.660]  right\n[01:04:35.660 --> 01:04:36.380]  so this is\n[01:04:36.380 --> 01:04:36.860]  just a\n[01:04:36.860 --> 01:04:37.320]  binary\n[01:04:37.320 --> 01:04:38.980]  variable\n[01:04:38.980 --> 01:04:39.980]  random\n[01:04:39.980 --> 01:04:40.320]  variable\n[01:04:40.320 --> 01:04:42.500]  so this\n[01:04:42.520 --> 01:04:47.040]  and so\n[01:04:47.040 --> 01:04:47.700]  the model\n[01:04:47.700 --> 01:04:48.400]  of generating\n[01:04:48.400 --> 01:04:49.140]  a document\n[01:04:49.140 --> 01:04:52.460]  which is\n[01:04:52.460 --> 01:04:54.080]  multivariate\n[01:04:54.080 --> 01:04:54.800]  Bernoulli\n[01:04:54.800 --> 01:04:56.680]  is that\n[01:04:56.680 --> 01:04:56.960]  we are\n[01:04:56.960 --> 01:04:57.680]  saying that\n[01:04:57.680 --> 01:04:58.560]  a document\n[01:04:58.560 --> 01:04:59.440]  you can\n[01:04:59.440 --> 01:04:59.840]  think of\n[01:04:59.840 --> 01:05:00.460]  a document\n[01:05:00.460 --> 01:05:01.180]  consisting\n[01:05:01.180 --> 01:05:01.680]  of\n[01:05:01.680 --> 01:05:05.980]  n\n[01:05:05.980 --> 01:05:06.940]  slots\n[01:05:06.940 --> 01:05:08.120]  one for\n[01:05:08.120 --> 01:05:09.160]  every word\n[01:05:09.160 --> 01:05:11.020]  or key\n[01:05:11.020 --> 01:05:11.380]  phrase\n[01:05:11.380 --> 01:05:13.680]  can you\n[01:05:13.680 --> 01:05:14.060]  hear me\n[01:05:14.060 --> 01:05:17.800]  can you\n[01:05:17.800 --> 01:05:18.760]  hear me\n[01:05:18.760 --> 01:05:22.400]  yes sir\n[01:05:22.400 --> 01:05:22.580]  you are\n[01:05:22.580 --> 01:05:22.880]  audible\n[01:05:22.880 --> 01:05:23.980]  okay\n[01:05:23.980 --> 01:05:24.520]  thank you\n[01:05:24.520 --> 01:05:25.760]  so essentially\n[01:05:25.760 --> 01:05:26.220]  we are saying\n[01:05:26.220 --> 01:05:26.940]  this document\n[01:05:26.940 --> 01:05:28.060]  has n\n[01:05:28.060 --> 01:05:29.080]  slots\n[01:05:29.080 --> 01:05:30.700]  one slot\n[01:05:30.700 --> 01:05:31.280]  for every\n[01:05:31.280 --> 01:05:31.820]  word or\n[01:05:31.820 --> 01:05:32.400]  key phrase\n[01:05:32.400 --> 01:05:34.080]  and what\n[01:05:34.080 --> 01:05:34.640]  we want to\n[01:05:34.640 --> 01:05:35.260]  decide is\n[01:05:35.260 --> 01:05:35.740]  does this\n[01:05:35.740 --> 01:05:36.280]  appear or\n[01:05:36.280 --> 01:05:36.800]  not appear\n[01:05:36.800 --> 01:05:37.080]  in the\n[01:05:37.080 --> 01:05:37.260]  document\n[01:05:37.260 --> 01:05:40.760]  so this\n[01:05:40.760 --> 01:05:41.060]  is the\n[01:05:41.060 --> 01:05:41.440]  equivalent\n[01:05:41.440 --> 01:05:42.060]  of our\n[01:05:42.060 --> 01:05:42.840]  binary\n[01:05:42.840 --> 01:05:43.760]  vectorization\n[01:05:43.760 --> 01:05:50.320]  what are\n[01:05:50.320 --> 01:05:51.080]  the parameters\n[01:05:51.080 --> 01:05:51.620]  of this\n[01:05:51.620 --> 01:05:51.940]  model\n[01:05:51.940 --> 01:05:56.660]  for each\n[01:05:56.660 --> 01:05:57.020]  element\n[01:05:57.020 --> 01:05:58.000]  we have\n[01:05:58.000 --> 01:05:59.040]  a separate\n[01:05:59.040 --> 01:05:59.700]  Bernoulli\n[01:05:59.700 --> 01:06:00.200]  distribution\n[01:06:00.200 --> 01:06:02.460]  the probability\n[01:06:02.460 --> 01:06:03.920]  of x1\n[01:06:03.920 --> 01:06:05.180]  being a\n[01:06:05.180 --> 01:06:05.460]  one\n[01:06:05.460 --> 01:06:08.320]  is going\n[01:06:08.320 --> 01:06:08.740]  to be\n[01:06:08.740 --> 01:06:09.060]  some\n[01:06:09.060 --> 01:06:09.660]  probability\n[01:06:09.660 --> 01:06:10.140]  theta\n[01:06:10.140 --> 01:06:10.460]  one\n[01:06:10.460 --> 01:06:13.780]  the probability\n[01:06:13.780 --> 01:06:15.500]  of x2\n[01:06:15.500 --> 01:06:16.060]  equal to\n[01:06:16.060 --> 01:06:16.380]  one\n[01:06:16.380 --> 01:06:18.060]  is going\n[01:06:18.060 --> 01:06:18.580]  to have\n[01:06:18.580 --> 01:06:19.200]  a different\n[01:06:19.200 --> 01:06:19.740]  probability\n[01:06:19.740 --> 01:06:23.580]  right\n[01:06:23.580 --> 01:06:24.160]  and so\n[01:06:24.160 --> 01:06:24.500]  what we\n[01:06:24.500 --> 01:06:24.900]  are saying\n[01:06:24.900 --> 01:06:25.260]  here\n[01:06:25.260 --> 01:06:26.740]  is\n[01:06:26.740 --> 01:06:29.720]  that for\n[01:06:29.720 --> 01:06:30.480]  each one\n[01:06:30.480 --> 01:06:31.140]  of these\n[01:06:31.140 --> 01:06:33.300]  words\n[01:06:33.300 --> 01:06:35.100]  we have\n[01:06:35.100 --> 01:06:36.440]  its own\n[01:06:36.440 --> 01:06:38.180]  parameter\n[01:06:38.180 --> 01:06:39.880]  theta\n[01:06:39.880 --> 01:06:40.300]  n\n[01:06:40.300 --> 01:06:43.320]  so there\n[01:06:43.320 --> 01:06:43.760]  are n\n[01:06:43.760 --> 01:06:44.320]  parameters\n[01:06:44.320 --> 01:06:44.820]  here\n[01:06:44.820 --> 01:06:45.760]  that we\n[01:06:45.760 --> 01:06:46.120]  need to\n[01:06:46.120 --> 01:06:46.420]  learn\n[01:06:46.420 --> 01:06:49.740]  but why\n[01:06:49.740 --> 01:06:50.060]  are we\n[01:06:50.060 --> 01:06:50.320]  saying\n[01:06:50.320 --> 01:06:51.200]  components\n[01:06:51.200 --> 01:06:55.020]  think of\n[01:06:55.020 --> 01:06:55.480]  components\n[01:06:55.480 --> 01:06:55.840]  as\n[01:06:55.840 --> 01:06:56.280]  topics\n[01:06:56.280 --> 01:07:03.240]  or\n[01:07:03.240 --> 01:07:03.860]  actually\n[01:07:03.860 --> 01:07:04.300]  let's not\n[01:07:04.300 --> 01:07:04.500]  even\n[01:07:04.500 --> 01:07:05.080]  confuse\n[01:07:05.080 --> 01:07:05.340]  it with\n[01:07:05.340 --> 01:07:05.700]  topics\n[01:07:05.700 --> 01:07:06.200]  right now\n[01:07:06.200 --> 01:07:06.700]  think of\n[01:07:06.700 --> 01:07:07.140]  each of\n[01:07:07.140 --> 01:07:07.860]  these as\n[01:07:07.860 --> 01:07:08.720]  classes\n[01:07:08.720 --> 01:07:10.880]  we are\n[01:07:10.880 --> 01:07:11.280]  talking\n[01:07:11.280 --> 01:07:11.860]  about\n[01:07:11.860 --> 01:07:15.560]  the\n[01:07:15.560 --> 01:07:16.220]  corpus\n[01:07:16.220 --> 01:07:17.260]  as a\n[01:07:17.260 --> 01:07:17.540]  whole\n[01:07:17.540 --> 01:07:18.140]  which is\n[01:07:18.140 --> 01:07:18.560]  a set\n[01:07:18.560 --> 01:07:18.740]  of\n[01:07:18.740 --> 01:07:19.420]  documents\n[01:07:19.420 --> 01:07:22.880]  being\n[01:07:22.880 --> 01:07:24.180]  generated\n[01:07:24.180 --> 01:07:24.860]  from a\n[01:07:24.860 --> 01:07:25.300]  mixture\n[01:07:25.300 --> 01:07:25.780]  model\n[01:07:25.780 --> 01:07:29.060]  so\n[01:07:29.060 --> 01:07:29.740]  whenever we\n[01:07:29.740 --> 01:07:30.340]  are generating\n[01:07:30.340 --> 01:07:30.980]  a document\n[01:07:30.980 --> 01:07:31.640]  we first\n[01:07:31.640 --> 01:07:32.200]  decide\n[01:07:32.200 --> 01:07:33.800]  which\n[01:07:33.800 --> 01:07:34.320]  component\n[01:07:34.320 --> 01:07:34.700]  are we\n[01:07:34.700 --> 01:07:35.040]  going to\n[01:07:35.040 --> 01:07:35.280]  call\n[01:07:35.280 --> 01:07:35.540]  which\n[01:07:35.540 --> 01:07:35.980]  class\n[01:07:35.980 --> 01:07:36.260]  are we\n[01:07:36.260 --> 01:07:36.640]  going to\n[01:07:36.640 --> 01:07:37.040]  generate\n[01:07:37.040 --> 01:07:37.600]  a document\n[01:07:37.600 --> 01:07:38.760]  and depending\n[01:07:38.760 --> 01:07:39.140]  on the\n[01:07:39.140 --> 01:07:39.660]  class\n[01:07:39.660 --> 01:07:42.960]  we will\n[01:07:42.960 --> 01:07:44.080]  generate\n[01:07:44.080 --> 01:07:45.040]  the words\n[01:07:45.040 --> 01:07:45.960]  that appear\n[01:07:45.960 --> 01:07:47.080]  and to\n[01:07:47.080 --> 01:07:47.740]  generate the\n[01:07:47.740 --> 01:07:48.260]  words that\n[01:07:48.260 --> 01:07:48.860]  appear we\n[01:07:48.860 --> 01:07:49.220]  need\n[01:07:49.220 --> 01:07:49.820]  n\n[01:07:49.820 --> 01:07:51.920]  parameters\n[01:07:51.920 --> 01:07:53.440]  so the\n[01:07:53.440 --> 01:07:54.060]  total number\n[01:07:54.060 --> 01:07:54.680]  of parameters\n[01:07:54.680 --> 01:07:55.160]  that we\n[01:07:55.160 --> 01:07:55.780]  need here\n[01:07:55.780 --> 01:07:56.460]  is if we\n[01:07:56.460 --> 01:07:56.920]  have k\n[01:07:56.920 --> 01:07:57.380]  classes\n[01:07:57.380 --> 01:07:59.420]  and we\n[01:07:59.420 --> 01:07:59.940]  have n\n[01:07:59.940 --> 01:08:00.620]  parameters\n[01:08:00.620 --> 01:08:01.680]  for each\n[01:08:01.680 --> 01:08:02.120]  class\n[01:08:02.120 --> 01:08:02.740]  we have\n[01:08:02.740 --> 01:08:03.460]  n times\n[01:08:03.460 --> 01:08:03.800]  k\n[01:08:03.800 --> 01:08:06.640]  parameters\n[01:08:06.640 --> 01:08:09.180]  but each\n[01:08:09.180 --> 01:08:09.520]  of the\n[01:08:09.520 --> 01:08:10.000]  classes\n[01:08:10.000 --> 01:08:10.800]  themselves\n[01:08:10.800 --> 01:08:12.940]  also have\n[01:08:12.940 --> 01:08:15.200]  a probability\n[01:08:15.200 --> 01:08:15.860]  of being\n[01:08:15.860 --> 01:08:16.860]  asked to be\n[01:08:16.860 --> 01:08:17.320]  generated\n[01:08:17.320 --> 01:08:19.020]  and so\n[01:08:19.020 --> 01:08:19.500]  we have\n[01:08:19.500 --> 01:08:20.080]  the probability\n[01:08:20.080 --> 01:08:21.060]  of c1\n[01:08:21.060 --> 01:08:24.740]  is equal\n[01:08:24.740 --> 01:08:25.920]  to\n[01:08:25.920 --> 01:08:28.300]  theta c1\n[01:08:28.300 --> 01:08:29.580]  probability\n[01:08:29.580 --> 01:08:30.680]  of c2\n[01:08:30.680 --> 01:08:31.360]  is equal\n[01:08:31.360 --> 01:08:31.900]  to theta\n[01:08:31.900 --> 01:08:32.640]  c2\n[01:08:32.640 --> 01:08:34.160]  till the\n[01:08:34.160 --> 01:08:34.720]  probability\n[01:08:34.720 --> 01:08:36.880]  of ck\n[01:08:36.880 --> 01:08:37.620]  which is\n[01:08:37.620 --> 01:08:38.320]  equal to\n[01:08:38.320 --> 01:08:40.200]  1 minus\n[01:08:40.200 --> 01:08:40.840]  the sum\n[01:08:40.840 --> 01:08:41.700]  of probability\n[01:08:41.700 --> 01:08:43.620]  of ck\n[01:08:43.620 --> 01:08:45.860]  minus cj\n[01:08:45.860 --> 01:08:46.660]  minus 1\n[01:08:46.660 --> 01:08:48.700]  where j\n[01:08:48.700 --> 01:08:49.520]  goes from\n[01:08:49.520 --> 01:08:51.000]  1 to\n[01:08:51.000 --> 01:08:52.560]  k\n[01:08:52.560 --> 01:08:54.440]  actually\n[01:08:54.440 --> 01:08:57.620]  1 minus\n[01:08:57.620 --> 01:08:58.700]  sigma\n[01:08:58.700 --> 01:08:59.260]  of\n[01:08:59.260 --> 01:09:01.960]  theta\n[01:09:01.960 --> 01:09:02.860]  cj\n[01:09:02.860 --> 01:09:03.860]  where j\n[01:09:03.860 --> 01:09:04.540]  goes from\n[01:09:04.540 --> 01:09:05.020]  1 to\n[01:09:05.020 --> 01:09:05.600]  k minus\n[01:09:05.600 --> 01:09:05.820]  1\n[01:09:05.820 --> 01:09:08.420]  so there\n[01:09:08.420 --> 01:09:08.880]  are k\n[01:09:08.880 --> 01:09:09.500]  minus 1\n[01:09:09.500 --> 01:09:09.820]  more\n[01:09:09.820 --> 01:09:10.380]  parameters\n[01:09:10.380 --> 01:09:10.920]  that we\n[01:09:10.920 --> 01:09:11.020]  need\n[01:09:11.020 --> 01:09:11.960]  so the\n[01:09:11.960 --> 01:09:12.280]  total\n[01:09:12.280 --> 01:09:12.740]  number of\n[01:09:12.740 --> 01:09:13.260]  parameters\n[01:09:13.260 --> 01:09:13.680]  we would\n[01:09:13.680 --> 01:09:13.980]  need\n[01:09:13.980 --> 01:09:15.780]  is this\n[01:09:15.780 --> 01:09:20.200]  so that\n[01:09:20.200 --> 01:09:20.560]  is the\n[01:09:20.560 --> 01:09:21.400]  multivariate\n[01:09:21.400 --> 01:09:22.020]  Bernoulli\n[01:09:22.020 --> 01:09:22.340]  model\n[01:09:22.340 --> 01:09:24.660]  what about\n[01:09:24.660 --> 01:09:24.940]  the\n[01:09:24.940 --> 01:09:25.900]  multinomial\n[01:09:25.900 --> 01:09:26.640]  distribution\n[01:09:26.640 --> 01:09:26.680]  distribution\n[01:09:26.680 --> 01:09:35.400]  is where\n[01:09:35.400 --> 01:09:35.980]  we have\n[01:09:35.980 --> 01:09:36.160]  a\n[01:09:36.160 --> 01:09:36.820]  categorical\n[01:09:36.820 --> 01:09:37.140]  variable\n[01:09:37.140 --> 01:09:40.660]  and so\n[01:09:40.660 --> 01:09:41.060]  what we\n[01:09:41.060 --> 01:09:41.420]  are doing\n[01:09:41.420 --> 01:09:42.300]  here is\n[01:09:42.300 --> 01:09:42.580]  we are\n[01:09:42.580 --> 01:09:43.180]  saying for\n[01:09:43.180 --> 01:09:44.260]  every element\n[01:09:44.260 --> 01:09:44.640]  of the\n[01:09:44.640 --> 01:09:45.220]  vocabulary\n[01:09:45.220 --> 01:09:45.780]  we call\n[01:09:45.780 --> 01:09:46.620]  them v1\n[01:09:46.620 --> 01:09:47.260]  v2\n[01:09:47.260 --> 01:09:50.140]  till vm\n[01:09:50.140 --> 01:09:53.280]  we have\n[01:09:53.280 --> 01:09:54.100]  a probability\n[01:09:54.100 --> 01:09:55.200]  of it\n[01:09:55.200 --> 01:09:55.640]  being\n[01:09:55.640 --> 01:09:57.060]  chosen as\n[01:09:57.060 --> 01:09:58.320]  a word\n[01:09:58.320 --> 01:09:58.820]  or key\n[01:09:58.820 --> 01:09:59.220]  phrase\n[01:09:59.220 --> 01:09:59.640]  in the\n[01:09:59.640 --> 01:09:59.980]  document\n[01:09:59.980 --> 01:10:01.560]  so we\n[01:10:01.560 --> 01:10:01.900]  have a\n[01:10:01.900 --> 01:10:02.320]  document\n[01:10:02.320 --> 01:10:05.320]  we now\n[01:10:05.320 --> 01:10:05.960]  decide on\n[01:10:05.960 --> 01:10:06.480]  a length\n[01:10:06.480 --> 01:10:06.840]  of the\n[01:10:06.840 --> 01:10:07.280]  document\n[01:10:07.280 --> 01:10:11.180]  and so\n[01:10:11.180 --> 01:10:11.440]  if we\n[01:10:11.440 --> 01:10:11.780]  say the\n[01:10:11.780 --> 01:10:12.080]  length\n[01:10:12.080 --> 01:10:12.740]  is\n[01:10:12.740 --> 01:10:13.700]  k\n[01:10:13.700 --> 01:10:16.500]  let's not\n[01:10:16.500 --> 01:10:16.820]  call it\n[01:10:16.820 --> 01:10:17.320]  k because\n[01:10:17.320 --> 01:10:17.580]  we've\n[01:10:17.580 --> 01:10:17.820]  already\n[01:10:17.820 --> 01:10:19.760]  let's call\n[01:10:19.760 --> 01:10:20.300]  it capital\n[01:10:20.300 --> 01:10:20.560]  n\n[01:10:20.560 --> 01:10:24.500]  we have\n[01:10:24.500 --> 01:10:25.720]  capital n\n[01:10:25.720 --> 01:10:26.560]  slots that\n[01:10:26.560 --> 01:10:26.900]  need to\n[01:10:26.900 --> 01:10:27.420]  be filled\n[01:10:27.420 --> 01:10:31.800]  again we\n[01:10:31.800 --> 01:10:32.300]  choose the\n[01:10:32.300 --> 01:10:34.140]  class to\n[01:10:34.140 --> 01:10:34.560]  which the\n[01:10:34.560 --> 01:10:34.980]  document\n[01:10:34.980 --> 01:10:36.020]  belongs and\n[01:10:36.020 --> 01:10:36.540]  then we\n[01:10:36.540 --> 01:10:38.060]  choose from\n[01:10:38.060 --> 01:10:38.960]  the probability\n[01:10:38.960 --> 01:10:40.960]  of words\n[01:10:40.960 --> 01:10:41.820]  given\n[01:10:41.820 --> 01:10:44.940]  the class\n[01:10:44.940 --> 01:10:45.340]  that has\n[01:10:45.340 --> 01:10:45.940]  been chosen\n[01:10:45.940 --> 01:10:48.380]  and what\n[01:10:48.380 --> 01:10:48.940]  is that\n[01:10:48.940 --> 01:10:49.940]  that is\n[01:10:49.940 --> 01:10:50.380]  this\n[01:10:50.380 --> 01:10:52.400]  categorical\n[01:10:52.400 --> 01:10:52.960]  distribution\n[01:10:52.960 --> 01:10:58.920]  and what\n[01:10:58.920 --> 01:10:59.220]  we are\n[01:10:59.220 --> 01:10:59.820]  saying now\n[01:10:59.820 --> 01:11:00.260]  is we\n[01:11:00.260 --> 01:11:01.040]  are choosing\n[01:11:01.040 --> 01:11:03.760]  from all\n[01:11:03.760 --> 01:11:04.080]  of our\n[01:11:04.080 --> 01:11:04.660]  vocabulary\n[01:11:04.660 --> 01:11:05.500]  based on\n[01:11:05.500 --> 01:11:05.760]  these\n[01:11:05.760 --> 01:11:06.360]  probabilities\n[01:11:06.360 --> 01:11:07.400]  we are\n[01:11:07.400 --> 01:11:07.780]  choosing\n[01:11:07.780 --> 01:11:08.480]  words at\n[01:11:08.480 --> 01:11:08.860]  random\n[01:11:08.860 --> 01:11:10.840]  and\n[01:11:10.840 --> 01:11:12.440]  filling our\n[01:11:12.440 --> 01:11:13.280]  slots with\n[01:11:13.280 --> 01:11:14.100]  those words\n[01:11:14.100 --> 01:11:18.860]  so what\n[01:11:18.860 --> 01:11:19.100]  are we\n[01:11:19.100 --> 01:11:19.440]  going to\n[01:11:19.440 --> 01:11:19.920]  end up\n[01:11:19.920 --> 01:11:20.300]  with\n[01:11:20.300 --> 01:11:20.960]  we are\n[01:11:20.960 --> 01:11:21.420]  going to\n[01:11:21.420 --> 01:11:21.940]  have\n[01:11:21.940 --> 01:11:22.820]  n\n[01:11:22.820 --> 01:11:24.880]  selections\n[01:11:24.880 --> 01:11:29.140]  from\n[01:11:29.140 --> 01:11:29.900]  m\n[01:11:29.900 --> 01:11:31.360]  possible\n[01:11:31.360 --> 01:11:32.040]  values\n[01:11:32.040 --> 01:11:35.720]  which are\n[01:11:35.720 --> 01:11:36.260]  each of\n[01:11:36.260 --> 01:11:36.580]  the\n[01:11:36.580 --> 01:11:37.880]  elements of\n[01:11:37.880 --> 01:11:38.000]  a\n[01:11:38.000 --> 01:11:38.380]  vocabulary\n[01:11:38.380 --> 01:11:42.680]  so what\n[01:11:42.680 --> 01:11:43.060]  is the\n[01:11:43.060 --> 01:11:43.500]  probability\n[01:11:43.500 --> 01:11:44.360]  of a\n[01:11:44.360 --> 01:11:44.800]  document\n[01:11:44.800 --> 01:11:45.300]  going to\n[01:11:45.300 --> 01:11:45.500]  be\n[01:11:45.500 --> 01:11:47.580]  given\n[01:11:47.580 --> 01:11:49.140]  that it\n[01:11:49.140 --> 01:11:49.520]  has been\n[01:11:49.520 --> 01:11:50.080]  generated\n[01:11:50.080 --> 01:11:51.040]  from a\n[01:11:51.040 --> 01:11:51.300]  particular\n[01:11:51.300 --> 01:11:51.800]  class\n[01:11:51.800 --> 01:11:55.000]  when we\n[01:11:55.000 --> 01:11:55.540]  see a\n[01:11:55.540 --> 01:11:56.040]  document\n[01:11:56.040 --> 01:11:59.420]  it is\n[01:11:59.420 --> 01:12:00.000]  of length\n[01:12:00.000 --> 01:12:00.480]  n\n[01:12:00.480 --> 01:12:02.280]  and there\n[01:12:02.280 --> 01:12:02.840]  are m\n[01:12:02.840 --> 01:12:03.820]  values\n[01:12:03.820 --> 01:12:04.160]  here\n[01:12:04.160 --> 01:12:06.780]  from which\n[01:12:06.780 --> 01:12:07.160]  we can\n[01:12:07.160 --> 01:12:07.580]  choose\n[01:12:07.580 --> 01:12:08.320]  these n\n[01:12:08.320 --> 01:12:09.820]  what we\n[01:12:09.820 --> 01:12:10.180]  are going\n[01:12:10.180 --> 01:12:10.620]  to end\n[01:12:10.620 --> 01:12:11.220]  up with\n[01:12:11.220 --> 01:12:16.620]  is\n[01:12:16.620 --> 01:12:17.260]  essentially\n[01:12:17.260 --> 01:12:18.480]  this here\n[01:12:18.480 --> 01:12:23.440]  this capital\n[01:12:23.440 --> 01:12:23.900]  n\n[01:12:23.900 --> 01:12:26.540]  is the\n[01:12:26.540 --> 01:12:26.980]  number of\n[01:12:26.980 --> 01:12:27.520]  trials\n[01:12:27.520 --> 01:12:30.420]  and these\n[01:12:30.420 --> 01:12:30.920]  are\n[01:12:30.920 --> 01:12:37.740]  the\n[01:12:37.740 --> 01:12:38.400]  occurrences\n[01:12:38.400 --> 01:12:39.760]  the number\n[01:12:39.760 --> 01:12:40.460]  of times\n[01:12:40.460 --> 01:12:41.200]  each of\n[01:12:41.200 --> 01:12:42.440]  our words\n[01:12:42.440 --> 01:12:43.000]  are occurring\n[01:12:43.000 --> 01:12:43.980]  so what we\n[01:12:43.980 --> 01:12:44.460]  had said\n[01:12:44.460 --> 01:12:45.300]  in terms of\n[01:12:45.300 --> 01:12:45.740]  our\n[01:12:45.740 --> 01:12:46.760]  notation\n[01:12:46.760 --> 01:12:47.200]  here\n[01:12:47.200 --> 01:12:50.040]  is we\n[01:12:50.040 --> 01:12:50.680]  have n\n[01:12:50.680 --> 01:12:51.300]  trials\n[01:12:51.300 --> 01:12:53.800]  we are\n[01:12:53.800 --> 01:12:54.600]  going to\n[01:12:54.600 --> 01:12:56.420]  say\n[01:12:56.420 --> 01:12:57.580]  n\n[01:12:57.580 --> 01:12:59.800]  divided\n[01:12:59.800 --> 01:13:00.320]  by\n[01:13:00.320 --> 01:13:01.320]  x1\n[01:13:01.320 --> 01:13:02.680]  factorial\n[01:13:02.680 --> 01:13:03.500]  x2\n[01:13:03.500 --> 01:13:04.120]  factorial\n[01:13:04.120 --> 01:13:05.380]  till\n[01:13:05.380 --> 01:13:06.460]  xm\n[01:13:06.460 --> 01:13:07.280]  factorial\n[01:13:07.280 --> 01:13:09.040]  where each\n[01:13:09.040 --> 01:13:09.520]  of these\n[01:13:09.520 --> 01:13:10.020]  x1\n[01:13:10.020 --> 01:13:10.420]  x2\n[01:13:10.420 --> 01:13:10.960]  and xm\n[01:13:10.960 --> 01:13:11.460]  are the\n[01:13:11.460 --> 01:13:12.220]  frequency\n[01:13:12.220 --> 01:13:13.040]  with which\n[01:13:13.040 --> 01:13:14.140]  each of\n[01:13:14.140 --> 01:13:15.060]  these elements\n[01:13:15.060 --> 01:13:16.060]  appear\n[01:13:16.060 --> 01:13:17.660]  in our\n[01:13:17.660 --> 01:13:18.300]  n\n[01:13:18.300 --> 01:13:21.600]  trials\n[01:13:21.600 --> 01:13:22.060]  that we\n[01:13:22.060 --> 01:13:22.220]  have\n[01:13:22.220 --> 01:13:25.260]  so this\n[01:13:25.260 --> 01:13:25.620]  is the\n[01:13:25.620 --> 01:13:26.180]  number of\n[01:13:26.180 --> 01:13:26.680]  ways\n[01:13:26.680 --> 01:13:27.680]  in which\n[01:13:27.680 --> 01:13:28.420]  n\n[01:13:28.420 --> 01:13:28.940]  slots\n[01:13:28.940 --> 01:13:29.500]  can be\n[01:13:29.500 --> 01:13:29.900]  filled\n[01:13:29.900 --> 01:13:30.360]  with\n[01:13:30.360 --> 01:13:31.060]  these\n[01:13:31.060 --> 01:13:31.720]  frequency\n[01:13:31.720 --> 01:13:32.180]  of\n[01:13:32.180 --> 01:13:35.500]  m\n[01:13:35.500 --> 01:13:36.720]  vocabulary\n[01:13:36.720 --> 01:13:37.600]  words\n[01:13:37.600 --> 01:13:39.000]  and we\n[01:13:39.000 --> 01:13:39.880]  then multiply\n[01:13:39.880 --> 01:13:40.360]  by the\n[01:13:40.360 --> 01:13:41.040]  probabilities\n[01:13:41.040 --> 01:13:41.640]  right\n[01:13:41.640 --> 01:13:42.540]  so p1\n[01:13:42.540 --> 01:13:42.920]  to the\n[01:13:42.920 --> 01:13:43.500]  power of\n[01:13:43.500 --> 01:13:44.240]  x1\n[01:13:44.240 --> 01:13:44.780]  times\n[01:13:44.780 --> 01:13:45.380]  p2\n[01:13:45.380 --> 01:13:45.700]  to the\n[01:13:45.700 --> 01:13:46.100]  power of\n[01:13:46.100 --> 01:13:46.580]  x2\n[01:13:46.580 --> 01:13:48.020]  all the\n[01:13:48.020 --> 01:13:48.340]  way to\n[01:13:48.340 --> 01:13:49.160]  pm\n[01:13:49.160 --> 01:13:49.580]  to the\n[01:13:49.580 --> 01:13:50.200]  power of\n[01:13:50.200 --> 01:13:50.720]  x\n[01:13:50.720 --> 01:13:54.140]  where p1\n[01:13:54.140 --> 01:13:54.800]  to pm\n[01:13:54.800 --> 01:13:55.580]  are these\n[01:13:55.580 --> 01:13:56.220]  probabilities\n[01:13:56.220 --> 01:13:58.900]  right so\n[01:13:58.900 --> 01:13:59.340]  that's what\n[01:13:59.340 --> 01:13:59.580]  we are\n[01:13:59.580 --> 01:14:00.100]  representing\n[01:14:00.100 --> 01:14:02.380]  okay\n[01:14:02.380 --> 01:14:11.200]  so the\n[01:14:11.200 --> 01:14:12.760]  multinomial\n[01:14:12.760 --> 01:14:14.120]  distribution\n[01:14:14.120 --> 01:14:15.540]  model of\n[01:14:15.540 --> 01:14:16.080]  a document\n[01:14:16.080 --> 01:14:17.980]  is different\n[01:14:17.980 --> 01:14:18.480]  from a\n[01:14:18.480 --> 01:14:19.200]  multivariate\n[01:14:19.200 --> 01:14:19.700]  bernoudi\n[01:14:19.700 --> 01:14:19.960]  model\n[01:14:19.960 --> 01:14:22.220]  where we\n[01:14:22.220 --> 01:14:22.700]  are saying\n[01:14:22.700 --> 01:14:23.200]  that the\n[01:14:23.200 --> 01:14:24.000]  words that\n[01:14:24.000 --> 01:14:24.560]  appear in\n[01:14:24.560 --> 01:14:25.140]  the document\n[01:14:25.140 --> 01:14:27.520]  they can\n[01:14:27.520 --> 01:14:28.380]  have a\n[01:14:28.380 --> 01:14:28.980]  frequency\n[01:14:28.980 --> 01:14:29.720]  associated\n[01:14:29.720 --> 01:14:30.240]  with them\n[01:14:30.240 --> 01:14:31.420]  they fill\n[01:14:31.420 --> 01:14:32.480]  up n\n[01:14:32.480 --> 01:14:33.380]  slots and\n[01:14:33.380 --> 01:14:33.900]  therefore we\n[01:14:33.900 --> 01:14:34.280]  have to\n[01:14:34.280 --> 01:14:34.900]  choose the\n[01:14:34.900 --> 01:14:35.400]  length of\n[01:14:35.400 --> 01:14:35.920]  a document\n[01:14:35.920 --> 01:14:36.380]  also\n[01:14:36.380 --> 01:14:40.040]  but once\n[01:14:40.040 --> 01:14:40.640]  we've chosen\n[01:14:40.640 --> 01:14:41.160]  the length\n[01:14:41.160 --> 01:14:41.440]  of the\n[01:14:41.440 --> 01:14:42.180]  document we\n[01:14:42.180 --> 01:14:42.500]  are\n[01:14:42.500 --> 01:14:44.640]  selecting\n[01:14:44.640 --> 01:14:45.500]  what to\n[01:14:45.500 --> 01:14:46.100]  fill these\n[01:14:46.100 --> 01:14:47.100]  documents by\n[01:14:47.100 --> 01:14:48.120]  a probability\n[01:14:48.120 --> 01:14:48.840]  distribution\n[01:14:48.840 --> 01:14:54.900]  okay and\n[01:14:54.900 --> 01:14:56.140]  that's the\n[01:14:56.140 --> 01:14:58.220]  multinomial\n[01:14:58.220 --> 01:14:59.660]  distribution\n[01:14:59.660 --> 01:15:00.120]  model\n[01:15:00.120 --> 01:15:01.620]  now given\n[01:15:01.620 --> 01:15:02.160]  these two\n[01:15:02.160 --> 01:15:02.760]  models\n[01:15:02.760 --> 01:15:04.700]  we end up\n[01:15:04.700 --> 01:15:06.160]  with if we\n[01:15:06.160 --> 01:15:06.800]  want to do a\n[01:15:06.800 --> 01:15:07.360]  classification\n[01:15:07.360 --> 01:15:10.020]  model we\n[01:15:10.020 --> 01:15:10.640]  end up\n[01:15:10.640 --> 01:15:11.160]  with\n[01:15:11.160 --> 01:15:14.640]  two\n[01:15:14.640 --> 01:15:16.500]  versions of\n[01:15:16.500 --> 01:15:17.060]  the naive\n[01:15:17.060 --> 01:15:17.840]  ways algorithm\n[01:15:17.840 --> 01:15:26.480]  remember that\n[01:15:26.480 --> 01:15:26.900]  the naive\n[01:15:26.900 --> 01:15:27.780]  base algorithm\n[01:15:27.780 --> 01:15:29.720]  made a\n[01:15:29.720 --> 01:15:30.260]  simplifying\n[01:15:30.260 --> 01:15:30.760]  assumption\n[01:15:30.760 --> 01:15:40.440]  if we\n[01:15:40.440 --> 01:15:41.120]  had n\n[01:15:41.120 --> 01:15:41.480]  input\n[01:15:41.480 --> 01:15:42.020]  variables\n[01:15:42.020 --> 01:15:43.640]  and we\n[01:15:43.640 --> 01:15:44.040]  had the\n[01:15:44.040 --> 01:15:44.820]  class c\n[01:15:44.820 --> 01:15:46.620]  and we\n[01:15:46.620 --> 01:15:47.280]  wanted to\n[01:15:47.280 --> 01:15:47.780]  know what\n[01:15:47.780 --> 01:15:48.680]  the probability\n[01:15:48.680 --> 01:15:51.920]  of a\n[01:15:51.920 --> 01:15:52.520]  class\n[01:15:52.520 --> 01:15:54.400]  given\n[01:15:54.400 --> 01:15:56.860]  a set\n[01:15:56.860 --> 01:15:57.320]  of input\n[01:15:57.320 --> 01:15:57.960]  values\n[01:15:57.960 --> 01:16:01.060]  evidence\n[01:16:01.060 --> 01:16:01.520]  e\n[01:16:01.520 --> 01:16:05.980]  we can\n[01:16:05.980 --> 01:16:06.340]  now\n[01:16:06.340 --> 01:16:07.940]  represent it\n[01:16:07.940 --> 01:16:09.140]  as\n[01:16:09.140 --> 01:16:10.060]  the\n[01:16:10.060 --> 01:16:10.700]  product\n[01:16:10.700 --> 01:16:12.920]  of\n[01:16:12.920 --> 01:16:14.620]  the\n[01:16:14.620 --> 01:16:15.220]  probability\n[01:16:15.220 --> 01:16:15.980]  of each\n[01:16:15.980 --> 01:16:16.720]  xi\n[01:16:16.720 --> 01:16:16.980]  i\n[01:16:16.980 --> 01:16:19.420]  given c\n[01:16:19.420 --> 01:16:21.980]  where i\n[01:16:21.980 --> 01:16:22.560]  goes from\n[01:16:22.560 --> 01:16:23.440]  1 to n\n[01:16:23.440 --> 01:16:25.400]  multiplied by\n[01:16:25.400 --> 01:16:25.620]  the\n[01:16:25.620 --> 01:16:26.920]  probability\n[01:16:26.920 --> 01:16:27.560]  of c\n[01:16:27.560 --> 01:16:28.660]  divided by\n[01:16:28.660 --> 01:16:30.460]  the probability\n[01:16:30.460 --> 01:16:30.920]  of\n[01:16:30.920 --> 01:16:33.660]  this is what\n[01:16:33.660 --> 01:16:33.980]  we had\n[01:16:33.980 --> 01:16:34.400]  done this\n[01:16:34.400 --> 01:16:34.720]  was the\n[01:16:34.720 --> 01:16:35.100]  naive\n[01:16:35.100 --> 01:16:35.600]  base\n[01:16:35.600 --> 01:16:37.980]  model\n[01:16:37.980 --> 01:16:38.440]  algorithm\n[01:16:38.440 --> 01:16:40.740]  now\n[01:16:40.740 --> 01:16:43.180]  why do\n[01:16:43.180 --> 01:16:43.580]  we have\n[01:16:43.580 --> 01:16:43.800]  two\n[01:16:43.800 --> 01:16:44.340]  different\n[01:16:44.340 --> 01:16:45.040]  versions of\n[01:16:45.040 --> 01:16:45.540]  the naive\n[01:16:45.540 --> 01:16:45.860]  base\n[01:16:45.860 --> 01:16:46.460]  algorithm\n[01:16:46.460 --> 01:16:48.380]  depending on\n[01:16:48.380 --> 01:16:50.020]  whether this\n[01:16:50.020 --> 01:16:50.440]  is a\n[01:16:50.440 --> 01:16:50.960]  binary\n[01:16:50.960 --> 01:16:51.700]  vector\n[01:16:51.700 --> 01:16:57.060]  or whether\n[01:16:57.060 --> 01:16:57.860]  this is a\n[01:16:57.860 --> 01:16:58.800]  count\n[01:16:58.800 --> 01:16:59.260]  vector\n[01:16:59.260 --> 01:17:01.440]  a count\n[01:17:01.440 --> 01:17:01.880]  vector\n[01:17:01.880 --> 01:17:04.300]  is really\n[01:17:04.300 --> 01:17:05.560]  required for\n[01:17:05.560 --> 01:17:05.800]  the\n[01:17:05.800 --> 01:17:06.200]  multi\n[01:17:06.200 --> 01:17:06.700]  momemium\n[01:17:06.700 --> 01:17:09.180]  document\n[01:17:09.180 --> 01:17:09.500]  model\n[01:17:09.500 --> 01:17:16.160]  why\n[01:17:16.160 --> 01:17:16.780]  because we\n[01:17:16.780 --> 01:17:17.160]  need to\n[01:17:17.160 --> 01:17:17.500]  know the\n[01:17:17.500 --> 01:17:18.060]  number of\n[01:17:18.060 --> 01:17:18.720]  times\n[01:17:18.720 --> 01:17:20.680]  n i t is\n[01:17:20.680 --> 01:17:21.240]  the number\n[01:17:21.240 --> 01:17:22.040]  of times\n[01:17:22.040 --> 01:17:25.080]  the tth\n[01:17:25.080 --> 01:17:25.580]  element\n[01:17:25.580 --> 01:17:27.920]  of the\n[01:17:27.920 --> 01:17:28.500]  vocabulary\n[01:17:28.500 --> 01:17:29.900]  appears\n[01:17:29.900 --> 01:17:34.720]  in the\n[01:17:34.720 --> 01:17:35.320]  document\n[01:17:35.320 --> 01:17:36.040]  t i\n[01:17:36.040 --> 01:17:40.220]  and that\n[01:17:40.220 --> 01:17:40.620]  is what\n[01:17:40.620 --> 01:17:41.480]  is represented\n[01:17:41.480 --> 01:17:42.160]  within the\n[01:17:42.160 --> 01:17:42.420]  count\n[01:17:42.420 --> 01:17:42.740]  vector\n[01:17:42.740 --> 01:17:43.640]  is the\n[01:17:43.640 --> 01:17:44.100]  number of\n[01:17:44.100 --> 01:17:44.580]  times that\n[01:17:44.580 --> 01:17:45.240]  word appears\n[01:17:45.240 --> 01:17:47.380]  and that's\n[01:17:47.380 --> 01:17:47.800]  why we\n[01:17:47.800 --> 01:17:48.900]  need this\n[01:17:48.900 --> 01:17:49.180]  count\n[01:17:49.180 --> 01:17:49.560]  vector\n[01:17:49.560 --> 01:17:50.580]  when we\n[01:17:50.580 --> 01:17:50.980]  want to\n[01:17:50.980 --> 01:17:51.400]  calculate\n[01:17:51.400 --> 01:17:51.820]  the\n[01:17:51.820 --> 01:17:52.300]  probability\n[01:17:52.300 --> 01:17:52.700]  of a\n[01:17:52.700 --> 01:17:53.220]  document\n[01:17:53.220 --> 01:17:54.580]  given\n[01:17:54.580 --> 01:17:55.720]  a class\n[01:17:55.720 --> 01:18:00.380]  when we\n[01:18:00.380 --> 01:18:00.860]  want to\n[01:18:00.860 --> 01:18:01.400]  use\n[01:18:01.400 --> 01:18:02.160]  instead\n[01:18:02.160 --> 01:18:04.480]  the\n[01:18:04.480 --> 01:18:06.940]  multivariate\n[01:18:06.940 --> 01:18:07.560]  Bernoulli\n[01:18:07.560 --> 01:18:08.480]  model\n[01:18:08.480 --> 01:18:12.040]  this d i\n[01:18:12.040 --> 01:18:13.260]  t is\n[01:18:13.260 --> 01:18:13.960]  either 1\n[01:18:13.960 --> 01:18:14.500]  or 0\n[01:18:14.500 --> 01:18:15.740]  where 1\n[01:18:15.740 --> 01:18:16.480]  is when\n[01:18:16.480 --> 01:18:17.780]  w t\n[01:18:17.780 --> 01:18:19.100]  appears\n[01:18:19.100 --> 01:18:21.960]  in di\n[01:18:21.960 --> 01:18:23.740]  and 0\n[01:18:23.740 --> 01:18:24.420]  when w\n[01:18:24.420 --> 01:18:24.680]  t\n[01:18:24.680 --> 01:18:26.560]  does not\n[01:18:26.560 --> 01:18:27.000]  appear\n[01:18:27.000 --> 01:18:31.640]  in di\n[01:18:31.640 --> 01:18:38.120]  right\n[01:18:38.120 --> 01:18:38.800]  and so\n[01:18:38.800 --> 01:18:40.160]  whenever we\n[01:18:40.160 --> 01:18:41.120]  are using\n[01:18:41.120 --> 01:18:42.100]  the multivariate\n[01:18:42.100 --> 01:18:42.740]  Bernoulli\n[01:18:42.740 --> 01:18:43.420]  model\n[01:18:43.420 --> 01:18:48.600]  the\n[01:18:48.600 --> 01:18:51.580]  value\n[01:18:51.580 --> 01:18:52.140]  here\n[01:18:52.140 --> 01:18:54.100]  how we\n[01:18:54.100 --> 01:18:54.600]  calculate\n[01:18:54.600 --> 01:18:55.260]  this\n[01:18:55.260 --> 01:18:56.960]  essentially\n[01:18:56.960 --> 01:18:58.480]  changes\n[01:18:58.480 --> 01:19:01.420]  out here\n[01:19:01.420 --> 01:19:02.560]  in the\n[01:19:02.560 --> 01:19:05.260]  multivariate\n[01:19:05.260 --> 01:19:05.800]  Bernoulli\n[01:19:05.800 --> 01:19:07.660]  we can\n[01:19:07.660 --> 01:19:08.100]  see we\n[01:19:08.100 --> 01:19:08.520]  are taking\n[01:19:08.520 --> 01:19:09.060]  the product\n[01:19:09.060 --> 01:19:09.460]  over the\n[01:19:09.460 --> 01:19:09.960]  vocabulary\n[01:19:09.960 --> 01:19:11.800]  and we\n[01:19:11.800 --> 01:19:12.400]  are using\n[01:19:12.400 --> 01:19:13.640]  the\n[01:19:13.640 --> 01:19:14.400]  Bernoulli\n[01:19:14.400 --> 01:19:20.120]  distributions\n[01:19:20.120 --> 01:19:20.760]  formula\n[01:19:20.760 --> 01:19:23.920]  right\n[01:19:23.920 --> 01:19:25.200]  p of x\n[01:19:25.200 --> 01:19:25.720]  to the\n[01:19:25.720 --> 01:19:26.500]  p of\n[01:19:26.500 --> 01:19:26.860]  theta\n[01:19:26.860 --> 01:19:31.000]  to the\n[01:19:31.000 --> 01:19:31.960]  power of\n[01:19:31.960 --> 01:19:33.140]  whatever\n[01:19:33.140 --> 01:19:33.380]  say\n[01:19:33.380 --> 01:19:33.620]  here\n[01:19:33.620 --> 01:19:36.040]  the p\n[01:19:36.040 --> 01:19:36.400]  to the\n[01:19:36.400 --> 01:19:36.840]  power of\n[01:19:36.840 --> 01:19:37.240]  x\n[01:19:37.240 --> 01:19:38.040]  times 1\n[01:19:38.040 --> 01:19:38.840]  minus p\n[01:19:38.840 --> 01:19:39.160]  to the\n[01:19:39.160 --> 01:19:39.620]  power of\n[01:19:39.620 --> 01:19:39.860]  1\n[01:19:39.860 --> 01:19:40.560]  minus x\n[01:19:40.560 --> 01:19:40.900]  is the\n[01:19:40.900 --> 01:19:41.300]  probability\n[01:19:41.300 --> 01:19:42.140]  of x\n[01:19:42.140 --> 01:19:44.300]  or we\n[01:19:44.300 --> 01:19:44.740]  can even\n[01:19:44.740 --> 01:19:45.360]  write this\n[01:19:45.360 --> 01:19:45.800]  as theta\n[01:19:45.800 --> 01:19:46.900]  we're doing\n[01:19:46.900 --> 01:19:47.300]  the same\n[01:19:47.300 --> 01:19:47.660]  thing\n[01:19:47.660 --> 01:19:51.060]  when we\n[01:19:51.060 --> 01:19:52.160]  are wanting\n[01:19:52.160 --> 01:19:53.840]  to use\n[01:19:53.840 --> 01:19:54.260]  the\n[01:19:54.260 --> 01:19:55.280]  multinomial\n[01:19:55.280 --> 01:19:56.960]  distribution\n[01:19:56.960 --> 01:20:00.320]  this has\n[01:20:00.320 --> 01:20:00.620]  to be\n[01:20:00.620 --> 01:20:01.180]  calculated\n[01:20:01.180 --> 01:20:02.240]  differently\n[01:20:02.240 --> 01:20:03.360]  and how\n[01:20:03.360 --> 01:20:03.720]  is it\n[01:20:03.720 --> 01:20:04.240]  calculated\n[01:20:04.240 --> 01:20:04.880]  differently\n[01:20:04.880 --> 01:20:05.240]  it's\n[01:20:05.240 --> 01:20:05.820]  basically\n[01:20:05.820 --> 01:20:07.740]  going to\n[01:20:07.740 --> 01:20:08.420]  use this\n[01:20:08.420 --> 01:20:08.840]  formula\n[01:20:08.840 --> 01:20:09.280]  here\n[01:20:09.280 --> 01:20:13.560]  which is\n[01:20:13.560 --> 01:20:14.280]  what we\n[01:20:14.280 --> 01:20:15.000]  have as\n[01:20:15.000 --> 01:20:15.520]  the formula\n[01:20:15.520 --> 01:20:16.680]  for the\n[01:20:16.680 --> 01:20:17.480]  multinomial\n[01:20:17.480 --> 01:20:18.040]  distribution\n[01:20:18.040 --> 01:20:19.940]  but we\n[01:20:19.940 --> 01:20:20.520]  are also\n[01:20:20.520 --> 01:20:21.120]  multiplying\n[01:20:21.120 --> 01:20:21.620]  by the\n[01:20:21.620 --> 01:20:22.260]  probability\n[01:20:22.260 --> 01:20:23.460]  of the\n[01:20:23.460 --> 01:20:23.940]  length\n[01:20:23.940 --> 01:20:24.480]  of the\n[01:20:24.480 --> 01:20:24.700]  document\n[01:20:24.700 --> 01:20:29.320]  and this\n[01:20:29.320 --> 01:20:29.760]  probability\n[01:20:29.760 --> 01:20:30.180]  of the\n[01:20:30.180 --> 01:20:30.460]  length\n[01:20:30.460 --> 01:20:30.780]  of the\n[01:20:30.780 --> 01:20:31.240]  document\n[01:20:31.240 --> 01:20:33.380]  we need\n[01:20:33.380 --> 01:20:33.780]  to figure\n[01:20:33.780 --> 01:20:34.280]  out what\n[01:20:34.280 --> 01:20:34.620]  is the\n[01:20:34.620 --> 01:20:35.260]  distribution\n[01:20:35.260 --> 01:20:35.840]  going to\n[01:20:35.840 --> 01:20:36.260]  be for\n[01:20:36.260 --> 01:20:36.620]  this\n[01:20:36.620 --> 01:20:41.840]  right so\n[01:20:41.840 --> 01:20:42.560]  that is\n[01:20:42.560 --> 01:20:43.380]  what changes\n[01:20:43.380 --> 01:20:45.220]  in the\n[01:20:45.220 --> 01:20:45.560]  naive\n[01:20:45.560 --> 01:20:45.860]  base\n[01:20:45.860 --> 01:20:46.160]  algorithm\n[01:20:46.160 --> 01:20:48.020]  depending\n[01:20:48.020 --> 01:20:49.200]  on what\n[01:20:49.200 --> 01:20:49.800]  kind of\n[01:20:49.800 --> 01:20:50.640]  vectorizer\n[01:20:50.640 --> 01:20:51.040]  we are\n[01:20:51.040 --> 01:20:51.360]  using\n[01:20:51.360 --> 01:20:51.820]  here when\n[01:20:51.820 --> 01:20:52.160]  we use\n[01:20:52.160 --> 01:20:52.640]  a binary\n[01:20:52.640 --> 01:20:53.380]  vectorizer\n[01:20:53.380 --> 01:20:56.460]  it works\n[01:20:56.460 --> 01:20:57.000]  pretty much\n[01:20:57.000 --> 01:20:57.420]  the same\n[01:20:57.420 --> 01:20:57.760]  way as\n[01:20:57.760 --> 01:20:58.360]  the standard\n[01:20:58.360 --> 01:20:58.840]  naive\n[01:20:58.840 --> 01:20:59.260]  base\n[01:20:59.260 --> 01:20:59.960]  when we\n[01:20:59.960 --> 01:21:00.680]  choose a\n[01:21:00.680 --> 01:21:01.000]  count\n[01:21:01.000 --> 01:21:01.760]  vectorizer\n[01:21:01.760 --> 01:21:03.500]  this part\n[01:21:03.500 --> 01:21:04.660]  changes\n[01:21:04.660 --> 01:21:06.280]  and we\n[01:21:06.280 --> 01:21:06.640]  need to\n[01:21:06.640 --> 01:21:07.480]  use instead\n[01:21:07.480 --> 01:21:08.040]  this\n[01:21:08.040 --> 01:21:08.520]  distribution\n[01:21:08.520 --> 01:21:09.300]  which is\n[01:21:09.300 --> 01:21:09.920]  the probability\n[01:21:09.920 --> 01:21:10.680]  of bi\n[01:21:10.680 --> 01:21:11.500]  given cj\n[01:21:11.500 --> 01:21:16.320]  okay I\n[01:21:16.320 --> 01:21:16.640]  know this\n[01:21:16.640 --> 01:21:17.020]  has got a\n[01:21:17.020 --> 01:21:17.500]  little heavy\n[01:21:17.500 --> 01:21:18.140]  I'm aware\n[01:21:18.140 --> 01:21:19.020]  of that so\n[01:21:19.020 --> 01:21:21.020]  let's leave\n[01:21:21.020 --> 01:21:21.440]  it here\n[01:21:21.440 --> 01:21:21.980]  for now\n[01:21:21.980 --> 01:21:22.400]  and let\n[01:21:22.400 --> 01:21:23.720]  me go\n[01:21:23.720 --> 01:21:24.160]  to\n[01:21:24.160 --> 01:21:27.520]  some\n[01:21:27.520 --> 01:21:27.920]  code\n[01:21:27.920 --> 01:21:36.320]  and I'll\n[01:21:36.320 --> 01:21:36.760]  be sharing\n[01:21:36.760 --> 01:21:37.520]  this in\n[01:21:37.520 --> 01:21:37.840]  your\n[01:21:37.840 --> 01:21:39.220]  educo lab\n[01:21:39.220 --> 01:21:39.640]  as well\n[01:21:39.640 --> 01:21:42.900]  so here\n[01:21:42.900 --> 01:21:43.800]  there are\n[01:21:43.800 --> 01:21:43.960]  going to\n[01:21:43.960 --> 01:21:45.400]  be two\n[01:21:45.400 --> 01:21:48.460]  ipython\n[01:21:48.460 --> 01:21:48.820]  notebooks\n[01:21:48.820 --> 01:21:49.160]  that I\n[01:21:49.160 --> 01:21:49.480]  share\n[01:21:49.480 --> 01:21:51.160]  one is\n[01:21:51.160 --> 01:21:52.360]  the text\n[01:21:52.360 --> 01:21:53.040]  vectorizer\n[01:21:53.040 --> 01:21:53.440]  and the\n[01:21:53.440 --> 01:21:53.880]  other is\n[01:21:53.880 --> 01:21:54.420]  toxic\n[01:21:54.420 --> 01:21:55.480]  comment\n[01:21:55.480 --> 01:21:56.100]  classification\n[01:21:56.100 --> 01:21:59.120]  here we\n[01:21:59.120 --> 01:21:59.840]  are using\n[01:21:59.840 --> 01:22:01.060]  in sklearn\n[01:22:01.060 --> 01:22:02.480]  a standard\n[01:22:02.480 --> 01:22:03.220]  data set\n[01:22:03.220 --> 01:22:04.480]  which consists\n[01:22:04.480 --> 01:22:05.640]  of news\n[01:22:05.640 --> 01:22:06.000]  groups\n[01:22:06.000 --> 01:22:06.740]  20 news\n[01:22:06.740 --> 01:22:07.100]  groups\n[01:22:07.100 --> 01:22:09.260]  so in\n[01:22:09.260 --> 01:22:09.500]  the good\n[01:22:09.500 --> 01:22:10.040]  old days\n[01:22:10.040 --> 01:22:11.260]  people\n[01:22:11.260 --> 01:22:12.320]  would have\n[01:22:12.320 --> 01:22:12.780]  news\n[01:22:12.780 --> 01:22:13.120]  groups\n[01:22:13.120 --> 01:22:13.500]  where they\n[01:22:13.500 --> 01:22:13.760]  would\n[01:22:13.760 --> 01:22:14.420]  email\n[01:22:14.420 --> 01:22:15.560]  into this\n[01:22:15.560 --> 01:22:16.220]  news\n[01:22:16.220 --> 01:22:16.560]  group\n[01:22:16.560 --> 01:22:17.200]  email\n[01:22:17.200 --> 01:22:17.740]  address\n[01:22:17.740 --> 01:22:19.160]  and then\n[01:22:19.160 --> 01:22:19.600]  all of\n[01:22:19.600 --> 01:22:20.100]  the messages\n[01:22:20.100 --> 01:22:21.100]  associated\n[01:22:21.100 --> 01:22:21.580]  with a\n[01:22:21.580 --> 01:22:22.000]  particular\n[01:22:22.000 --> 01:22:23.440]  topic\n[01:22:23.440 --> 01:22:23.960]  would get\n[01:22:23.960 --> 01:22:24.440]  collated\n[01:22:24.440 --> 01:22:25.140]  and would\n[01:22:25.140 --> 01:22:25.720]  get shared\n[01:22:25.720 --> 01:22:26.240]  with other\n[01:22:26.240 --> 01:22:26.840]  people that\n[01:22:26.840 --> 01:22:27.380]  were interested\n[01:22:27.380 --> 01:22:27.780]  in that\n[01:22:27.780 --> 01:22:28.120]  topic\n[01:22:28.120 --> 01:22:28.600]  this was\n[01:22:28.600 --> 01:22:28.780]  in the\n[01:22:28.780 --> 01:22:29.120]  good old\n[01:22:29.120 --> 01:22:29.380]  days\n[01:22:29.380 --> 01:22:30.260]  when the\n[01:22:30.260 --> 01:22:30.540]  internet\n[01:22:30.540 --> 01:22:30.980]  was not\n[01:22:30.980 --> 01:22:31.480]  as mature\n[01:22:31.480 --> 01:22:31.880]  as it\n[01:22:31.880 --> 01:22:32.300]  is today\n[01:22:32.300 --> 01:22:33.000]  we didn't\n[01:22:33.000 --> 01:22:33.980]  have social\n[01:22:33.980 --> 01:22:34.380]  media\n[01:22:34.380 --> 01:22:34.800]  and so\n[01:22:34.800 --> 01:22:35.100]  on\n[01:22:35.100 --> 01:22:37.420]  so\n[01:22:37.420 --> 01:22:37.860]  essentially\n[01:22:37.860 --> 01:22:38.380]  this data\n[01:22:38.380 --> 01:22:38.660]  set\n[01:22:38.660 --> 01:22:39.160]  consists\n[01:22:39.160 --> 01:22:39.520]  of a\n[01:22:39.520 --> 01:22:39.700]  number\n[01:22:39.700 --> 01:22:39.920]  of\n[01:22:39.920 --> 01:22:40.300]  messages\n[01:22:40.300 --> 01:22:40.860]  and we\n[01:22:40.860 --> 01:22:41.480]  can see\n[01:22:41.480 --> 01:22:42.280]  that the\n[01:22:42.280 --> 01:22:42.640]  structure\n[01:22:42.640 --> 01:22:42.980]  of the\n[01:22:42.980 --> 01:22:43.240]  data\n[01:22:43.240 --> 01:22:43.640]  set\n[01:22:43.640 --> 01:22:46.380]  we have\n[01:22:46.380 --> 01:22:47.480]  essentially\n[01:22:47.480 --> 01:22:48.200]  a dictionary\n[01:22:48.200 --> 01:22:50.040]  and one\n[01:22:50.040 --> 01:22:50.280]  of the\n[01:22:50.280 --> 01:22:50.600]  elements\n[01:22:50.600 --> 01:22:50.860]  of the\n[01:22:50.860 --> 01:22:51.180]  dictionary\n[01:22:51.180 --> 01:22:51.860]  is data\n[01:22:51.860 --> 01:22:53.380]  and that\n[01:22:53.380 --> 01:22:54.780]  has as\n[01:22:54.780 --> 01:22:56.120]  its value\n[01:22:56.120 --> 01:22:57.120]  a list\n[01:22:57.120 --> 01:23:01.020]  and in\n[01:23:01.020 --> 01:23:01.800]  that list\n[01:23:01.800 --> 01:23:03.120]  you basically\n[01:23:03.120 --> 01:23:03.980]  have a\n[01:23:03.980 --> 01:23:04.320]  set of\n[01:23:04.320 --> 01:23:04.740]  messages\n[01:23:04.740 --> 01:23:05.320]  this is\n[01:23:05.320 --> 01:23:05.560]  one\n[01:23:05.560 --> 01:23:06.020]  message\n[01:23:06.020 --> 01:23:08.320]  and in\n[01:23:08.320 --> 01:23:08.900]  that message\n[01:23:08.900 --> 01:23:09.520]  you have\n[01:23:09.520 --> 01:23:10.160]  who sent\n[01:23:10.160 --> 01:23:10.460]  it\n[01:23:10.460 --> 01:23:11.440]  what was\n[01:23:11.440 --> 01:23:12.060]  the subject\n[01:23:12.060 --> 01:23:13.580]  and where\n[01:23:13.580 --> 01:23:13.880]  did they\n[01:23:13.880 --> 01:23:14.400]  post it\n[01:23:14.400 --> 01:23:14.960]  and so on\n[01:23:14.960 --> 01:23:16.200]  and then\n[01:23:16.200 --> 01:23:16.720]  it has\n[01:23:16.720 --> 01:23:17.360]  some text\n[01:23:17.360 --> 01:23:20.000]  so really\n[01:23:20.000 --> 01:23:20.380]  what we\n[01:23:20.380 --> 01:23:20.740]  want to\n[01:23:20.740 --> 01:23:21.240]  do is\n[01:23:21.240 --> 01:23:21.660]  we want\n[01:23:21.660 --> 01:23:22.240]  to use\n[01:23:22.240 --> 01:23:22.960]  this text\n[01:23:22.960 --> 01:23:23.360]  we want\n[01:23:23.360 --> 01:23:23.800]  to ignore\n[01:23:23.800 --> 01:23:24.560]  these email\n[01:23:24.560 --> 01:23:24.980]  addresses\n[01:23:24.980 --> 01:23:25.400]  and all\n[01:23:25.400 --> 01:23:25.760]  that kind\n[01:23:25.760 --> 01:23:26.140]  of stuff\n[01:23:26.140 --> 01:23:26.540]  we want\n[01:23:26.540 --> 01:23:26.880]  to really\n[01:23:26.880 --> 01:23:27.460]  use this\n[01:23:27.460 --> 01:23:27.920]  text to\n[01:23:27.920 --> 01:23:28.220]  be able\n[01:23:28.220 --> 01:23:28.920]  to automatically\n[01:23:28.920 --> 01:23:30.140]  classify it\n[01:23:30.140 --> 01:23:31.480]  into whether\n[01:23:31.480 --> 01:23:32.140]  it is\n[01:23:32.140 --> 01:23:33.200]  about the\n[01:23:33.200 --> 01:23:33.960]  Christian religion\n[01:23:33.960 --> 01:23:34.800]  or whether\n[01:23:34.800 --> 01:23:34.980]  it is\n[01:23:34.980 --> 01:23:35.760]  about hockey\n[01:23:35.760 --> 01:23:38.800]  or the\n[01:23:38.800 --> 01:23:39.360]  Middle East\n[01:23:39.360 --> 01:23:40.140]  or motorcycles\n[01:23:40.140 --> 01:23:40.660]  so if you\n[01:23:40.660 --> 01:23:41.020]  look at\n[01:23:41.020 --> 01:23:41.480]  this\n[01:23:41.480 --> 01:23:43.160]  the\n[01:23:43.160 --> 01:23:44.040]  labels\n[01:23:44.040 --> 01:23:44.680]  the class\n[01:23:44.680 --> 01:23:45.040]  labels\n[01:23:45.040 --> 01:23:45.440]  that we\n[01:23:45.440 --> 01:23:45.840]  are hoping\n[01:23:45.840 --> 01:23:46.400]  to use\n[01:23:46.400 --> 01:23:46.680]  here\n[01:23:46.680 --> 01:23:47.980]  are actually\n[01:23:47.980 --> 01:23:48.840]  a taxonomy\n[01:23:48.840 --> 01:23:50.100]  now what\n[01:23:50.100 --> 01:23:50.380]  is a\n[01:23:50.380 --> 01:23:50.980]  taxonomy\n[01:23:50.980 --> 01:23:52.900]  a taxonomy\n[01:23:52.900 --> 01:23:53.540]  is a\n[01:23:53.540 --> 01:23:54.020]  tree\n[01:23:54.020 --> 01:23:54.560]  structure\n[01:23:54.560 --> 01:24:07.600]  so when\n[01:24:07.600 --> 01:24:08.140]  Yahoo\n[01:24:08.140 --> 01:24:09.820]  was the\n[01:24:09.820 --> 01:24:10.300]  main\n[01:24:10.300 --> 01:24:11.340]  search engine\n[01:24:11.340 --> 01:24:11.900]  for the\n[01:24:11.900 --> 01:24:12.340]  internet\n[01:24:12.340 --> 01:24:14.840]  they had\n[01:24:14.840 --> 01:24:15.400]  two ways\n[01:24:15.400 --> 01:24:15.700]  in which\n[01:24:15.700 --> 01:24:16.020]  you could\n[01:24:16.020 --> 01:24:16.500]  navigate\n[01:24:16.500 --> 01:24:19.220]  and find\n[01:24:19.220 --> 01:24:19.840]  content\n[01:24:19.840 --> 01:24:20.800]  one was\n[01:24:20.800 --> 01:24:22.420]  they provided\n[01:24:22.420 --> 01:24:23.420]  or created\n[01:24:23.420 --> 01:24:24.340]  a taxonomy\n[01:24:24.340 --> 01:24:29.060]  for the\n[01:24:29.060 --> 01:24:29.500]  internet\n[01:24:29.500 --> 01:24:31.840]  where you\n[01:24:31.840 --> 01:24:32.680]  had everything\n[01:24:32.680 --> 01:24:33.140]  here\n[01:24:33.140 --> 01:24:35.420]  I can't\n[01:24:35.420 --> 01:24:36.120]  remember the\n[01:24:36.120 --> 01:24:36.680]  exact word\n[01:24:36.680 --> 01:24:37.060]  but this\n[01:24:37.060 --> 01:24:37.720]  was basically\n[01:24:37.720 --> 01:24:38.960]  the root\n[01:24:38.960 --> 01:24:39.280]  node\n[01:24:39.280 --> 01:24:41.220]  and then\n[01:24:41.220 --> 01:24:42.480]  you had\n[01:24:42.480 --> 01:24:43.280]  different\n[01:24:43.280 --> 01:24:47.000]  child\n[01:24:47.000 --> 01:24:47.520]  nodes\n[01:24:47.520 --> 01:24:48.260]  now in\n[01:24:48.260 --> 01:24:48.920]  our case\n[01:24:48.920 --> 01:24:49.320]  here\n[01:24:49.320 --> 01:24:51.800]  we have\n[01:24:51.800 --> 01:24:53.220]  social\n[01:24:53.220 --> 01:24:55.440]  recreation\n[01:24:55.440 --> 01:24:59.240]  as two\n[01:24:59.240 --> 01:24:59.540]  of the\n[01:24:59.540 --> 01:24:59.800]  child\n[01:24:59.800 --> 01:25:00.100]  nodes\n[01:25:00.100 --> 01:25:00.380]  that I\n[01:25:00.380 --> 01:25:00.800]  can see\n[01:25:00.800 --> 01:25:01.100]  here\n[01:25:01.100 --> 01:25:02.940]  within\n[01:25:02.940 --> 01:25:03.440]  social\n[01:25:03.440 --> 01:25:04.180]  we have\n[01:25:04.180 --> 01:25:04.720]  further\n[01:25:04.720 --> 01:25:05.660]  splits\n[01:25:05.660 --> 01:25:07.040]  and I\n[01:25:07.040 --> 01:25:07.440]  can see\n[01:25:07.440 --> 01:25:07.860]  one of\n[01:25:07.860 --> 01:25:08.420]  them is\n[01:25:08.420 --> 01:25:09.640]  religion\n[01:25:09.640 --> 01:25:11.360]  and then\n[01:25:11.360 --> 01:25:12.160]  within religion\n[01:25:12.160 --> 01:25:12.780]  we have\n[01:25:12.780 --> 01:25:13.540]  Christian\n[01:25:13.540 --> 01:25:17.200]  I presume\n[01:25:17.200 --> 01:25:17.840]  we will also\n[01:25:17.840 --> 01:25:18.520]  have other\n[01:25:18.520 --> 01:25:19.240]  nodes here\n[01:25:19.240 --> 01:25:20.660]  like\n[01:25:20.660 --> 01:25:21.500]  Islam\n[01:25:21.500 --> 01:25:23.500]  or maybe\n[01:25:23.500 --> 01:25:25.880]  Hindu\n[01:25:25.880 --> 01:25:27.920]  Sikh\n[01:25:27.920 --> 01:25:28.480]  and so\n[01:25:28.480 --> 01:25:28.700]  on\n[01:25:28.700 --> 01:25:30.360]  right\n[01:25:30.360 --> 01:25:31.860]  and here\n[01:25:31.860 --> 01:25:32.720]  in recreation\n[01:25:32.720 --> 01:25:33.880]  we have\n[01:25:33.880 --> 01:25:34.500]  sports\n[01:25:34.500 --> 01:25:37.940]  and within\n[01:25:37.940 --> 01:25:38.680]  sports we\n[01:25:38.680 --> 01:25:38.900]  have\n[01:25:38.900 --> 01:25:39.260]  hockey\n[01:25:39.260 --> 01:25:40.800]  and again\n[01:25:40.800 --> 01:25:41.300]  we would\n[01:25:41.300 --> 01:25:42.040]  expect it\n[01:25:42.040 --> 01:25:42.560]  to have\n[01:25:42.560 --> 01:25:43.020]  cricket\n[01:25:43.020 --> 01:25:43.760]  and various\n[01:25:43.760 --> 01:25:44.060]  other\n[01:25:44.060 --> 01:25:45.540]  things too\n[01:25:45.540 --> 01:25:46.560]  we also\n[01:25:46.560 --> 01:25:46.880]  have\n[01:25:46.880 --> 01:25:47.400]  talk\n[01:25:47.400 --> 01:25:49.600]  politics\n[01:25:49.600 --> 01:25:50.000]  right\n[01:25:50.000 --> 01:25:50.360]  so\n[01:25:50.360 --> 01:25:51.300]  out here\n[01:25:51.300 --> 01:25:52.200]  this would\n[01:25:52.200 --> 01:25:52.800]  be talk\n[01:25:52.800 --> 01:25:54.460]  this would\n[01:25:54.460 --> 01:25:55.360]  be politics\n[01:25:55.360 --> 01:25:58.220]  and then\n[01:25:58.220 --> 01:25:58.620]  we have\n[01:25:58.620 --> 01:25:59.240]  Middle East\n[01:25:59.240 --> 01:26:04.480]  and again\n[01:26:04.480 --> 01:26:04.840]  we would\n[01:26:04.840 --> 01:26:05.320]  expect\n[01:26:05.320 --> 01:26:05.960]  other\n[01:26:05.960 --> 01:26:07.820]  subtopics\n[01:26:07.820 --> 01:26:08.240]  as well\n[01:26:08.240 --> 01:26:09.640]  right\n[01:26:09.640 --> 01:26:09.960]  so the\n[01:26:09.960 --> 01:26:10.580]  dot here\n[01:26:10.580 --> 01:26:10.940]  this is\n[01:26:10.940 --> 01:26:11.380]  just a\n[01:26:11.380 --> 01:26:11.740]  way of\n[01:26:11.740 --> 01:26:12.300]  converting\n[01:26:12.300 --> 01:26:13.260]  your taxonomy\n[01:26:13.260 --> 01:26:13.880]  into a\n[01:26:13.880 --> 01:26:14.200]  string\n[01:26:14.200 --> 01:26:14.620]  where you\n[01:26:14.620 --> 01:26:15.020]  use a\n[01:26:15.020 --> 01:26:15.460]  dot as\n[01:26:15.460 --> 01:26:16.220]  a separator\n[01:26:16.220 --> 01:26:17.840]  to separate\n[01:26:17.840 --> 01:26:18.380]  these out\n[01:26:18.380 --> 01:26:19.000]  and basically\n[01:26:19.000 --> 01:26:20.600]  what Yahoo\n[01:26:20.600 --> 01:26:21.060]  did\n[01:26:21.060 --> 01:26:22.580]  was they\n[01:26:22.580 --> 01:26:23.180]  would have\n[01:26:23.180 --> 01:26:23.780]  a bunch\n[01:26:23.780 --> 01:26:24.560]  of documents\n[01:26:24.560 --> 01:26:25.360]  hanging off\n[01:26:25.360 --> 01:26:26.940]  not just\n[01:26:26.940 --> 01:26:27.360]  the leaf\n[01:26:27.360 --> 01:26:27.920]  nodes but\n[01:26:27.920 --> 01:26:28.200]  you could\n[01:26:28.200 --> 01:26:28.780]  also have\n[01:26:28.780 --> 01:26:29.340]  documents\n[01:26:29.340 --> 01:26:30.080]  hanging off\n[01:26:30.080 --> 01:26:30.960]  here\n[01:26:30.960 --> 01:26:31.740]  where they\n[01:26:31.740 --> 01:26:32.300]  didn't know\n[01:26:32.300 --> 01:26:33.140]  quite exactly\n[01:26:33.140 --> 01:26:33.660]  how to\n[01:26:33.660 --> 01:26:34.300]  split it\n[01:26:34.300 --> 01:26:34.920]  further\n[01:26:34.920 --> 01:26:35.920]  into its\n[01:26:35.920 --> 01:26:36.200]  sub\n[01:26:36.200 --> 01:26:37.460]  categories\n[01:26:37.460 --> 01:26:40.540]  and so\n[01:26:40.540 --> 01:26:41.080]  the idea\n[01:26:41.080 --> 01:26:42.380]  here is\n[01:26:42.380 --> 01:26:42.780]  that we\n[01:26:42.780 --> 01:26:43.300]  want to\n[01:26:43.300 --> 01:26:43.900]  classify\n[01:26:43.900 --> 01:26:45.100]  every one\n[01:26:45.100 --> 01:26:45.520]  of these\n[01:26:45.520 --> 01:26:46.000]  messages\n[01:26:46.000 --> 01:26:47.220]  into a\n[01:26:47.220 --> 01:26:47.820]  leaf node\n[01:26:47.820 --> 01:26:49.340]  and there\n[01:26:49.340 --> 01:26:50.000]  are 20\n[01:26:50.000 --> 01:26:50.700]  such leaf\n[01:26:50.700 --> 01:26:51.280]  nodes that\n[01:26:51.280 --> 01:26:51.540]  we have\n[01:26:51.540 --> 01:26:51.960]  provided\n[01:26:51.960 --> 01:26:53.660]  and here\n[01:26:53.660 --> 01:26:54.280]  they are\n[01:26:54.280 --> 01:26:54.580]  right\n[01:26:54.580 --> 01:26:55.700]  so alt\n[01:26:55.700 --> 01:26:56.860]  dot atheism\n[01:26:56.860 --> 01:26:58.360]  computer\n[01:26:58.360 --> 01:26:59.700]  dot graphics\n[01:26:59.700 --> 01:27:04.280]  operating\n[01:27:04.280 --> 01:27:04.940]  systems\n[01:27:04.940 --> 01:27:05.500]  and so\n[01:27:05.500 --> 01:27:05.720]  on\n[01:27:05.720 --> 01:27:06.000]  right\n[01:27:06.000 --> 01:27:08.040]  so the\n[01:27:08.040 --> 01:27:08.660]  first thing\n[01:27:08.660 --> 01:27:09.560]  that we\n[01:27:09.560 --> 01:27:09.940]  want to\n[01:27:09.940 --> 01:27:10.460]  do here\n[01:27:10.460 --> 01:27:11.040]  is get\n[01:27:11.040 --> 01:27:11.620]  the data\n[01:27:11.620 --> 01:27:12.200]  into a\n[01:27:12.200 --> 01:27:12.460]  shape\n[01:27:12.460 --> 01:27:12.820]  that we\n[01:27:12.820 --> 01:27:12.920]  are\n[01:27:12.920 --> 01:27:13.340]  interested\n[01:27:13.340 --> 01:27:13.660]  in\n[01:27:13.660 --> 01:27:16.100]  if we\n[01:27:16.100 --> 01:27:16.680]  look at\n[01:27:16.680 --> 01:27:17.340]  the data\n[01:27:17.340 --> 01:27:17.700]  that we\n[01:27:17.700 --> 01:27:18.000]  have been\n[01:27:18.000 --> 01:27:18.560]  provided\n[01:27:18.560 --> 01:27:20.780]  the data\n[01:27:20.780 --> 01:27:21.560]  consists\n[01:27:21.560 --> 01:27:21.960]  of\n[01:27:21.960 --> 01:27:23.000]  five\n[01:27:23.000 --> 01:27:24.240]  different\n[01:27:24.240 --> 01:27:24.860]  keys\n[01:27:24.860 --> 01:27:25.620]  they are\n[01:27:25.620 --> 01:27:26.420]  dictionaries\n[01:27:26.420 --> 01:27:27.180]  and their\n[01:27:27.180 --> 01:27:27.700]  keys are\n[01:27:27.700 --> 01:27:28.140]  data\n[01:27:28.140 --> 01:27:28.940]  file names\n[01:27:28.940 --> 01:27:29.880]  target names\n[01:27:29.880 --> 01:27:30.420]  target\n[01:27:30.420 --> 01:27:31.440]  and\n[01:27:31.440 --> 01:27:31.940]  description\n[01:27:31.940 --> 01:27:33.180]  we are\n[01:27:33.180 --> 01:27:33.380]  only\n[01:27:33.380 --> 01:27:33.820]  interested\n[01:27:33.820 --> 01:27:34.440]  in\n[01:27:34.440 --> 01:27:35.480]  actually\n[01:27:35.480 --> 01:27:35.680]  the\n[01:27:35.680 --> 01:27:36.200]  content\n[01:27:36.200 --> 01:27:37.500]  and the\n[01:27:37.500 --> 01:27:37.880]  target\n[01:27:37.880 --> 01:27:38.300]  name\n[01:27:38.300 --> 01:27:39.580]  the\n[01:27:39.580 --> 01:27:40.020]  target\n[01:27:40.020 --> 01:27:41.300]  is just\n[01:27:41.300 --> 01:27:41.780]  a\n[01:27:41.780 --> 01:27:42.440]  one-to-one\n[01:27:42.440 --> 01:27:42.860]  mapping\n[01:27:42.860 --> 01:27:43.360]  of a\n[01:27:43.360 --> 01:27:43.720]  string\n[01:27:43.720 --> 01:27:44.480]  onto an\n[01:27:44.480 --> 01:27:44.920]  integer\n[01:27:44.920 --> 01:27:45.360]  from what\n[01:27:45.360 --> 01:27:45.680]  I can\n[01:27:45.680 --> 01:27:46.120]  take up\n[01:27:46.120 --> 01:27:47.360]  right\n[01:27:47.360 --> 01:27:47.680]  so we\n[01:27:47.680 --> 01:27:48.200]  brought that\n[01:27:48.200 --> 01:27:48.540]  in out\n[01:27:48.540 --> 01:27:49.080]  here also\n[01:27:49.080 --> 01:27:49.500]  we don't\n[01:27:49.500 --> 01:27:49.700]  really\n[01:27:49.700 --> 01:27:53.040]  and so\n[01:27:53.040 --> 01:27:53.500]  the first\n[01:27:53.500 --> 01:27:53.880]  thing we\n[01:27:53.880 --> 01:27:54.100]  do\n[01:27:54.100 --> 01:27:54.620]  typically\n[01:27:54.620 --> 01:27:55.160]  like we\n[01:27:55.160 --> 01:27:55.440]  said\n[01:27:55.440 --> 01:27:55.940]  is\n[01:27:55.940 --> 01:27:56.500]  we will\n[01:27:56.500 --> 01:27:57.200]  lemmatize\n[01:27:57.200 --> 01:27:57.760]  the data\n[01:27:57.760 --> 01:27:59.960]  we will\n[01:27:59.960 --> 01:28:03.020]  clean up\n[01:28:03.020 --> 01:28:04.360]  some issues\n[01:28:04.360 --> 01:28:04.760]  in the\n[01:28:04.760 --> 01:28:05.140]  data\n[01:28:05.140 --> 01:28:06.700]  like\n[01:28:06.700 --> 01:28:07.920]  remove\n[01:28:07.920 --> 01:28:08.580]  email\n[01:28:08.580 --> 01:28:09.120]  addresses\n[01:28:09.120 --> 01:28:10.440]  and stuff\n[01:28:10.440 --> 01:28:11.120]  like this\n[01:28:11.120 --> 01:28:12.340]  remove\n[01:28:12.340 --> 01:28:13.020]  new line\n[01:28:13.020 --> 01:28:13.700]  characters\n[01:28:13.700 --> 01:28:15.180]  single\n[01:28:15.180 --> 01:28:15.760]  quotes\n[01:28:15.760 --> 01:28:16.280]  right\n[01:28:16.280 --> 01:28:17.200]  so punctuation\n[01:28:17.200 --> 01:28:19.160]  so that's\n[01:28:19.160 --> 01:28:19.480]  happening\n[01:28:19.480 --> 01:28:19.860]  here\n[01:28:19.860 --> 01:28:22.040]  and then\n[01:28:22.040 --> 01:28:23.820]  out here\n[01:28:23.820 --> 01:28:24.240]  is where\n[01:28:24.240 --> 01:28:25.260]  the lemmatization\n[01:28:25.260 --> 01:28:25.960]  is happening\n[01:28:25.960 --> 01:28:28.100]  using\n[01:28:28.100 --> 01:28:28.980]  another\n[01:28:28.980 --> 01:28:29.460]  library\n[01:28:29.460 --> 01:28:29.900]  that you\n[01:28:29.900 --> 01:28:30.320]  will now\n[01:28:30.320 --> 01:28:30.840]  have to\n[01:28:30.840 --> 01:28:31.380]  start to\n[01:28:31.380 --> 01:28:31.960]  explore\n[01:28:31.960 --> 01:28:33.560]  called\n[01:28:33.560 --> 01:28:34.240]  spacey\n[01:28:34.240 --> 01:28:35.680]  and spacey\n[01:28:35.680 --> 01:28:36.720]  is one\n[01:28:36.720 --> 01:28:37.160]  of the\n[01:28:37.160 --> 01:28:37.900]  leading\n[01:28:37.900 --> 01:28:38.740]  natural\n[01:28:38.740 --> 01:28:39.100]  language\n[01:28:39.100 --> 01:28:39.700]  processing\n[01:28:39.700 --> 01:28:40.480]  libraries\n[01:28:40.480 --> 01:28:40.940]  that is\n[01:28:40.940 --> 01:28:41.220]  there\n[01:28:41.220 --> 01:28:42.320]  so getting\n[01:28:42.320 --> 01:28:42.820]  familiarity\n[01:28:42.820 --> 01:28:43.500]  with that\n[01:28:43.500 --> 01:28:43.840]  is going\n[01:28:43.840 --> 01:28:44.020]  to be\n[01:28:44.020 --> 01:28:44.380]  important\n[01:28:44.380 --> 01:28:47.760]  now when\n[01:28:47.760 --> 01:28:48.760]  we lemmatize\n[01:28:48.760 --> 01:28:49.380]  the results\n[01:28:49.380 --> 01:28:49.700]  of the\n[01:28:49.700 --> 01:28:50.500]  lemmatization\n[01:28:50.500 --> 01:28:52.660]  depend on\n[01:28:52.660 --> 01:28:53.360]  another\n[01:28:53.360 --> 01:28:54.760]  piece of\n[01:28:54.760 --> 01:28:55.420]  information\n[01:28:55.420 --> 01:28:56.000]  which are\n[01:28:56.000 --> 01:28:56.440]  called\n[01:28:56.440 --> 01:28:58.080]  POS tags\n[01:28:58.080 --> 01:29:00.940]  right\n[01:29:00.940 --> 01:29:01.460]  so the\n[01:29:01.460 --> 01:29:03.440]  these POS tags\n[01:29:03.440 --> 01:29:04.040]  are assigned\n[01:29:04.040 --> 01:29:04.760]  to individual\n[01:29:04.760 --> 01:29:05.480]  words\n[01:29:05.480 --> 01:29:07.000]  POS\n[01:29:07.000 --> 01:29:07.660]  stands for\n[01:29:07.660 --> 01:29:08.220]  part of\n[01:29:08.220 --> 01:29:08.720]  speech\n[01:29:08.720 --> 01:29:13.200]  and so\n[01:29:13.200 --> 01:29:13.580]  if you\n[01:29:13.580 --> 01:29:14.580]  look up\n[01:29:14.580 --> 01:29:15.000]  part of\n[01:29:15.000 --> 01:29:15.480]  speech\n[01:29:15.480 --> 01:29:15.860]  you will\n[01:29:15.860 --> 01:29:16.440]  find\n[01:29:16.440 --> 01:29:24.140]  part of\n[01:29:24.140 --> 01:29:24.780]  speech\n[01:29:24.780 --> 01:29:26.940]  essentially\n[01:29:26.940 --> 01:29:27.920]  you are\n[01:29:27.920 --> 01:29:28.520]  assigning\n[01:29:28.520 --> 01:29:30.380]  tags to\n[01:29:30.380 --> 01:29:31.200]  every word\n[01:29:31.200 --> 01:29:36.260]  tags like\n[01:29:36.260 --> 01:29:36.800]  nouns\n[01:29:36.800 --> 01:29:37.720]  adjectives\n[01:29:37.720 --> 01:29:38.240]  verb\n[01:29:38.240 --> 01:29:38.840]  adverb\n[01:29:38.840 --> 01:29:39.740]  right\n[01:29:39.740 --> 01:29:40.960]  and\n[01:29:40.960 --> 01:29:41.520]  knowing\n[01:29:41.520 --> 01:29:41.880]  the\n[01:29:41.880 --> 01:29:42.420]  POS\n[01:29:42.420 --> 01:29:42.860]  tag\n[01:29:42.860 --> 01:29:44.660]  improves\n[01:29:44.660 --> 01:29:44.900]  the\n[01:29:44.900 --> 01:29:45.480]  quality\n[01:29:45.480 --> 01:29:46.140]  of\n[01:29:46.140 --> 01:29:48.700]  lemmatization\n[01:29:48.700 --> 01:29:49.380]  that is\n[01:29:49.380 --> 01:29:49.840]  done to\n[01:29:49.840 --> 01:29:50.100]  it\n[01:29:50.100 --> 01:29:51.400]  right\n[01:29:51.400 --> 01:29:51.880]  because you\n[01:29:51.880 --> 01:29:52.320]  wouldn't want\n[01:29:52.320 --> 01:29:53.240]  to lemmatize\n[01:29:53.240 --> 01:29:54.320]  somebody's\n[01:29:54.320 --> 01:29:54.620]  name\n[01:29:54.620 --> 01:29:55.380]  for example\n[01:29:55.380 --> 01:29:57.480]  even though\n[01:29:57.480 --> 01:29:57.820]  it looks\n[01:29:57.820 --> 01:29:58.140]  like a\n[01:29:58.140 --> 01:29:58.380]  prime\n[01:29:58.380 --> 01:29:58.760]  template\n[01:29:58.760 --> 01:29:59.320]  for doing\n[01:29:59.320 --> 01:29:59.560]  so\n[01:29:59.560 --> 01:30:01.280]  so what\n[01:30:01.280 --> 01:30:01.500]  we are\n[01:30:01.500 --> 01:30:02.180]  doing here\n[01:30:02.180 --> 01:30:02.980]  is we\n[01:30:02.980 --> 01:30:03.660]  are loading\n[01:30:03.660 --> 01:30:04.460]  a model\n[01:30:04.460 --> 01:30:05.620]  a pre-trained\n[01:30:05.620 --> 01:30:06.020]  model\n[01:30:06.020 --> 01:30:07.000]  in spacey\n[01:30:07.000 --> 01:30:10.560]  if we\n[01:30:10.560 --> 01:30:10.920]  look at\n[01:30:10.920 --> 01:30:11.240]  this\n[01:30:11.240 --> 01:30:11.780]  en\n[01:30:11.780 --> 01:30:12.160]  stands\n[01:30:12.160 --> 01:30:12.380]  for\n[01:30:12.380 --> 01:30:12.680]  english\n[01:30:12.680 --> 01:30:13.160]  so this\n[01:30:13.160 --> 01:30:13.420]  is a\n[01:30:13.420 --> 01:30:13.700]  model\n[01:30:13.700 --> 01:30:14.340]  specifically\n[01:30:14.340 --> 01:30:14.720]  for the\n[01:30:14.720 --> 01:30:15.000]  english\n[01:30:15.000 --> 01:30:15.680]  language\n[01:30:15.680 --> 01:30:17.400]  and sm\n[01:30:17.400 --> 01:30:18.080]  stands for\n[01:30:18.080 --> 01:30:18.580]  small\n[01:30:18.580 --> 01:30:19.040]  that is\n[01:30:19.040 --> 01:30:19.620]  large\n[01:30:19.620 --> 01:30:20.420]  models\n[01:30:20.420 --> 01:30:21.080]  also\n[01:30:21.080 --> 01:30:21.720]  and\n[01:30:21.720 --> 01:30:22.280]  medium\n[01:30:22.280 --> 01:30:22.640]  sized\n[01:30:22.640 --> 01:30:23.040]  models\n[01:30:23.040 --> 01:30:23.380]  that you\n[01:30:23.380 --> 01:30:23.580]  can\n[01:30:23.580 --> 01:30:24.720]  pull up\n[01:30:24.720 --> 01:30:26.920]  and\n[01:30:26.920 --> 01:30:27.380]  typically\n[01:30:27.380 --> 01:30:28.520]  when we\n[01:30:28.520 --> 01:30:28.920]  load\n[01:30:28.920 --> 01:30:29.200]  this\n[01:30:29.200 --> 01:30:29.620]  model\n[01:30:29.620 --> 01:30:29.900]  this\n[01:30:29.900 --> 01:30:30.300]  model\n[01:30:30.300 --> 01:30:30.820]  has\n[01:30:30.820 --> 01:30:31.220]  all\n[01:30:31.220 --> 01:30:31.720]  sorts\n[01:30:31.720 --> 01:30:32.300]  of\n[01:30:32.300 --> 01:30:33.600]  processing\n[01:30:33.600 --> 01:30:34.520]  capabilities\n[01:30:34.520 --> 01:30:35.960]  and we\n[01:30:35.960 --> 01:30:36.140]  are\n[01:30:36.140 --> 01:30:36.920]  disabling\n[01:30:36.920 --> 01:30:37.580]  named\n[01:30:37.580 --> 01:30:38.020]  entity\n[01:30:38.020 --> 01:30:38.720]  recognition\n[01:30:38.720 --> 01:30:40.280]  and\n[01:30:40.280 --> 01:30:41.380]  entity\n[01:30:41.380 --> 01:30:42.180]  parsing\n[01:30:42.180 --> 01:30:42.820]  or sentence\n[01:30:42.820 --> 01:30:43.440]  parsing\n[01:30:43.440 --> 01:30:44.100]  in here\n[01:30:44.100 --> 01:30:45.360]  all we\n[01:30:45.360 --> 01:30:45.840]  are interested\n[01:30:45.840 --> 01:30:46.380]  in doing\n[01:30:46.380 --> 01:30:46.820]  at this\n[01:30:46.820 --> 01:30:47.160]  point\n[01:30:47.160 --> 01:30:47.800]  is\n[01:30:47.800 --> 01:30:48.940]  tokenization\n[01:30:48.940 --> 01:30:50.080]  and\n[01:30:50.080 --> 01:30:50.600]  POS\n[01:30:50.600 --> 01:30:51.100]  tags\n[01:30:51.100 --> 01:30:52.260]  identification\n[01:30:52.260 --> 01:30:58.140]  so\n[01:30:58.140 --> 01:30:59.820]  when we\n[01:30:59.820 --> 01:31:00.880]  run this\n[01:31:00.880 --> 01:31:04.240]  what is\n[01:31:04.240 --> 01:31:04.580]  happening\n[01:31:04.580 --> 01:31:05.200]  here is\n[01:31:05.200 --> 01:31:05.520]  we are\n[01:31:05.520 --> 01:31:05.900]  getting\n[01:31:05.900 --> 01:31:06.800]  our\n[01:31:06.800 --> 01:31:07.500]  data\n[01:31:07.500 --> 01:31:07.980]  from\n[01:31:07.980 --> 01:31:08.460]  our\n[01:31:08.460 --> 01:31:09.860]  data\n[01:31:09.860 --> 01:31:10.180]  frame\n[01:31:10.180 --> 01:31:10.500]  here\n[01:31:10.500 --> 01:31:11.800]  and we\n[01:31:11.800 --> 01:31:12.260]  are choosing\n[01:31:12.260 --> 01:31:13.060]  just the\n[01:31:13.060 --> 01:31:13.600]  values\n[01:31:13.600 --> 01:31:14.100]  in the\n[01:31:14.100 --> 01:31:14.580]  content\n[01:31:14.580 --> 01:31:15.020]  column\n[01:31:15.020 --> 01:31:16.440]  we are\n[01:31:16.440 --> 01:31:16.840]  converting\n[01:31:16.840 --> 01:31:17.320]  it into\n[01:31:17.320 --> 01:31:18.000]  a list\n[01:31:18.000 --> 01:31:18.580]  we are\n[01:31:18.580 --> 01:31:19.200]  removing\n[01:31:19.200 --> 01:31:20.340]  emails\n[01:31:20.340 --> 01:31:22.820]  we are\n[01:31:22.820 --> 01:31:23.400]  removing\n[01:31:23.400 --> 01:31:23.820]  new\n[01:31:23.820 --> 01:31:24.380]  line\n[01:31:24.380 --> 01:31:24.960]  characters\n[01:31:24.960 --> 01:31:25.600]  removing\n[01:31:25.600 --> 01:31:26.600]  certain\n[01:31:26.600 --> 01:31:27.580]  punctuation\n[01:31:27.580 --> 01:31:29.660]  we are\n[01:31:29.660 --> 01:31:30.140]  loading\n[01:31:30.140 --> 01:31:30.460]  this\n[01:31:30.460 --> 01:31:30.960]  model\n[01:31:30.960 --> 01:31:31.420]  the\n[01:31:31.420 --> 01:31:31.780]  NLP\n[01:31:31.780 --> 01:31:32.300]  model\n[01:31:32.300 --> 01:31:34.600]  and\n[01:31:34.600 --> 01:31:35.220]  we\n[01:31:35.220 --> 01:31:35.700]  are\n[01:31:35.700 --> 01:31:39.820]  oh\n[01:31:39.820 --> 01:31:40.280]  in fact\n[01:31:40.280 --> 01:31:40.940]  before we\n[01:31:40.940 --> 01:31:41.520]  load that\n[01:31:41.520 --> 01:31:41.880]  NLP\n[01:31:41.880 --> 01:31:42.260]  model\n[01:31:42.260 --> 01:31:42.660]  we are\n[01:31:42.660 --> 01:31:43.520]  executing\n[01:31:43.520 --> 01:31:44.160]  on this\n[01:31:44.160 --> 01:31:44.520]  here\n[01:31:44.520 --> 01:31:46.100]  where\n[01:31:46.100 --> 01:31:46.980]  we are\n[01:31:46.980 --> 01:31:47.620]  removing\n[01:31:47.620 --> 01:31:48.500]  punctuation\n[01:31:48.500 --> 01:31:49.600]  and converting\n[01:31:49.600 --> 01:31:50.460]  each\n[01:31:50.460 --> 01:31:52.420]  element\n[01:31:52.420 --> 01:31:53.000]  in our\n[01:31:53.000 --> 01:31:53.460]  list\n[01:31:53.460 --> 01:31:54.060]  which is\n[01:31:54.060 --> 01:31:54.540]  one of\n[01:31:54.540 --> 01:31:55.160]  these\n[01:31:55.160 --> 01:31:57.760]  posts\n[01:31:57.760 --> 01:31:58.220]  done\n[01:31:58.220 --> 01:31:59.140]  to our\n[01:31:59.140 --> 01:32:01.180]  groups\n[01:32:01.180 --> 01:32:03.460]  and converting\n[01:32:03.460 --> 01:32:04.080]  it into\n[01:32:04.080 --> 01:32:05.520]  a list\n[01:32:05.520 --> 01:32:06.200]  of words\n[01:32:06.200 --> 01:32:10.120]  and we\n[01:32:10.120 --> 01:32:10.740]  are then\n[01:32:10.740 --> 01:32:12.060]  calling the\n[01:32:12.060 --> 01:32:13.040]  lemmatization\n[01:32:13.040 --> 01:32:14.760]  piece\n[01:32:14.760 --> 01:32:15.240]  here\n[01:32:15.240 --> 01:32:17.000]  that is\n[01:32:17.000 --> 01:32:17.540]  using\n[01:32:17.540 --> 01:32:18.680]  the\n[01:32:18.680 --> 01:32:19.260]  spacey\n[01:32:19.260 --> 01:32:19.620]  model\n[01:32:19.620 --> 01:32:22.120]  and\n[01:32:22.120 --> 01:32:24.100]  looking\n[01:32:24.100 --> 01:32:24.540]  at\n[01:32:24.540 --> 01:32:25.320]  only\n[01:32:25.320 --> 01:32:26.320]  lemmatization\n[01:32:26.320 --> 01:32:27.180]  of\n[01:32:27.180 --> 01:32:29.740]  these\n[01:32:29.740 --> 01:32:30.520]  post\n[01:32:30.520 --> 01:32:30.900]  times\n[01:32:30.900 --> 01:32:33.360]  right\n[01:32:33.360 --> 01:32:34.200]  so it's\n[01:32:34.200 --> 01:32:34.760]  taking each\n[01:32:34.760 --> 01:32:35.380]  token and\n[01:32:35.380 --> 01:32:36.140]  lemmatizing\n[01:32:36.140 --> 01:32:36.460]  it\n[01:32:36.460 --> 01:32:38.440]  and\n[01:32:38.440 --> 01:32:39.080]  providing\n[01:32:39.080 --> 01:32:39.720]  that in\n[01:32:39.720 --> 01:32:39.960]  our\n[01:32:39.960 --> 01:32:41.460]  texts\n[01:32:41.460 --> 01:32:42.240]  out\n[01:32:42.240 --> 01:32:43.140]  which is\n[01:32:43.140 --> 01:32:43.460]  another\n[01:32:43.460 --> 01:32:44.100]  list\n[01:32:44.100 --> 01:32:47.100]  which is\n[01:32:47.100 --> 01:32:47.260]  being\n[01:32:47.260 --> 01:32:47.760]  stored in\n[01:32:47.760 --> 01:32:48.000]  data\n[01:32:48.000 --> 01:32:48.700]  lemmatized\n[01:32:48.700 --> 01:32:49.420]  and that\n[01:32:49.420 --> 01:32:49.820]  is being\n[01:32:49.820 --> 01:32:50.240]  returned\n[01:32:50.240 --> 01:32:51.680]  so when\n[01:32:51.680 --> 01:32:52.380]  we call\n[01:32:52.380 --> 01:32:52.880]  our\n[01:32:52.880 --> 01:32:53.800]  get\n[01:32:53.800 --> 01:32:54.460]  lemmatized\n[01:32:54.460 --> 01:32:54.840]  clean\n[01:32:54.840 --> 01:32:55.280]  data\n[01:32:55.280 --> 01:32:57.120]  passing\n[01:32:57.120 --> 01:32:57.500]  the\n[01:32:57.500 --> 01:32:57.960]  data\n[01:32:57.960 --> 01:32:58.340]  frame\n[01:32:58.340 --> 01:32:58.700]  here\n[01:32:58.700 --> 01:32:59.860]  what we\n[01:32:59.860 --> 01:33:00.260]  end up\n[01:33:00.260 --> 01:33:00.520]  getting\n[01:33:00.520 --> 01:33:01.080]  back\n[01:33:01.080 --> 01:33:02.540]  is this\n[01:33:02.540 --> 01:33:03.260]  list of\n[01:33:03.260 --> 01:33:03.780]  words\n[01:33:03.780 --> 01:33:04.520]  that appear\n[01:33:04.520 --> 01:33:05.380]  within\n[01:33:05.380 --> 01:33:07.040]  that message\n[01:33:07.040 --> 01:33:07.860]  that has\n[01:33:07.860 --> 01:33:08.080]  been\n[01:33:08.080 --> 01:33:08.860]  lemmatized\n[01:33:08.860 --> 01:33:10.960]  punctuation\n[01:33:10.960 --> 01:33:11.320]  has been\n[01:33:11.320 --> 01:33:11.740]  removed\n[01:33:11.740 --> 01:33:12.120]  but we\n[01:33:12.120 --> 01:33:12.420]  can see\n[01:33:12.420 --> 01:33:12.760]  we still\n[01:33:12.760 --> 01:33:13.080]  have some\n[01:33:13.080 --> 01:33:13.420]  issues\n[01:33:13.420 --> 01:33:13.720]  here\n[01:33:13.720 --> 01:33:14.220]  we have\n[01:33:14.220 --> 01:33:14.340]  an\n[01:33:14.340 --> 01:33:14.820]  apostrophe\n[01:33:14.820 --> 01:33:15.140]  s\n[01:33:15.140 --> 01:33:15.400]  out\n[01:33:15.400 --> 01:33:15.640]  here\n[01:33:15.640 --> 01:33:16.560]  so really\n[01:33:16.560 --> 01:33:16.980]  we should\n[01:33:16.980 --> 01:33:17.920]  be doing\n[01:33:17.920 --> 01:33:18.440]  some further\n[01:33:18.440 --> 01:33:18.840]  cleaning\n[01:33:18.840 --> 01:33:19.360]  but this\n[01:33:19.360 --> 01:33:20.560]  is the\n[01:33:20.560 --> 01:33:20.940]  basic\n[01:33:20.940 --> 01:33:21.360]  cleaning\n[01:33:21.360 --> 01:33:21.820]  that we\n[01:33:21.820 --> 01:33:22.140]  are doing\n[01:33:22.140 --> 01:33:22.480]  here\n[01:33:22.480 --> 01:33:25.540]  now at\n[01:33:25.540 --> 01:33:26.200]  this point\n[01:33:26.200 --> 01:33:28.080]  I'm going\n[01:33:28.080 --> 01:33:28.420]  to take\n[01:33:28.420 --> 01:33:29.100]  the data\n[01:33:29.100 --> 01:33:29.740]  lemmatized\n[01:33:29.740 --> 01:33:30.240]  and that\n[01:33:30.240 --> 01:33:30.820]  is really\n[01:33:30.820 --> 01:33:31.400]  the training\n[01:33:31.400 --> 01:33:31.860]  data\n[01:33:31.860 --> 01:33:32.620]  that we\n[01:33:32.620 --> 01:33:32.980]  have\n[01:33:32.980 --> 01:33:35.600]  and I'm\n[01:33:35.600 --> 01:33:36.320]  passing it\n[01:33:36.320 --> 01:33:36.840]  through\n[01:33:36.840 --> 01:33:38.040]  in sklearn\n[01:33:38.040 --> 01:33:38.420]  we have\n[01:33:38.420 --> 01:33:38.840]  the feature\n[01:33:38.840 --> 01:33:39.440]  extraction\n[01:33:39.440 --> 01:33:40.080]  dot text\n[01:33:40.080 --> 01:33:40.480]  count\n[01:33:40.480 --> 01:33:40.880]  vector\n[01:33:40.880 --> 01:33:45.680]  and I'm\n[01:33:45.680 --> 01:33:46.140]  saying I\n[01:33:46.140 --> 01:33:46.680]  want to\n[01:33:46.680 --> 01:33:48.400]  tokenize\n[01:33:48.400 --> 01:33:49.260]  based on\n[01:33:49.260 --> 01:33:49.920]  words\n[01:33:49.920 --> 01:33:52.240]  I only\n[01:33:52.240 --> 01:33:53.140]  want words\n[01:33:53.140 --> 01:33:53.480]  in my\n[01:33:53.480 --> 01:33:54.020]  vocabulary\n[01:33:54.020 --> 01:33:55.020]  that have\n[01:33:55.020 --> 01:33:55.940]  a minimum\n[01:33:55.940 --> 01:33:56.640]  of 10\n[01:33:56.640 --> 01:33:57.260]  documents\n[01:33:57.260 --> 01:33:57.680]  that they\n[01:33:57.680 --> 01:33:58.240]  appear in\n[01:33:58.240 --> 01:33:59.920]  I want\n[01:33:59.920 --> 01:34:00.600]  to use\n[01:34:00.600 --> 01:34:01.120]  a list\n[01:34:01.120 --> 01:34:01.700]  of stock\n[01:34:01.700 --> 01:34:02.160]  words\n[01:34:02.160 --> 01:34:02.580]  that is\n[01:34:02.580 --> 01:34:03.060]  already\n[01:34:03.060 --> 01:34:03.920]  embedded\n[01:34:03.920 --> 01:34:04.880]  within this\n[01:34:04.880 --> 01:34:05.680]  function\n[01:34:05.680 --> 01:34:06.640]  this method\n[01:34:06.640 --> 01:34:08.500]  they're the\n[01:34:08.500 --> 01:34:08.900]  English\n[01:34:08.900 --> 01:34:09.600]  stock words\n[01:34:09.600 --> 01:34:10.080]  that I want\n[01:34:10.080 --> 01:34:10.800]  to remove\n[01:34:10.800 --> 01:34:12.000]  I want\n[01:34:12.000 --> 01:34:12.460]  to make\n[01:34:12.460 --> 01:34:12.960]  all of\n[01:34:12.960 --> 01:34:13.640]  my words\n[01:34:13.640 --> 01:34:14.420]  lowercase\n[01:34:14.420 --> 01:34:16.640]  and\n[01:34:16.640 --> 01:34:18.260]  I only\n[01:34:18.260 --> 01:34:18.880]  want to\n[01:34:18.880 --> 01:34:19.700]  have in\n[01:34:19.700 --> 01:34:20.460]  my vocabulary\n[01:34:20.460 --> 01:34:21.600]  words that\n[01:34:21.600 --> 01:34:22.300]  have greater\n[01:34:22.300 --> 01:34:22.700]  than or\n[01:34:22.700 --> 01:34:23.420]  equal to\n[01:34:23.420 --> 01:34:25.720]  three characters\n[01:34:25.720 --> 01:34:26.600]  within it\n[01:34:26.600 --> 01:34:30.860]  and I do\n[01:34:30.860 --> 01:34:31.980]  a fit\n[01:34:31.980 --> 01:34:32.580]  transform\n[01:34:32.580 --> 01:34:33.080]  on it\n[01:34:33.080 --> 01:34:34.760]  and I\n[01:34:34.760 --> 01:34:35.480]  end up\n[01:34:35.480 --> 01:34:36.580]  with\n[01:34:36.580 --> 01:34:38.460]  with this\n[01:34:38.460 --> 01:34:39.640]  fit transform\n[01:34:39.640 --> 01:34:40.440]  creating a\n[01:34:40.440 --> 01:34:41.000]  vocabulary\n[01:34:41.000 --> 01:34:41.880]  where it's\n[01:34:41.880 --> 01:34:42.700]  matched every\n[01:34:42.700 --> 01:34:43.820]  unique token\n[01:34:43.820 --> 01:34:45.160]  every word\n[01:34:45.160 --> 01:34:46.520]  onto your\n[01:34:46.520 --> 01:34:47.560]  unique ID\n[01:34:47.560 --> 01:34:49.720]  and essentially\n[01:34:49.720 --> 01:34:50.560]  what I'm seeing\n[01:34:50.560 --> 01:34:51.420]  here is that\n[01:34:51.420 --> 01:34:53.040]  the vector\n[01:34:53.040 --> 01:34:54.120]  that we generate\n[01:34:54.120 --> 01:34:55.280]  for every document\n[01:34:55.280 --> 01:34:59.020]  the 7083rd\n[01:34:59.020 --> 01:35:00.580]  element in\n[01:35:00.580 --> 01:35:01.260]  that vector\n[01:35:01.260 --> 01:35:02.520]  is going\n[01:35:02.520 --> 01:35:03.000]  to be\n[01:35:03.000 --> 01:35:03.680]  representing\n[01:35:03.680 --> 01:35:04.280]  the word\n[01:35:04.280 --> 01:35:04.720]  thing\n[01:35:04.720 --> 01:35:05.320]  and the\n[01:35:05.320 --> 01:35:05.900]  frequency\n[01:35:05.900 --> 01:35:06.600]  within it\n[01:35:06.600 --> 01:35:07.140]  because I'm\n[01:35:07.140 --> 01:35:07.580]  calling the\n[01:35:07.580 --> 01:35:07.920]  count\n[01:35:07.920 --> 01:35:10.660]  right so\n[01:35:10.660 --> 01:35:11.300]  that's all\n[01:35:11.300 --> 01:35:11.620]  of my\n[01:35:11.620 --> 01:35:12.120]  vocabulary\n[01:35:12.120 --> 01:35:12.620]  here\n[01:35:12.620 --> 01:35:15.240]  of which\n[01:35:15.240 --> 01:35:15.740]  we've seen\n[01:35:15.740 --> 01:35:16.340]  we have\n[01:35:16.340 --> 01:35:16.740]  a few\n[01:35:16.740 --> 01:35:17.140]  thousand\n[01:35:17.140 --> 01:35:17.740]  words\n[01:35:17.740 --> 01:35:21.360]  we can\n[01:35:21.360 --> 01:35:21.680]  see the\n[01:35:21.680 --> 01:35:22.200]  vocabulary\n[01:35:22.200 --> 01:35:23.460]  is actually\n[01:35:23.460 --> 01:35:25.320]  7846\n[01:35:25.320 --> 01:35:27.160]  right now\n[01:35:27.160 --> 01:35:27.560]  when I\n[01:35:27.560 --> 01:35:28.040]  give you\n[01:35:28.040 --> 01:35:28.760]  this\n[01:35:28.760 --> 01:35:30.960]  this\n[01:35:30.960 --> 01:35:31.420]  ipython\n[01:35:31.420 --> 01:35:31.820]  notebook\n[01:35:31.820 --> 01:35:33.320]  you can\n[01:35:33.320 --> 01:35:33.820]  play around\n[01:35:33.820 --> 01:35:34.200]  with some\n[01:35:34.200 --> 01:35:34.400]  of the\n[01:35:34.400 --> 01:35:34.960]  parameters\n[01:35:34.960 --> 01:35:36.120]  reduce\n[01:35:36.120 --> 01:35:37.540]  the minimum\n[01:35:37.540 --> 01:35:38.340]  df for\n[01:35:38.340 --> 01:35:39.080]  example from\n[01:35:39.080 --> 01:35:39.840]  10 to a\n[01:35:39.840 --> 01:35:40.520]  lower value\n[01:35:40.520 --> 01:35:41.400]  what that's\n[01:35:41.400 --> 01:35:41.780]  going to do\n[01:35:41.780 --> 01:35:42.380]  is increase\n[01:35:42.380 --> 01:35:43.020]  the vocabulary\n[01:35:43.020 --> 01:35:44.500]  if you\n[01:35:44.500 --> 01:35:45.560]  change the\n[01:35:45.560 --> 01:35:46.620]  token pattern\n[01:35:46.620 --> 01:35:47.520]  to be\n[01:35:47.520 --> 01:35:48.780]  not three\n[01:35:48.780 --> 01:35:49.720]  characters\n[01:35:49.720 --> 01:35:50.320]  minimum\n[01:35:50.320 --> 01:35:50.840]  but two\n[01:35:50.840 --> 01:35:51.280]  characters\n[01:35:51.280 --> 01:35:51.620]  minimum\n[01:35:51.620 --> 01:35:52.280]  again you're\n[01:35:52.280 --> 01:35:52.620]  going to\n[01:35:52.620 --> 01:35:53.360]  increase the\n[01:35:53.360 --> 01:35:54.040]  size of the\n[01:35:54.040 --> 01:35:54.460]  vocabulary\n[01:35:54.460 --> 01:35:55.260]  right\n[01:35:55.260 --> 01:35:57.300]  and we can\n[01:35:57.300 --> 01:35:57.960]  see that we\n[01:35:57.960 --> 01:35:59.380]  have in\n[01:35:59.380 --> 01:36:00.540]  our data\n[01:36:00.540 --> 01:36:01.920]  vectors that\n[01:36:01.920 --> 01:36:02.200]  have been\n[01:36:02.200 --> 01:36:02.760]  returned\n[01:36:02.760 --> 01:36:05.420]  11,314\n[01:36:05.420 --> 01:36:06.640]  vectors\n[01:36:06.640 --> 01:36:08.480]  and data\n[01:36:08.480 --> 01:36:09.460]  vectorized\n[01:36:09.460 --> 01:36:10.700]  is a\n[01:36:10.700 --> 01:36:11.100]  sparse\n[01:36:11.100 --> 01:36:11.680]  matrix\n[01:36:11.680 --> 01:36:13.860]  right so\n[01:36:13.860 --> 01:36:14.220]  it's saying\n[01:36:14.220 --> 01:36:14.840]  that this\n[01:36:14.840 --> 01:36:15.240]  is a\n[01:36:15.240 --> 01:36:15.640]  sparse\n[01:36:15.640 --> 01:36:16.400]  matrix the\n[01:36:16.400 --> 01:36:17.100]  dimensionality\n[01:36:17.100 --> 01:36:17.420]  of that\n[01:36:17.420 --> 01:36:17.800]  sparse\n[01:36:17.800 --> 01:36:18.580]  matrix is\n[01:36:18.580 --> 01:36:20.400]  11,314\n[01:36:20.400 --> 01:36:22.720]  by 7,846\n[01:36:22.720 --> 01:36:25.100]  so every\n[01:36:25.100 --> 01:36:26.540]  row is\n[01:36:26.540 --> 01:36:27.420]  a document\n[01:36:27.420 --> 01:36:29.520]  and every\n[01:36:29.520 --> 01:36:31.060]  word in\n[01:36:31.060 --> 01:36:31.720]  our vocabulary\n[01:36:31.720 --> 01:36:32.480]  is a\n[01:36:32.480 --> 01:36:33.300]  column in\n[01:36:33.300 --> 01:36:33.540]  here\n[01:36:33.540 --> 01:36:36.580]  and it\n[01:36:36.580 --> 01:36:37.320]  says that\n[01:36:37.320 --> 01:36:38.080]  out of\n[01:36:38.080 --> 01:36:38.420]  all of\n[01:36:38.420 --> 01:36:38.880]  these\n[01:36:38.880 --> 01:36:40.480]  elements\n[01:36:40.480 --> 01:36:42.260]  there are\n[01:36:42.260 --> 01:36:42.880]  just\n[01:36:42.880 --> 01:36:45.720]  7,2,9489\n[01:36:45.720 --> 01:36:48.340]  elements that\n[01:36:48.340 --> 01:36:48.700]  have a\n[01:36:48.700 --> 01:36:49.420]  non-zero\n[01:36:49.420 --> 01:36:49.860]  value\n[01:36:49.860 --> 01:36:53.240]  so it's\n[01:36:53.240 --> 01:36:53.820]  a sparse\n[01:36:53.820 --> 01:36:54.340]  matrix\n[01:36:54.340 --> 01:36:54.820]  why do\n[01:36:54.820 --> 01:36:55.140]  are there\n[01:36:55.140 --> 01:36:55.480]  so many\n[01:36:55.480 --> 01:36:55.900]  zeros\n[01:36:55.900 --> 01:36:56.520]  because\n[01:36:56.520 --> 01:36:58.080]  every document\n[01:36:58.080 --> 01:36:59.460]  only contains\n[01:36:59.460 --> 01:37:00.000]  a small\n[01:37:00.000 --> 01:37:00.740]  subset of\n[01:37:00.740 --> 01:37:01.180]  words\n[01:37:01.180 --> 01:37:02.940]  compared to\n[01:37:02.940 --> 01:37:03.140]  the\n[01:37:03.140 --> 01:37:03.700]  vocabulary\n[01:37:03.700 --> 01:37:04.220]  size\n[01:37:04.220 --> 01:37:04.420]  and\n[01:37:04.420 --> 01:37:04.920]  imagine\n[01:37:04.920 --> 01:37:05.240]  we are\n[01:37:05.240 --> 01:37:05.680]  looking at\n[01:37:05.680 --> 01:37:06.180]  a vocabulary\n[01:37:06.180 --> 01:37:06.760]  size of\n[01:37:06.760 --> 01:37:08.080]  only 7,800\n[01:37:08.080 --> 01:37:09.020]  words right\n[01:37:09.020 --> 01:37:09.400]  now\n[01:37:09.400 --> 01:37:10.640]  imagine when\n[01:37:10.640 --> 01:37:11.400]  that's a\n[01:37:11.400 --> 01:37:12.020]  million words\n[01:37:12.020 --> 01:37:12.320]  or two\n[01:37:12.320 --> 01:37:13.100]  million words\n[01:37:13.100 --> 01:37:13.900]  how sparse\n[01:37:13.900 --> 01:37:14.460]  that matrix\n[01:37:14.460 --> 01:37:15.000]  is going to\n[01:37:15.000 --> 01:37:15.180]  be\n[01:37:15.180 --> 01:37:16.560]  right\n[01:37:16.560 --> 01:37:17.700]  and this\n[01:37:17.700 --> 01:37:18.900]  matrix is\n[01:37:18.900 --> 01:37:19.520]  in what's\n[01:37:19.520 --> 01:37:19.700]  called\n[01:37:19.700 --> 01:37:20.300]  compressed\n[01:37:20.300 --> 01:37:20.960]  sparse\n[01:37:20.960 --> 01:37:21.820]  row format\n[01:37:21.820 --> 01:37:23.140]  what I\n[01:37:23.140 --> 01:37:23.580]  would highly\n[01:37:23.580 --> 01:37:24.360]  recommend is\n[01:37:24.360 --> 01:37:26.120]  you look at\n[01:37:26.120 --> 01:37:28.520]  scipy and\n[01:37:28.520 --> 01:37:29.320]  the different\n[01:37:29.320 --> 01:37:30.540]  formats in\n[01:37:30.540 --> 01:37:31.300]  which sparse\n[01:37:31.300 --> 01:37:32.140]  matrices are\n[01:37:32.140 --> 01:37:32.760]  represented\n[01:37:32.760 --> 01:37:36.180]  and why\n[01:37:36.180 --> 01:37:37.700]  one representation\n[01:37:37.700 --> 01:37:38.620]  is better than\n[01:37:38.620 --> 01:37:39.000]  another\n[01:37:39.000 --> 01:37:40.700]  depending on\n[01:37:40.700 --> 01:37:41.460]  what kind of\n[01:37:41.460 --> 01:37:42.360]  operations you\n[01:37:42.360 --> 01:37:42.920]  want to do\n[01:37:42.920 --> 01:37:43.600]  on it\n[01:37:43.600 --> 01:37:44.340]  right\n[01:37:44.340 --> 01:37:45.760]  so I\n[01:37:45.760 --> 01:37:46.420]  realize we\n[01:37:46.420 --> 01:37:46.760]  are out of\n[01:37:46.760 --> 01:37:47.320]  time so I'll\n[01:37:47.320 --> 01:37:47.860]  stop here\n[01:37:47.860 --> 01:37:48.480]  but we'll\n[01:37:48.480 --> 01:37:49.140]  continue from\n[01:37:49.140 --> 01:37:49.900]  here in the\n[01:37:49.900 --> 01:37:50.440]  next lecture\n[01:37:50.440 --> 01:37:50.880]  tomorrow\n[01:37:50.880 --> 01:37:53.340]  and I'll\n[01:37:53.340 --> 01:37:53.960]  share this\n[01:37:53.960 --> 01:37:54.560]  so that you\n[01:37:54.560 --> 01:37:55.300]  can go through\n[01:37:55.300 --> 01:37:55.780]  it in your\n[01:37:55.780 --> 01:37:56.440]  own time just\n[01:37:56.440 --> 01:37:56.920]  after this\n[01:37:56.920 --> 01:37:57.180]  lecture\n[01:37:57.180 --> 01:37:57.940]  all right\n[01:37:57.940 --> 01:37:59.400]  thanks for\n[01:37:59.400 --> 01:37:59.940]  staying there\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!pip install stanza","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:15:28.167679Z","iopub.execute_input":"2024-11-15T05:15:28.168133Z","iopub.status.idle":"2024-11-15T05:15:41.033061Z","shell.execute_reply.started":"2024-11-15T05:15:28.168095Z","shell.execute_reply":"2024-11-15T05:15:41.032033Z"}},"outputs":[{"name":"stdout","text":"Collecting stanza\n  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.13.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.26.4)\nRequirement already satisfied: protobuf>=3.15.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.32.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from stanza) (3.3)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.66.4)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nDownloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: stanza\nSuccessfully installed stanza-1.9.2\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import sent_tokenize\n\ndef split_into_paragraphs_custom(text, max_len=3000):\n    sentences = sent_tokenize(text)\n    paragraphs = []\n    current_paragraph = \"\"\n\n    for sentence in sentences:\n        if len(current_paragraph) + len(sentence) > max_len:\n            paragraphs.append(current_paragraph)\n            current_paragraph = sentence\n        else:\n            current_paragraph += \" \" + sentence\n\n    if current_paragraph:\n        paragraphs.append(current_paragraph)\n\n    return paragraphs\n\ntext = reslarge['text']\nparagraphs = split_into_paragraphs_custom(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:27:12.970807Z","iopub.execute_input":"2024-11-15T05:27:12.971203Z","iopub.status.idle":"2024-11-15T05:27:12.997637Z","shell.execute_reply.started":"2024-11-15T05:27:12.971164Z","shell.execute_reply":"2024-11-15T05:27:12.996610Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:27:15.013044Z","iopub.execute_input":"2024-11-15T05:27:15.013770Z","iopub.status.idle":"2024-11-15T05:27:15.020643Z","shell.execute_reply.started":"2024-11-15T05:27:15.013727Z","shell.execute_reply":"2024-11-15T05:27:15.019736Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"[\"  Thank you. Good morning, everyone. So we had started to discuss text analysis or natural language processing. And we had looked at a number of examples, the kinds of things that we can do with text. And today we're going to start with actually some of the techniques that we use to convert documents to vectors. Now, hopefully, let's just recap very quickly what we have talked about up until now. So we have as input a set of documents. We are ignoring any hyperlinks now. We have talked about how we can model hyperlinks as graphs. And so if you ever had to analyze content that actually had hyperlinks as well, then you would actually have to combine content with what the graph-based algorithms were. So we're going to focus on the content itself, right? So we have a corpus. What we want to do is convert that corpus into a tabular representation. where we have a set of variables that we have extracted from this text. And we call this process vectorization. We talked about the fact that the columns here, the features that we extract here, the simplest method, which is known as a syntactic approach to vectorization, creates a vocabulary and then uses that vocabulary, each of the individual elements of this vocabulary as features. We talked about the fact that the basic vocabulary is all unique words that appear within the corpus. But then we also talked about key phrases like the United Nations, like the United States of America, like the Republic of India. Or in fact, we even used some examples like Durga Puja. These are all key phrases. And these need to be extracted from the text because we don't really know which sequence of words creates a key phrase. And in fact, key phrases are sometimes even dependent on what are these documents about. And then what we said was that actually some key phrases have a type. And we refer to those as named entities. where we had seen examples like the Bhartia Janta Party being a named entity. The Congress would be another example of a named entity. The Ahmadinejad Party, like the Bhartia Janta Party, like the Bhartia Janta Party would be a key phrase that has a type, therefore it would be a named entity. All three of these would be of type political party. And then the three examples that we gave are what we refer to as instances of this type. So the unique words and the key phrases create our vocabulary. And we can then assign each one of these elements of the vocabulary as columns. And then we can either put a one or a zero as the vector representation of a single document. Does this element of the vocabulary appear in this? Does it not appear? We also saw an alternative. And so in this case, this is a binary vector. We also looked at an alternative where we can put in the frequency with which words appear in the document. And in Python library vocabulary, we call that the count vectorizer. The vectorizer is what creates the vector. You basically have a count vector when you produce it through the count vectorizer.\",\n \"And then we also looked at the TFI-DF vectorizer. That balanced the frequency with which a word appeared in a document and the frequency with which it appeared across the corpus, across documents. So the more common the word that it appears in all documents within the corpus, the less excited we are about it as representing what an individual document is about. The higher the frequency of the word in the document, the more encouraged we are that this document is something to do with this element of the vocab. Right? And we will look at this in a little more detail soon in any case. But these are the three syntactic vectorization approaches. And then what we said was the big problem with this, there are a number of problems, but the biggest problem is that it is vocabulary dependent. And so what we really want is a representation of a document that is semantic in nature. where these columns represent semantic elements. What is this document about? Conceptually. So at a higher level of abstraction, we are wanting to have columns that can represent the semantics. What is this document about? And what we said there was if we can find topics where topics drive the vocabulary that is used. So you could actually say the author of a document and the topic together drive the words or vocabulary. used in a topic. And in fact, typically a document isn't related to a single topic. It is a mixture of topics. And so based on that mixture of topics, the vocabulary within this document is generated. and we will look at or we will model these topics as hidden variables. And another word for them is latent. Latent. Latent. Latent. Latent. Latent. Latent. Latent. Latent. Okay. And so this is what we are, we have discussed to date. We haven't gotten into any depth. Now we are going to go into depth. So the basic idea here is that documents should be represented as vectors in what is referred to as the vector space model where every point in this space and of course I'm only drawing two dimensions or three dimensions but every point in this space is now a document. and as long as we can have all documents represented in this space as vectors we can now calculate similarity between them. We can therefore apply a lot of the machine learning algorithms now on documents as we had done on structured data. So this idea of a vector representation is really all about creating a structured representation of documents. If we think of named entities we could also instead say forget about the vocabulary that does not belong to the set of named entities. I am only interested in any political party that is mentioned any person that is mentioned and so on and so forth. Right? And so out here if for a document I have multiple political parties that I've mentioned now what I can do is I can store them as an array for example. Right? And similarly people that I've mentioned I could represent as an array.\",\n \"Now of course the array is not great for me because I need one value in each element and so I could actually represent it by doing the equivalent of one hot encoding. And so I would generate a much larger dimensionality where I would now have the BJP, the Congress, etc. etc. as columns and I would now have a one in each of the ones that appear and zeros in the ones that don't appear. Right? So out here what am I doing? If I limit myself to only using the named entities, I am ignoring all words that are not entities, I am restricting my vocabulary. I am doing dimensionality reduction and focusing on just the different types of data that are represented in the text. Now, which one is the better approach to use? Ultimately, you're going to end up with this vector representation. all that's happening is that out of the complete vocabulary, which is key phrases and unique words, we are now coming up with a representation where we have limited it to be only those key phrases and words that are instances of some entities. things. Now, if there's too much loss of information by dropping the rest of the vocabulary, you're not going to get a very good model, right? But this is really, again, a feature engineering problem. So, for now, we will assume that we want to use all of the unique words and key phrases in our vocabulary. and that would be our vector space model for our documents. Now, one of the downsides of doing this representation, which is also known as the bag of words representation, is the fact that we ignore the ordering of words. So, the poor man ate the food and the man ate the poor food have the same bag of words representation because we are ignoring the sequence in which the words are appeared. Right? So, in a way, this is a simplification of the problem where we are saying, well, the sequence in which the words appear are not important. All that's important is that the word appeared. and so when we were talking about the Bayesian approach to text, and we said, you know, they were really representing only statistical properties, and one of the simplifying assumptions they made was that the sequence was not important. We look at why that was the case, but you can see that the vector space model was kind of influenced by that thought process, that, you know, the order of words is not important. So, then, essentially, from a pre-processing of text is concerned, the basic steps are we tokenize the text, which means we break it down to a word representation. Now, of course, you would have, for some of you who have played with JackGPT, you know that they also talk about the number of tokens, they don't talk about words, right? And part of the reason why they talk about tokens rather than words is that they have actually got a subword tokenization method, right?\",\n \"And so, while we have talked about taking words as the primitive object and finding key phrases and representing that as a vector representation, which is the traditional way of doing it, the more recent innovations have started to say, actually, we are better off looking at subword tokenization. So, breaking it by the way, in case of vector space model, not in case of tokenization, in case of vector space model, are we taking the plurals of words, or separate words or using the plurals and the singulars are same? It depends on what you do, right? Here, the second step is stemming and lemmatization, right? and this typically removes not only plurals and singular and k makes them one, but it actually goes to the root word, right? Like organize without the e, replaces organizing, organizes, and organize. Okay. Okay. Okay. So, again, the reason for doing that is to reduce the dimensionality. The curse of dimensionality is a real problem when it comes to text, right? Because the number of unique words and key phrases often goes into millions. So, once we have done the tokenization, again, we may or may not want to remove the punctuation because in certain applications, the punctuation may be important. Another thing that people say you should do is to convert every word to lowercase. That may or may not be the best thing to do. When you're looking for named entities, for example, there are features that say, well, you know, if you have some capital letters, it's likely to be a noun, which could be a person or a, you know, a location or whatever. But in general, what we would do is to minimize the vocabulary size after tokenizing and removing punctuation, we would also change the case to lowercase for all words. And then we would do stemming and lemmatization, two different approaches, to mapping multiple words to their root form. And then we remove stop words, words like a, and, the, and, but. So there is a list of stop words. So this is just based on a lookup to say, is this word in the stop words? if it is, we throw it out and don't add it to our vector representation. Now, stemming and lemmatization. Lemmatization requires a thesaurus. Stemming algorithmically changes words. Right? So am will become be in lemmatization. And remains am from stemming. Going, remains going in stemming, but becomes go in, sorry, become, going, remains going in lemmatization, but becomes go in stemming. And having, becomes have in lemmatization, where it becomes H-A-V in stemming. Now, stemming typically can result in these kind of non-English words while identifying the root. And so it takes away a little bit from the understanding of what is the word that is important within a document once you stemmed it. So just be aware of this, but these are ways that were used in the past, and so it's less relevant in any case at this point to reduce the vocabulary size. And I'll tell you why it's become less relevant later on. Okay.\",\n \"Now, we talked about the syntactic representation, and we've talked about TF-IDF. so once we have identified our vocabulary, whether this is a word or a key phrase, we need to assign it a weight, and that one way of assigning the weight is TF-IDF, other than just looking at the count, or a binary representation, or one of zeros. those. So let's look at a very simple example. We have these eight documents here. We've actually just got the titles of these documents, and we can see by looking at this that the first few documents seem to be about human-machine interface. the last three documents here are about graph theory. We studied how to analyze graphs, so these are related to that topic. Now, this is our corpus, all of it, and so we can convert it into a vectorized form in this way here. Now, what I've been talking about is that every word becomes a column. I have taken the transpose of that. And so it doesn't really matter. What we are now seeing is that a document has a vector representation that looks like this. Now, to make it easier, I've taken a subset of the vocabulary. We are taking the vocabulary to only be words, not key phrases. And so we have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. A vocabulary of size 12 here, and you can see that we have count vectorized each of our documents. words. Now, if we look at the word human in document 1, if we were calculating the TF-IDF, we would say, okay, human has a frequency of 1. There are 1, 2, 3, 4, 5, 6 words. This is a stock word. So we have removed it. And I can only see the 3 words here. The rest of the words I have kind of cut off because I didn't have enough space to show it. So 1 out of 6 is the frequency part of this. So this is the TF, the term frequency. And here 8 is the number of documents. And we can see human appears in document 1 and document 4. So it's document frequency is 2. And so the way we calculate TF, IDF is term frequency inverse document frequency. So 2 by 8 is the document frequency. We make it 8 by 2. And we get this value of 0.23. So if we wanted to now instead represent our documents as vectors of TF, IDF scores, we can calculate them. So the first step really of calculating the TF, IDF is to really do a counter vectorization of the documents. Now I'm going to skip this because we looked at this in principal component analysis. But what I'm going to move on to is what is singular value decomposition. singular value decomposition is a linear algebra method and is based around eigenvalue decomposition. And this was the first semantic vectorization approach that was proposed. Now singular value decomposition was the method used for doing semantic vectorization. When applied to textual documents, it was known as latent semantic analysis. singular value decomposition. So what was singular value decomposition first of all? This matrix that I had X where we had word one, word two, till word M, and we had D1, D2, to Dn. We are going to refer to this as X.\",\n \"And so when we go X transpose, X transpose is going to be an N by M matrix multiplied by an M by N matrix. So what are we going to end up with? We are going to end up with a N by N matrix. When we were doing principal component analysis, what we did was we said we have X1, X2, to XN, and we had various objects, O1, 2, O, M. And we first computed, the covariance matrix. And the covariance matrix was an N by N matrix, which, given that we have these variables describing each object, it stored the variance along the diagonal of the matrix, and the covariance in the non-diagonal elements here. Right? So this here was the variance of Xi, and this, depending on which row and column it was, was the covariance of Xi and Xi. So here also we've got an N by N matrix. So what can we kind of visualize this as? We can visualize it that we are treating this matrix X as each word being an object that is represented by its occurrence within the N document. So the documents themselves are now our variables that in some way are describing our word. And so this X transpose X, if we do an eigenvector decomposition on it, which is essentially the same as what we do in principal component analysis, and we have our eigenvectors. Right? What that means is for every one of these eigenvectors, X transpose X times VI is equal to lambda I. VI. This is by just the definition of an eigenvector and eigenvalue which is lambda I. Now let us define R vectors U1 to UR as this here. So we are taking the X matrix, which is this matrix here. We are multiplying by VI, which is the eigenvector of X transpose X. And we are multiplying by 1 divided by the square root of lambda I. it turns out that these UIs are also orthonormal. Why am I saying also? Because these eigenvectors are orthonormal. So these sets of vectors are orthonormal as well. So if I now say X and I essentially put B1 B2 to BR as columns within a matrix and I call this matrix V. Then what we have is X times V is equal to U lambda. Where what are U? U R, U 1, U 2, 2, U R. And lambda here is lambda 1, lambda 2 to lambda R. R. right? And it's the square root of the lambda. Sorry. This should be square root of that. So we've taken this lambda I and multiplied it on this side. right? Now, if we multiply both sides by the inverse of V, we get X V V inverse is equal to U lambda V inverse. So this will become the identity matrix so we can ignore it. so this is just X. And because the vectors V are orthonormal, V inverse is actually the same as V transpose. Right? because if we are looking at a vector, a matrix V, the inverse is something that produces the identity matrix. Now, each element in the product of these two, right? So V, V inverse, each element here, any of these elements, are essentially the dot product of a vector in V, which is a row, and a vector in V inverse, which is a column. So if we take the transpose, what are we going to end up with?\",\n \"This element here is going to be V1 dot V1, which is the length of the vector, which is 1, because it's orthonormal. And any non-diagonal element is going to be VI dot VJ. And of course, that is equal to 0, because they are appendicular to each other, right? That's the orthogonal part of this. And so V, V transpose is going to be the identity matrix. Oops, apologies. And so the inverse of a matrix where the columns are perpendicular to each other is going to be the transpose, right? And so what we are getting here is an interesting behavior. That if we take the eigenvectors of X transpose X, we stack them up into columns and take the transpose of that, which is the equivalent of stacking them as rows, and we define a new set of vectors as we have done here, what we are getting is actually a decomposition or matrix, factorization. This is called matrix factorization, where we have a matrix X that can be represented as a product of two or more matrices. Where have we heard factorization before? In school, we were told factorize 12, and we would write 2 plus 2, 2 times 2 times 3. These are the prime factors, right? So we are doing the same here, but we are doing it at a matrix. Now, this here, like we said, is a diagonal matrix with the diagonal elements being the square root of lambda i. transpose. Now, if we look to calculate XX transpose now, we know that X can be written as U lambda V transpose, so we plug those in here, and what do we get? We get U lambda square U transpose, right? because what happens when we open this up, it becomes U transpose, lambda transpose, V transpose of transpose, which is V. Oops, the wrong way. It becomes V lambda U transpose. That's this part here. We multiply it with U lambda V transpose, V transpose, V is the identity matrix, so it disappears, and we end up with U lambda square U transpose. The square of a diagonal matrix is really the square of the diagonal element, and so you end up with essentially this representation here, which is XX transpose is equal to U, the matrix with all lambda 1 to lambda R, U transpose, and if we now take this to this side by multiplying by U, we are basically going to get XX transpose U is equal to 2 with lambda 1 to lambda N, which is actually what we were seeing here. so while the V is are the eigenvectors of X transpose X, these U is are the eigenvectors of XX transpose and both have the same eigenvalues. X transpose now let's look at what XX transpose is. X X transpose X is M by N N transpose would be N by M and so this would be an M by M matrix. So what we are doing here is just as we said that X transpose X was like calculating the covariance matrix where we treated our documents as our variables out here XX transpose on the other hand is the covariance matrix for the representation of documents as words. Right? There are M words in our vocabulary here. We are representing this as an M by N matrix. So we are treating every word as a variable instead. Right?\",\n \"And so in a way what we are doing here when we do singular value decomposition and do a matrix factorization of X into a product of three matrices what we end up with is a parallel eigenvalue decomposition of X transpose X and X X transpose. X so we have the eigenvectors and remember what we did here. Right? When we did a principal component analysis and we chose the K eigenvectors what did we end up with? We ended up with an R dimensional representation of each of our objects. so what has happened here is that we have taken R X transpose sorry XX transpose where we are saying our words are variables and as a result we have got an R dimensional representation so the U is an R dimensional representation of documents and our V is our R dimensional representation of the of words. Right? These were our variables therefore we got an R dimensional representation for words. In the other case we were treating each of these as our observations and treating words as our variables so we got a R dimensional representation of documents. Right? So we get not just a representation of objects in R dimensional space as we get with principal component analysis and the matrix is with of documents where the matrix is represented as shown here. We end up with an R dimensional representation for words and an R dimensional representation of documents and that is what is referred to as latent semantic analysis. Right? So let's look at what happens here. We have our X matrix which is this one here. Documents as columns and if we look at sorry let's yeah okay so we are saying X is equal to U S V transpose X here was something by 8. How many words were there? 12 words. 12 times 8. This now is going to be 12 times R and becomes our word representation in R dimensional space. Our S here becomes R by R and our V transpose will be R by 8. Which means V itself is going to be 8 by R. Right? And there were 8 documents. and there were 12 words. So we have words as represented in R dimensional space and we have documents represented in R dimensional space. Now just as we did in principal component analysis these diagonal elements are representing the variance. But remember that the variance is the eigenvalues. This matrix here is the square root of the eigenvalues. Right? And so these are referred to as singular values. And just as we had a descending order to the eigenvalues after we did principal component analysis we have a descending order here. The larger the singular value the more of the variance is being captured. But the variance itself being captured is the square of these. Right?\",\n \"So in this case if I want to just visualize my words and documents I will choose the two top eigenvalues as a result of choosing this as a 2 by 2 so I'm saying R is 2 therefore I must look at each word now as being two dimensional with these coordinates and every document which is a column is being represented as a two dimensional representation so now of course I can visualize those like this so each dot here is a word and each dot here is a document and what we can see is there are these three documents that are close to each other these are quite separated from these documents and then you have this one document that's sitting out here which document is sitting right out here it is document number 5 0 1 2 3 4 5 I think I messed up here I've added a new document but essentially if we look at what's happening here not sure why I have nine documents there I'll have to look at it maybe I by mistake incorporated this but if we look at these documents the last three documents were supposed to be graph related and what we are saying is we have one two three four at least these documents potentially are the documents that are also close to each other and this one may be the one that is a little further away from I'll have to come back to you right but here we can see that these are all graph theory these are all human computer interaction and maybe this here is a little more on response time rather than human computer interaction so that's why it is separated we also see that some words here are coming out as similar to each other these are words that have their second axis okay so this word here and this word here are close to each other so what were those words trees and trees and miners and so trees graphs and miners these are definitely to do with graph theory that's why they are appearing close to each other on the other hand if we look at human and user we would expect these to be close to each other right a user of a computer is as of now at least a human so that's the first and fourth so first and second third fourth it's not coming out that close right but what we would expect is that the coordinates of these would come close now of course take into account that these are this is a very small corpus that we are using look at this these two are exactly the same so what are these 1 2 3 4 5 6 7 1 2 3 4 5 6 7 response and time are identical in their representation right so we can see that something interesting is happening here now when we get this 12 by 2 matrix here multiplied by a 2 by 2 matrix multiplied by a 2 by in this case 9 but should be 8 matrix by multiplying just these red portions we get a reconstruction of our x so this is what our x looked like where we can see that clearly the vocabulary that was being used in these documents were different from the vocabulary being used in these documents right but but the sparseness here the zeros here made it look like maybe these weren't as similar to each other but when we reconstruct x by taking u s v transpose we are now getting not a sparse matrix but a dense matrix and words that previously had the value zero in here now don't have a zero value because even though they didn't mention the value the word system they did mention computer and computer and system are really synonyms in the context of what we are talking and so we are now filling in these missing values with numbers other than zero if there are other words in that document that really are suggesting that instead of using the word system we could have used computer right and we are seeing that a lot of this area now has larger positive values across the board and similarly here we have larger positive values across the board compared to what we have in this one right so this shashi tharoor versus my lecture problem to some extent is being resolved by latent semantic analysis but critics of this method said what do you mean by a negative value by looking at a positive value we can say yeah okay so you know this word has some importance in the second document but when we have a negative value what does that really mean does it mean that this word appearing takes away from the meaning of this document or what is that negative value representing right so this was the what was represented as a rebuttal by the Bayesians who then went on to propose a probabilistic latent semantic analysis where essentially you ended up with probabilities in here and so when you looked at the coordinates out here where how do we interpret this these are called topic one and topic two and what was being said here is that word one has a membership of this topic of point two two so this word here is much stronger in its connection to topic one compared to this word here and this word here was much more related to topic two compared to this word here and we can see again right that these last three words are associated with topic two the first bunch of words are much more related to topic one and this word here seems to have an equal representation across those topics and what was that word survey because survey appeared once here in a document that had more of this vocabulary and also appeared here once in a set of documents that actually had this vocabulary right so the jury is out or maybe this word transcends both topics and that is what is represented out here right so you can see that what we are getting is a partitioning of the vocabulary out here but the criticism was why minus what does this minus represent and you can see how if you instead had probabilities where word one was looking like this word two and so on and then the latter words word 10 word 11 and word 12 word 9 word 9 looked more like 4 and 0.6 whereas these were more 0.8 0.9 0.92 the interpretation is so much easier right what we are saying is that if a topic topic one is what is generating a word or if we are seeing a word in a document the likelihood is that topic one generated this word much more than the likelihood of topic two generated it's four times more likely that topic one would have generated this word and so when we look across all the words that appear in a document we can then come up with a document representation saying that the document has maybe 60% of its vocabulary from topic one and 40% of its vocabulary from topic two and that would become the document representation here again what we can see is that these are related to topic one and these are related more to topic two right so the partitioning is happening of documents but what these actually mean especially the negative values became a question that was asked so let's take a break from the theory of it oh actually maybe not I was going to go into some text but let me now do this next topic before I get on to some of the code so what we have seen is three methods for syntactic vectorization and as of now we have seen one method for semantic representation or semantic vectorization where each document now is represented in this two-dimensional topic space either way what we now want to do is apply this to something and when we started this topic we had talked about text categorization where we have documents that have a particular class associated is this a sports document is this a politics document is it a whatever so now that we have been able to vectorize documents this essentially becomes a standard machine learning problem but before we even vectorize the documents or maybe we could look at this as justifying why we are doing what we are doing in the vectorization especially the syntactic vectorization we talked about the vector space model for documents let's look at some probabilistic models for documents instead so the idea here is we are talking about a probabilistic model therefore we must be talking about generative models and what we are saying is that we have a corpus a corpus consists of a number of documents and each document is generated from a mixture model parametrized by some parameter vector term now we've come across mixture models before when we were looking at the EM algorithm for clustering and essentially a mixture model is a probability distribution defined by a linear combination of individual probability distribution also known as components and when we were studying the EM algorithm we had assumed that these components are what they are Gaussian right and each Gaussian had its own parameters and there was another parameter which was the probability of a component being called upon to generate data so similarly we can say that a document DI is generated by a number of components and each of those components has a probability of being asked to generate a part of this document so when we think of a document we are really saying that a document is a bag of words why are we using the word bag we are saying it's a bag not a set because a bag can have multiple copies of the same word a set has only unique elements right so what we are saying is that this document has words in it and those words are generated by calling upon these components so the question is what kind of probability distribution are these components and there are two models probabilistic models of documents one is the multivariate Bern-Lowli and the second is the multinomial distribution model so we are going to look at both of those the multivariate Bern-Lowli model what does that tell you there are two really important words here multivariate what is multivariate jay kishan any idea what it is multivariate we say multivariate statistics also no sir okay sumit any idea what multivariate stands for this word multiple variables it's as simple as that multivariate means multiple variables so we are saying we have x1 x2 x3 till xn what are these x1 to xn in our case because we are looking at documents these are elements of our vocabulary so they could be words they could be keywords key phrases and of course Bernoulli we understand in that they either occur or don't occur right so this is just a binary variable random variable so this and so the model of generating a document which is multivariate Bernoulli is that we are saying that a document you can think of a document consisting of n slots one for every word or key phrase can you hear me can you hear me yes sir you are audible okay thank you so essentially we are saying this document has n slots one slot for every word or key phrase and what we want to decide is does this appear or not appear in the document so this is the equivalent of our binary vectorization what are the parameters of this model for each element we have a separate Bernoulli distribution the probability of x1 being a one is going to be some probability theta one the probability of x2 equal to one is going to have a different probability right and so what we are saying here is that for each one of these words we have its own parameter theta n so there are n parameters here that we need to learn but why are we saying components think of components as topics or actually let's not even confuse it with topics right now think of each of these as classes we are talking about the corpus as a whole which is a set of documents being generated from a mixture model so whenever we are generating a document we first decide which component are we going to call which class are we going to generate a document and depending on the class we will generate the words that appear and to generate the words that appear we need n parameters so the total number of parameters that we need here is if we have k classes and we have n parameters for each class we have n times k parameters but each of the classes themselves also have a probability of being asked to be generated and so we have the probability of c1 is equal to theta c1 probability of c2 is equal to theta c2 till the probability of ck which is equal to 1 minus the sum of probability of ck minus cj minus 1 where j goes from 1 to k actually 1 minus sigma of theta cj where j goes from 1 to k minus 1 so there are k minus 1 more parameters that we need so the total number of parameters we would need is this so that is the multivariate Bernoulli model what about the multinomial distribution distribution is where we have a categorical variable and so what we are doing here is we are saying for every element of the vocabulary we call them v1 v2 till vm we have a probability of it being chosen as a word or key phrase in the document so we have a document we now decide on a length of the document and so if we say the length is k let's not call it k because we've already let's call it capital n we have capital n slots that need to be filled again we choose the class to which the document belongs and then we choose from the probability of words given the class that has been chosen and what is that that is this categorical distribution and what we are saying now is we are choosing from all of our vocabulary based on these probabilities we are choosing words at random and filling our slots with those words so what are we going to end up with we are going to have n selections from m possible values which are each of the elements of a vocabulary so what is the probability of a document going to be given that it has been generated from a particular class when we see a document it is of length n and there are m values here from which we can choose these n what we are going to end up with is essentially this here this capital n is the number of trials and these are the occurrences the number of times each of our words are occurring so what we had said in terms of our notation here is we have n trials we are going to say n divided by x1 factorial x2 factorial till xm factorial where each of these x1 x2 and xm are the frequency with which each of these elements appear in our n trials that we have so this is the number of ways in which n slots can be filled with these frequency of m vocabulary words and we then multiply by the probabilities right so p1 to the power of x1 times p2 to the power of x2 all the way to pm to the power of x where p1 to pm are these probabilities right so that's what we are representing okay so the multinomial distribution model of a document is different from a multivariate bernoudi model where we are saying that the words that appear in the document they can have a frequency associated with them they fill up n slots and therefore we have to choose the length of a document also but once we've chosen the length of the document we are selecting what to fill these documents by a probability distribution okay and that's the multinomial distribution model now given these two models we end up with if we want to do a classification model we end up with two versions of the naive ways algorithm remember that the naive base algorithm made a simplifying assumption if we had n input variables and we had the class c and we wanted to know what the probability of a class given a set of input values evidence e we can now represent it as the product of the probability of each xi i given c where i goes from 1 to n multiplied by the probability of c divided by the probability of this is what we had done this was the naive base model algorithm now why do we have two different versions of the naive base algorithm depending on whether this is a binary vector or whether this is a count vector a count vector is really required for the multi momemium document model why because we need to know the number of times n i t is the number of times the tth element of the vocabulary appears in the document t i and that is what is represented within the count vector is the number of times that word appears and that's why we need this count vector when we want to calculate the probability of a document given a class when we want to use instead the multivariate Bernoulli model this d i t is either 1 or 0 where 1 is when w t appears in di and 0 when w t does not appear in di right and so whenever we are using the multivariate Bernoulli model the value here how we calculate this essentially changes out here in the multivariate Bernoulli we can see we are taking the product over the vocabulary and we are using the Bernoulli distributions formula right p of x to the p of theta to the power of whatever say here the p to the power of x times 1 minus p to the power of 1 minus x is the probability of x or we can even write this as theta we're doing the same thing when we are wanting to use the multinomial distribution this has to be calculated differently and how is it calculated differently it's basically going to use this formula here which is what we have as the formula for the multinomial distribution but we are also multiplying by the probability of the length of the document and this probability of the length of the document we need to figure out what is the distribution going to be for this right so that is what changes in the naive base algorithm depending on what kind of vectorizer we are using here when we use a binary vectorizer it works pretty much the same way as the standard naive base when we choose a count vectorizer this part changes and we need to use instead this distribution which is the probability of bi given cj okay I know this has got a little heavy I'm aware of that so let's leave it here for now and let me go to some code and I'll be sharing this in your educo lab as well so here there are going to be two ipython notebooks that I share one is the text vectorizer and the other is toxic comment classification here we are using in sklearn a standard data set which consists of news groups 20 news groups so in the good old days people would have news groups where they would email into this news group email address and then all of the messages associated with a particular topic would get collated and would get shared with other people that were interested in that topic this was in the good old days when the internet was not as mature as it is today we didn't have social media and so on so essentially this data set consists of a number of messages and we can see that the structure of the data set we have essentially a dictionary and one of the elements of the dictionary is data and that has as its value a list and in that list you basically have a set of messages this is one message and in that message you have who sent it what was the subject and where did they post it and so on and then it has some text so really what we want to do is we want to use this text we want to ignore these email addresses and all that kind of stuff we want to really use this text to be able to automatically classify it into whether it is about the Christian religion or whether it is about hockey or the Middle East or motorcycles so if you look at this the labels the class labels that we are hoping to use here are actually a taxonomy now what is a taxonomy a taxonomy is a tree structure so when Yahoo was the main search engine for the internet they had two ways in which you could navigate and find content one was they provided or created a taxonomy for the internet where you had everything here I can't remember the exact word but this was basically the root node and then you had different child nodes now in our case here we have social recreation as two of the child nodes that I can see here within social we have further splits and I can see one of them is religion and then within religion we have Christian I presume we will also have other nodes here like Islam or maybe Hindu Sikh and so on right and here in recreation we have sports and within sports we have hockey and again we would expect it to have cricket and various other things too we also have talk politics right so out here this would be talk this would be politics and then we have Middle East and again we would expect other subtopics as well right so the dot here this is just a way of converting your taxonomy into a string where you use a dot as a separator to separate these out and basically what Yahoo did was they would have a bunch of documents hanging off not just the leaf nodes but you could also have documents hanging off here where they didn't know quite exactly how to split it further into its sub categories and so the idea here is that we want to classify every one of these messages into a leaf node and there are 20 such leaf nodes that we have provided and here they are right so alt dot atheism computer dot graphics operating systems and so on right so the first thing that we want to do here is get the data into a shape that we are interested in if we look at the data that we have been provided the data consists of five different keys they are dictionaries and their keys are data file names target names target and description we are only interested in actually the content and the target name the target is just a one-to-one mapping of a string onto an integer from what I can take up right so we brought that in out here also we don't really and so the first thing we do typically like we said is we will lemmatize the data we will clean up some issues in the data like remove email addresses and stuff like this remove new line characters single quotes right so punctuation so that's happening here and then out here is where the lemmatization is happening using another library that you will now have to start to explore called spacey and spacey is one of the leading natural language processing libraries that is there so getting familiarity with that is going to be important now when we lemmatize the results of the lemmatization depend on another piece of information which are called POS tags right so the these POS tags are assigned to individual words POS stands for part of speech and so if you look up part of speech you will find part of speech essentially you are assigning tags to every word tags like nouns adjectives verb adverb right and knowing the POS tag improves the quality of lemmatization that is done to it right because you wouldn't want to lemmatize somebody's name for example even though it looks like a prime template for doing so so what we are doing here is we are loading a model a pre-trained model in spacey if we look at this en stands for english so this is a model specifically for the english language and sm stands for small that is large models also and medium sized models that you can pull up and typically when we load this model this model has all sorts of processing capabilities and we are disabling named entity recognition and entity parsing or sentence parsing in here all we are interested in doing at this point is tokenization and POS tags identification so when we run this what is happening here is we are getting our data from our data frame here and we are choosing just the values in the content column we are converting it into a list we are removing emails we are removing new line characters removing certain punctuation we are loading this model the NLP model and we are oh in fact before we load that NLP model we are executing on this here where we are removing punctuation and converting each element in our list which is one of these posts done to our groups and converting it into a list of words and we are then calling the lemmatization piece here that is using the spacey model and looking at only lemmatization of these post times right so it's taking each token and lemmatizing it and providing that in our texts out which is another list which is being stored in data lemmatized and that is being returned so when we call our get lemmatized clean data passing the data frame here what we end up getting back is this list of words that appear within that message that has been lemmatized punctuation has been removed but we can see we still have some issues here we have an apostrophe s out here so really we should be doing some further cleaning but this is the basic cleaning that we are doing here now at this point I'm going to take the data lemmatized and that is really the training data that we have and I'm passing it through in sklearn we have the feature extraction dot text count vector and I'm saying I want to tokenize based on words I only want words in my vocabulary that have a minimum of 10 documents that they appear in I want to use a list of stock words that is already embedded within this function this method they're the English stock words that I want to remove I want to make all of my words lowercase and I only want to have in my vocabulary words that have greater than or equal to three characters within it and I do a fit transform on it and I end up with with this fit transform creating a vocabulary where it's matched every unique token every word onto your unique ID and essentially what I'm seeing here is that the vector that we generate for every document the 7083rd element in that vector is going to be representing the word thing and the frequency within it because I'm calling the count right so that's all of my vocabulary here of which we've seen we have a few thousand words we can see the vocabulary is actually 7846 right now when I give you this this ipython notebook you can play around with some of the parameters reduce the minimum df for example from 10 to a lower value what that's going to do is increase the vocabulary if you change the token pattern to be not three characters minimum but two characters minimum again you're going to increase the size of the vocabulary right and we can see that we have in our data vectors that have been returned 11,314 vectors and data vectorized is a sparse matrix right so it's saying that this is a sparse matrix the dimensionality of that sparse matrix is 11,314 by 7,846 so every row is a document and every word in our vocabulary is a column in here and it says that out of all of these elements there are just 7,2,9489 elements that have a non-zero value so it's a sparse matrix why do are there so many zeros because every document only contains a small subset of words compared to the vocabulary size and imagine we are looking at a vocabulary size of only 7,800 words right now imagine when that's a million words or two million words how sparse that matrix is going to be right and this matrix is in what's called compressed sparse row format what I would highly recommend is you look at scipy and the different formats in which sparse matrices are represented and why one representation is better than another depending on what kind of operations you want to do on it right so I realize we are out of time so I'll stop here but we'll continue from here in the next lecture tomorrow and I'll share this so that you can go through it in your own time just after this lecture all right thanks for staying there\"]"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage\ntext = reslarge['text']\nparagraphs = split_into_paragraphs(text)\nl = 0\nfor para in paragraphs:\n    print(para + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:17:39.928647Z","iopub.execute_input":"2024-11-15T05:17:39.929337Z","iopub.status.idle":"2024-11-15T05:17:54.602294Z","shell.execute_reply.started":"2024-11-15T05:17:39.929291Z","shell.execute_reply":"2024-11-15T05:17:54.599565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:06:00.767961Z","iopub.execute_input":"2024-11-15T05:06:00.768696Z","iopub.status.idle":"2024-11-15T05:06:00.774345Z","shell.execute_reply.started":"2024-11-15T05:06:00.768659Z","shell.execute_reply":"2024-11-15T05:06:00.773364Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"len(reslarge['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:57:20.333669Z","iopub.execute_input":"2024-11-15T04:57:20.334561Z","iopub.status.idle":"2024-11-15T04:57:20.340141Z","shell.execute_reply.started":"2024-11-15T04:57:20.334516Z","shell.execute_reply":"2024-11-15T04:57:20.339270Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"49867"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"paragraphs[0:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:04:43.973698Z","iopub.execute_input":"2024-11-15T05:04:43.974392Z","iopub.status.idle":"2024-11-15T05:04:43.980683Z","shell.execute_reply.started":"2024-11-15T05:04:43.974351Z","shell.execute_reply":"2024-11-15T05:04:43.979764Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"[\"  Thank you. Good morning, everyone. So we had started to discuss text analysis or natural language processing. And we had looked at a number of examples, the kinds of things that we can do with text. And today we're going to start with actually some of the techniques that we use to convert documents to vectors. Now, hopefully, let's just recap very quickly what we have talked about up until now. So we have as input a set of documents. We are ignoring any hyperlinks now. We have talked about how we can model hyperlinks as graphs. And so if you ever had to analyze content that actually had hyperlinks as well, then you would actually have to combine content with what the graph-based algorithms were. So we're going to focus on the content itself, right? So we have a corpus. What we want to do is convert that corpus into a tabular representation. where we have a set of variables that we have extracted from this text. And we call this process vectorization. We talked about the fact that the columns here, the features that we extract here, the simplest method, which is known as a syntactic approach to vectorization, creates a vocabulary and then uses that vocabulary, each of the individual elements of this vocabulary as features. We talked about the fact that the basic vocabulary is all unique words that appear within the corpus. But then we also talked about key phrases like the United Nations, like the United States of America, like the Republic of India. Or in fact, we even used some examples like Durga Puja. These are all key phrases. And these need to be extracted from the text because we don't really know which sequence of words creates a key phrase. And in fact, key phrases are sometimes even dependent on what are these documents about.\",\n 'And then what we said was that actually some key phrases have a type. And we refer to those as named entities. where we had seen examples like the Bhartia Janta Party being a named entity. The Congress would be another example of a named entity. The Ahmadinejad Party, like the Bhartia Janta Party, like the Bhartia Janta Party would be a key phrase that has a type, therefore it would be a named entity. All three of these would be of type political party. And then the three examples that we gave are what we refer to as instances of this type. So the unique words and the key phrases create our vocabulary. And we can then assign each one of these elements of the vocabulary as columns. And then we can either put a one or a zero as the vector representation of a single document. Does this element of the vocabulary appear in this? Does it not appear? We also saw an alternative. And so in this case, this is a binary vector. We also looked at an alternative where we can put in the frequency with which words appear in the document. And in Python library vocabulary, we call that the count vectorizer. The vectorizer is what creates the vector. You basically have a count vector when you produce it through the count vectorizer. And then we also looked at the TFI-DF vectorizer. That balanced the frequency with which a word appeared in a document and the frequency with which it appeared across the corpus, across documents. So the more common the word that it appears in all documents within the corpus, the less excited we are about it as representing what an individual document is about. The higher the frequency of the word in the document, the more encouraged we are that this document is something to do with this element of the vocab. Right?']"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}