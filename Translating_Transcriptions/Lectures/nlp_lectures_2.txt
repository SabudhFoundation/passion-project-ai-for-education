Thank you. Good morning, everyone. So we had started to discuss text analysis or natural language processing. And we had looked at a number of examples, the kinds of things that we can do with text. And today we're going to start with actually some of the techniques that we use to convert documents to vectors. Now, hopefully, let's just recap very quickly what we have talked about up until now. So we have as input a set of documents. We are ignoring any hyperlinks now. We have talked about how we can model hyperlinks as graphs. And so if you ever had to analyze content that actually had hyperlinks as well, then you would actually have to combine content with what the graph-based algorithms were. So we're going to focus on the content itself, right? So we have a corpus. What we want to do is convert that corpus into a tabular representation. where we have a set of variables that we have extracted from this text. And we call this process vectorization. We talked about the fact that the columns here, the features that we extract here, the simplest method, which is known as a syntactic approach to vectorization, creates a vocabulary and then uses that vocabulary, each of the individual elements of this vocabulary as features. We talked about the fact that the basic vocabulary is all unique words that appear within the corpus. But then we also talked about key phrases like the United Nations, like the United States of America, like the Republic of India. Or in fact, we even used some examples like Durga Puja. These were all key phrases. And these need to be extracted from the text because we don't really know which sequence of words creates a key phrase. And in fact, key phrases are sometimes even dependent on what are these documents about. And then what we said was that actually some key phrases have a type. And we refer to those as named entities. where we had seen examples like the Bhartia Janta Party being a named entity. The Congress would be another example of a named entity. The Ahmad B Party, like the Bhartia Janta Party, would be a key phrase that has a type. Therefore, it would be a named entity. All three of these would be of type political party. And then the three examples that we gave are what we refer to as instances of this type. So, the unique words and the key phrases create our vocabulary. And we can then assign each one of these elements of the vocabulary as columns. and then we can either put a one or a zero as the vector representation of a single document. Does this element of the vocabulary appear in this? Does it not appear? We also saw an alternative. And so, in this case, this is a binary vector. We also looked at an alternative where we can put in the frequency with which words appear in the document. And in Python library vocabulary, we call that the count vectorizer. The vectorizer is what creates the vector. You basically have a count vector when you produce it through the count vectorizer. And then we also looked at the TFIDF vectorizer that balanced the frequency with which a word appeared in a document and the frequency with which it appeared across the corpus, across documents. So, the more common the word that it appears in all documents within the corpus, the less excited we are about it as representing what an individual document is about. The higher the frequency of the word in the document, the more encouraged we are that this document is something to do with this element of the vocab. Right? And we will look at this in a little more detail soon in any case, but these are the three syntactic vectorization approaches. And then what we said was the big problem with this, there are a number of problems, but the biggest problem is that it is vocabulary dependent. And so, what we really want is a representation of a document that is semantic in nature where these columns represent semantic elements. What is this document about conceptually? So, at a higher level of abstraction, we are wanting to have columns columns that can represent the semantics. What is this document about? And what we said there was if we can find topics where topics drive the vocabulary that is used. So, you could actually say the author of a document and the topic together drive the words or vocabulary used in a topic. And in fact, typically a document isn't related to a single topic. It is a mixture of topics. and so based on that mixture of topics, the vocabulary within this document is generated. And we will look at or we will model these topics as hidden variables. And another word for them is latent variables. okay, and so this is what we are, we have discussed to date, we haven't gotten into any depth, now we are going to go into depth. So, the basic idea here is that documents should be represented as vectors in what is referred to as the vector space model where every point in this space and of course I'm only drawing two dimensions or three dimensions but every point in this space is now a document. And as long as we can have all documents represented in this space as vectors, we can now calculate similarity between them. We can therefore apply a lot of the machine learning algorithms now on documents as we had done on structured data. So, this idea of a vector representation is really all about creating a structured representation of documents. If we think of named entities, we could also instead say, forget about the vocabulary that does not belong to the set of named entities. I am only interested in any political party that is mentioned, mentioned, any person that is mentioned, and so on and so forth. Right? And so, out here, if for a document, I have multiple political parties that I have mentioned, now what I can do is I can store them as an array, for example. Right? And similarly, people that I mentioned I could represent as an array. Now, of course, the array is not great for me because I need one value in each element, and so I could actually represent it by doing the equivalent of one-hot encoding. And so I would generate a much larger dimensionality where I would now have the BJP, the Congress, et cetera, et cetera, as columns, and I would now have a one in each of the ones that appear and zeros in the ones that don't appear. Right? So out here, what am I doing? If I limit myself to only using the named entities, I am ignoring all words that are not entities, I am restricting my vocabulary. I am doing dimensionality reduction and focusing on just the different types of data that are represented in the text. Now, which one is the better approach to use, ultimately, you are going to end up with this vector representation. All that's happening is that out of the complete vocabulary, which is key phrases and unique words, we are now coming up with a representation where we have limited it to be only those key phrases and words that are instances of some entities. things. Now, if there's too much loss of information by dropping the rest of the vocabulary, you're not going to get a very good model, right? But this is really, again, a feature engineering problem. So, for now, we will assume that we want to use all of the unique words and key phrases in our vocabulary, and that would be our vector space model for our documents. Now, one of the downsides of doing this representation, which is also known as the bag of words representation, is the fact that we ignore the ordering of words. So, the poor man ate the food and the man ate the poor food, have the same bag of words representation because we are ignoring the sequence in which the words are. Right? So, in a way, this is a simplification of the problem, where we are saying, well, the sequence in which the words appear are not important. All that's important is that the word appeared. And so, when we were talking about the Bayesian approach to text, and we said, you know, they were really representing only statistical properties. And one of the simplifying assumptions they made was that the sequence was not important. We look at why that was the case, but you can see that the vector space model was kind of influenced by that thought process, that, you know, the order of words is not important. So, then, essentially, from a pre-processing of text is concerned, the basic steps are we tokenize the text, which means we break it down to a word representation. Now, of course, you would have, for some of you who have played with JackGPT, you know that they also talk about the number of tokens. they don't talk about words, right? And part of the reason why they talk about tokens rather than words is that they have actually got a subword tokenization method, right? And so, while we have talked about taking words as the primitive object and finding key phrases and representing that as a vector representation, which is the traditional way of doing it, the more recent innovations have started to say, actually, we are better off looking at subword tokenization. So, breaking it by the way. Yes, Sula? In case of vector space model, not in case of tokenization, in case of vector space model, are we taking the plurals of words as separate words or using the plurals and the singulars are same? It depends on what you do, right? Here, the second step is stemming and lemmatization, right? And this typically removes not only plurals and singular and makes them one, but it actually goes to the root word, right? Like organize without the E replaces organizing, organizes, and organize. okay. Okay, so again, the reason for doing that is to reduce the dimensionality. The curse of dimensionality is a real problem when it comes to text, right? Because the number of unique words and key phrases often goes into millions. So, once we have done the tokenization, again, we may or may not want to remove the punctuation, because in certain applications, the punctuation may be important. Another thing that people say you should do is to convert every word to lower case. That may or may not be the best thing to do. When you're looking for named entities, for example, there are features that say, well, you know, if you have some capital letters, it's likely to be a noun, which could be a person or a location or whatever. But in general, what we would do is to minimize the vocabulary size after tokenizing and removing punctuation, we would also change the case to lowercase for all words. And then we would do stemming and lemmatization, two different approaches to mapping multiple words to their root form. And then we remove stop words, words like a, and, the, and, but. So there is a list of stop words. So this is just based on a lookup to say, is this word in the stop words? If it is, we throw it out and don't add it to our vector representation. Now, stemming and lemmatization. Lemmatization requires a thesaurus. Stemming algorithmically changes words. Right? So am will become be in lemmatization. and remains am from stemming. Going, remains going in stemming but becomes go in, sorry, become, going, remains going in lemmatization but becomes go in stemming. And having, becomes have in lemmatization but becomes H-A-V in stemming. Now, stemming typically can result in these kind of non-English words while identifying the root. And so it takes away a little bit from the understanding of what is the word that is important within a document once you've stemmed it. So just be aware of this but these are ways that were used in the past. and so it's less relevant in any case at this point to reduce the vocabulary size. And I'll tell you why it's become less relevant later on. Okay. Now, we talked about the syntactic representation and we've talked about TF-IDF. So once we have identified our vocabulary whether this is a word or a key phrase, we need to assign it a weight and that one way of assigning the weight is TF-IDF other than just looking at the count or a binary representation or one of zeros. So let's look at a very simple example. We have these eight documents here. We've actually just got the titles of these documents and we can see by looking at this, that the first few documents seem to be about human-machine interface. the last three documents here are about graph theory. Right? We studied how to analyze graphs. So these are related to that topic. Now, this is our corpus, all of it, and so we can convert it into a vectorized form in this way here. Now, what I've been talking about is that every word becomes a column. I have taken the transpose of that. And so it doesn't really matter. What we are now saying is that a document has a vector representation that looks like this. Now, to make it easier, I've taken a subset of the vocabulary. We are taking the vocabulary to only be words, not key phrases, and so we have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. A vocabulary of size 12 here, and you can see that we have count vectorized each of our documents. Now, if we look at the word human in document 1, if we were calculating the TFIDF, we would say, okay, human has a frequency of 1, there are 1, 2, 3, 4, 5, 6 words. This is a stock word. So, we have removed it. And I can only see the three words here, the rest of the words I have kind of cut off because I didn't have enough space to show it. So, 1 out of 6 is the frequency part of this. So, this is the TF, the term frequency. And here, 8 is the number of documents. and we can see human appears in document 1 and document 4. So, it's document frequency is 2. And so, the way we calculate TFIDF is term frequency inverse document frequency. So, 2 by 8 is the document frequency. We make it 8 by 2. and we get this value of 0.23. So, if we wanted to now instead represent our documents as vectors of TFIDF scores, we can calculate them. So, the first step really of calculating the TFIDF is to really do a count vectorization of the documents. Now, I'm going to skip this because we looked at this in principal component analysis. but what I'm going to move on to is what is singular value decomposition. Singular value decomposition is a linear algebra method and is based around eigenvalue decomposition. And this was the first semantic vectorization approach that was proposed. Now, singular value decomposition was the method used for doing semantic vectorization. When applied to textual documents, it was known as latent semantic analysis. singular value decomposition. So what was singular value decomposition, first of all? This matrix that I had, X, where we had word one, word two, till word M, and we had d1, d2, to dN. We are going to refer to this as X. And so when we go X transpose, X transpose is going to be an N by M matrix multiplied by an M by N matrix. So what are we going to end up with? We are going to end up with a N by N matrix. When we were doing principal component analysis, what we did was we said we have X1, X2, to XN, and we had various objects, O1, 2, O, M. And we first computed the covariance matrix. and the covariance matrix was an N by N matrix, which, given that we have these variables describing each object, it stored the variance along the diagonal of the matrix, and the covariance in the non-diagonal elements here. Right? So this here was the variance of Xi, and this, depending on where, which row and column it was, was the covariance of Xi and XJ. XJ. So here also we've got an N by N matrix. So what can we kind of visualize this as? We can visualize it that we are treating this matrix X as each word being an object that is represented by its occurrence within the N document. So the documents themselves are now our variables that in some way are describing our word. And so this X transpose X, if we do an eigenvector decomposition on it, which is essentially the same as what we do in principal component analysis, and we have our eigenvectors. Right? What that means is for every one of these eigenvectors, X transpose X times VI is equal to lambda I VI. This is by just the definition of an eigenvector and eigenvalue which is lambda I. Now let us define R vectors U1 to UR as this here. So we are taking the X matrix, which is this matrix here. We are multiplying by VI, which is the eigenvector of X transpose X. And we are multiplying by 1 divided by the square root of lambda I. it turns out that these UIs are also orthonormal. Why am I saying also? Because these eigenvectors are orthonormal. So these sets of vectors are orthonormal as well. So if I now say X and I essentially put B1 B2 to BR as columns within a matrix and I call this matrix V. Then what we have is X times V is equal to U lambda. Where what are U? U R, U 1, U 2, 2, U R. And lambda here is lambda 1, lambda 2 to lambda R. R. right? And it's the square root of the lambda. Sorry. This should be square root of that. So we've taken this lambda I and multiplied it on this side. now if we multiply both sides by the inverse of V we get X V V inverse is equal to U lambda V inverse. X. So this will become the identity matrix so we can ignore it. So this is just X. And because the vectors V are orthonormal V inverse is actually the same as V transpose. right? Because if we are looking at a vector a matrix V the inverse is something that produces the identity matrix. Now each element in the product of these two right? So V V inverse each element here any of these elements are essentially the dot product of a vector in V which is a row and a vector in V inverse which is a column. So if we take the transpose what are we going to end up with? This element here is going to be V1 dot V1 which is the length of the vector which is 1 because it's orthonormal and any non-diagonal element is going to be VI dot VJ and of course that is equal to 0 because they are appendicular to each other right? That's the orthogonal part of this. And so V V transpose is going to be the identity matrix. Oops apologies. And so the inverse of a matrix where the columns are perpendicular to each other is going to be the transpose right? And so what we are getting here is an interesting behavior that if we take the eigenvectors of X transpose X we stack them up into columns and take the transpose of that which is the equivalent of stacking them as rows and we define a new set of vectors as we have done here. what we are getting is actually a decomposition or matrix factorization. This is called matrix factorization where we have a matrix X that can be represented as a product of two or more matrices. Where have we heard factorization before? in school we were told factorize 12 and we would write 2 plus 2 2 times 2 times 3. These are the prime factors. Right? So we are doing the same here but we are doing it at a matrix level. Now this here like we said is a diagonal matrix with the diagonal elements being the square root of lambda i. Now if we look to calculate x x transpose now we know that x can be written as u lambda v transpose so we plug those in here and what do we get? We get u lambda square u transpose transpose right? Because what happens when we open this up it becomes u transpose lambda transpose v transpose of transpose which is v. Oops the wrong way. It becomes v lambda u transpose that's this part here. We multiply it with u lambda v transpose v transpose v is the identity matrix so it disappears and we end up with u lambda square u transpose the square of a diagonal matrix is really the square of the diagonal elements and so you end up with essentially this representation here which is x x transpose is equal to u the matrix with all lambda 1 to lambda r u transpose and if we now take this to this side by multiplying by u we are basically going to get x x transpose u is equal to 2 with lambda 1 lambda n which is actually what we were seeing here so while the vi's are the eigenvectors of x transpose x these u is are the eigenvectors of x x transpose and both have the same eigenvalues now let's look at what x x transpose is x x transpose x is m by n n transpose would be n by m and so this would be an m by m matrix so what we are doing here is just as we said that x transpose x was like calculating the covariance matrix where we treated our documents as our variables out here x x transpose on the other hand is the covariance matrix for the representation of documents as words right there are m words in our vocabulary here we are representing this as an m by n matrix so we are treating every word as a variable instead right and so in a way what we are doing here when we do singular value decomposition and do a matrix factorization of x into a product of three matrices what we end up with is a parallel eigenvalue decomposition of x transpose x and x x transpose so we have the eigenvectors and remember what we did here right when we did a principal component analysis and we chose the k eigenvectors what did we end up with we ended up with an r dimensional representation of each of our objects so what has happened here is that we have taken r x transpose sorry x x transpose where we are saying our words are our variables and as a result we have got an r dimensional representation so the u is an r dimensional representation of documents and our v is our r dimensional representation of words right these were our variables therefore we got an r dimensional representation for words in the other case we were treating each of these as our observations and treating words as our variables so we got a r dimensional representation of documents right so we get not just a representation of objects in r dimensional space as we get with principal component analysis with singular value decomposition applied to a vectorization of documents where the matrix is represented as shown here we end up with an r dimensional representation for words and an r dimensional representation of documents and that is what is referred to as latent semantic analysis right so let's look at what happens here we have our x matrix which is this one here documents as columns and if we look at sorry let's yeah okay so we are saying x is equal to mu sv transpose x here was something by 8 how many words were there 12 words 12 times 8 this now is going to be 12 times r and becomes our word representation in r dimensional space r s here becomes r by r and r v transpose will be r by 8 which means v itself is going to be 8 by r right and there were 8 documents and there were 12 words so we have words as represented in r dimensional space and we have documents represented in r dimensional space now just as we did in principal component analysis these diagonal elements are representing the variance but remember that the variance is the eigenvalues this matrix here is the square root of the eigenvalues right and so these are referred to as singular values and just as we had a descending order to the eigenvalues after we did principal component analysis we have a descending order here the larger the singular value the more of the variance is being captured but the variance itself being captured is the square of these right so in this case if I want to just visualize my words and documents I will choose the two top eigenvalues as a result of choosing this as a two by two so I'm saying r is two therefore I must look at each word now as being two dimensional with these coordinates and every document which is a column is being represented as a two dimensional representation so now of course I can visualize those like this so each dot here is a word and each dot here is a document and what we can see is there are these three documents that are close to each other these are quite separated from these documents and then you have this one document that's sitting out here which document is sitting right out here it is document number five zero one two three four five I think I messed up here I've added a new document but essentially if we look at what's happening here not sure why I have nine documents there I'll have to look at it maybe I by mistake incorporated this but if we look at these documents the last three documents were supposed to be graph related and what we're saying is we have one two three four at least these documents potentially are the documents that are also close to each other and this one may be the one that is a little further away from I'll have to come back to you right but here we can see that these are all graph theory these are all human computer interaction and maybe this here is a little more on response time rather than human computer interaction so that's why it is separated we also see that some words here are coming out as similar to each other these are words that have their second axis okay so this word here and this word here are close to each other so what were those words graph and my trees and miners right and so trees graphs and miners these are definitely to do with graph theory that's why they are appearing close to each other on the other hand if we look at human and user we would expect these to be close to each other right a user of a computer is as of now at least a human so that's the first and fourth the first and second third fourth it's not coming up that close right but what we would expect is that the coordinates of these would come close now of course take into account that these are this is a very small corpus that we are using look at this these two are very similar exactly the same so what are these one two three four five six seven one two three four five six seven response and time are identical in their representation right so we can see that something interesting is happening here now when we get this twelve by two matrix here multiplied by a two by two matrix multiplied by a two by in this case nine but should be eight matrix by multiplying just these red portions we get a reconstruction of our x so this is what our x looked like where we can see that clearly the vocabulary that was being used in these documents were different from the vocabulary being used in these documents right but the sparseness here the zeros here made it look like maybe these weren't as similar to each other but when we reconstruct x by taking u s v transpose we are now getting not a sparse matrix but a dense matrix and words that previously had the value zero in here now don't have a zero value because even though they didn't mention the value the word system they did mention computer and computer and system are really synonyms in the context of what we are talking and so we are now filling in these missing values with numbers other than zero if there are other words in that document that really are suggesting that instead of using the word system we could have used computer right and we are seeing that a lot of this area now has larger positive values across the board and similarly here we have larger positive values across the board compared to what we have in this one right so this Shashi Tharoor versus my lecture problem to some extent is being resolved by latent semantic analysis but critics of this method said what do you mean by a negative value by looking at a positive value we can say yeah okay so you know this word has some importance in the second document but when we have a negative value what does that really mean does it mean that this word appearing takes away from the meaning of this document or what is that negative value representing right so this was the what was represented as a rebuttal by the Bayesians who then went on to propose a probabilistic latent semantic analysis where essentially you ended up with probabilities in here and so when you looked at the coordinates out here where how do we interpret this these are called topic one and topic two and what was being said here is that word one has a membership of this topic of point two two so this word here is much stronger in its connection to topic one compared to this word here and this word here was much more related to topic two compared to this word and we can see again right that these last three words are associated with topic two the first bunch of words are much more related to topic one and this word here seems to have an equal representation across those topics and what was that word survey because survey appeared once here in a document that had more of this vocabulary and also appeared here once in a set of documents that actually had this vocabulary right so the jury is out or maybe this word transcends both topics and that is what is represented out here right so you can see that what we are getting is a partitioning of the vocabulary out here but the criticism was y minus what does this minus represent and you can see how if you instead had probabilities where word one was looking like this word two and so on and then the latter words word ten word eleven and word twelve word nine word nine looked more like four and 0.6 whereas these were more 0.8 0.9 0.9 two the interpretation is so much easier right what we are saying is that if a topic topic one is what is generating a word or if we have seen a word in a document the likelihood is that topic one generated this word much more than the likelihood of topic two generated it's four times more likely that topic one would have generated this word and so when we look across all the words that appear in a document we can then come up with a document representation saying that the document has maybe 60% of its vocabulary from topic one and 40% of its vocabulary from topic two and that would become the document representation here again what we can see is that these are related to topic one and these are related more to topic two right so the partitioning is happening of documents but what these actually mean especially the negative values became a question that was asked so let's take a break from the theory of it oh actually maybe not I was going to go into some text but let me now do this next topic before I get on to some of the code so what we have seen is three methods for syntactic vectorization representation and as of now we have seen one method for semantic representation or semantic vectorization where each document now is represented in this two-dimensional topic space either way what we now want to do is apply this to something and when we started this topic we had talked about text categorization where we have documents that have a particular class associated is this a sports document is this a politics document is this or whatever so now that we have been able to vectorize documents this essentially becomes a standard machine learning problem but before we even vectorize the documents or maybe we could look at this as justifying why we are doing what we are doing in the vectorization especially the syntactic vectorization we talked about the vector space model for documents let's look at some probabilistic models for documents instead so the idea here is we are talking about a probabilistic model therefore we must be talking about generative models and what we are saying is that we have a corpus a corpus consists of a number of documents and each document is generated from a mixture model parameterized by some parameter vector vector now we've come across mixture models before when we were looking at the EM algorithm for clustering and essentially a mixture model is a probability distribution defined by a linear combination of individual probability distribution also known as components and when we were studying the EM algorithm we had assumed that these components are what they are Gaussian right and each Gaussian had its own parameters and there was another parameter which was the probability of a component being called upon to generate data so similarly we can say that a document DI is generated by a number of components and each of those components has a probability of being asked to generate a part of this document so when we think of a document we are really saying that a document is a bag of words why are we using the word bag we are saying it's a bag not a set because a bag can have multiple copies of the same word a set has only unique elements right so what we are saying is that this document has words in it and those words are generated by calling upon these components so the question is what kind of probability distribution are these components and there are two models probabilistic models of documents one is the multivariate Bern-Lowley and the second is the multinomial distribution model so we are going to look at both of those the multivariate Bern-Lowley model what does that tell you there are two really important words here multivariate what is multivariate jay kishan any idea what it is multivariate we say multivariate statistics also no sir okay sumit any idea what multivariate stands for this word multiple variables it's as simple as that multivariate means multiple variables so we are saying we have x1 x2 x3 till xn what are these x1 to xn in our case because we are looking at documents these are elements of our vocabulary so they could be words they could be key words key phrases and of course we understand in that they either occur or don't occur right so this is just a binary variable random variable so this and so the model of generating a document which is multivariate is that we are saying that a document you can think of a document consisting of n slots one for every word or key phrase can you hear me can you hear me yes sir you are audible okay thank you so essentially we are saying this document has n slots one slot for every word or key phrase and what we want to decide is does this appear or not appear in the document so this is the equivalent of our binary vectorization what are the parameters of this model for each element we have a separate Bernoulli distribution the probability of x one being a one is going to be some probability theta one the probability of x two equal to one is going to have a different problem right and so what we are saying here is that for each one of these words we have its own parameter theta n so there are n parameters here that we need to learn but why are we saying components think of components as topics or actually let's not even confuse it with topics right now think of each of these as classes we are talking about the corpus as a whole which is a set of documents being generated from a mixture model so whenever we are generating a document we first decide which component are we going to call which class are we going to generate a document and depending on the class we will generate the words that appear and to generate the words that appear we need n parameters so the total number of parameters that we need here is if we have k classes and we have n parameters for each class we have n times k parameters but each of the classes themselves also have a probability of being asked to be generated and so we have the probability of c1 is equal to theta c1 probability of c2 is equal to theta c2 till the probability of ck which is equal to 1 minus the sum of probability of ck minus cj minus c1 where j goes from 1 to k actually 1 minus sigma of theta cj where j goes from 1 to k minus 1 so there are k minus 1 more parameters than we need so the total number of parameters we would need is this so that is the multivariate Bernoulli model what about the multinomial distribution the multinomial distribution is where we have a categorical variable and so what we are doing here is we are saying for every element of the vocabulary we call them v1 v2 till vm we have a probability of it being chosen as a word or key phrase in the document so we have a document we now decide on a length of the document and so if we say the length is k let's not call it k because we've already let's call it capital n we have capital n slots that need to be filled again we choose the class to which the document belongs and then we choose from the probability of words given the class that has been chosen and what is that that is this categorical distribution and what we are saying now is we are choosing from all of our vocabulary based on these probabilities we are choosing words at random and filling our slots with those words so what are we going to end up with we are going to have n selections from m possible values which are each of the elements of a vocabulary so what is the probability of a document going to be given that it has been generated from a particular class when we see a document it is of length n and there are m values here from which we can choose these n what we are going to end up with is essentially this here this capital n is the number of trials and these are the occurrences the number of times each of our words are occurring so what we had said in terms of our notation here is we have n trials we are going to say n divided by x1 factorial x2 factorial till xm factorial where each of these x1 x2 and xm are the frequency with which each of these elements appear in our n trials that we have so this is the number of ways in which n slots can be filled with these frequency of m vocabulary words and we then multiply by the probabilities right so p1 to the power of x1 times p2 to the power of x2 all the way to pm to the power of x where p1 to pm are these probabilities right so that's what we are representing okay so the multinomial distribution model of a document is different from a multivariate bernoulli model where we are saying that the words that appear in the document they can have a frequency associated with them they fill up n slots and therefore we have to choose the length of a document also but once we've chosen the length of the document we are selecting what to fill these documents by a probability distribution okay and that's the multinomial distribution model now given these two models we end up with if we want to do a classification model we end up with two versions of the naive ways algorithm remember that the naive base algorithm made a simplifying assumption if we had n input variables and we had the class c and we wanted to know what's the probability of a class given a set of input values evidence e we can now represent it as the product of the probability of each x i given c where i goes from 1 to n multiplied by the probability of c divided by the probability of this is what we had done this was the naive base model now why do we have two different versions of the naive base algorithm depending on whether this is a binary vector or whether this is a count vector a count vector is really required for the multi-movem document model why because we need to know the number of times n i t is the number of times the t th element of the vocabulary appears in the document t i and that is what is represented within the count vector is the number of times that word appears and that's why we need this count vector when we want to calculate the probability of a document given a class when we want to use instead the multivariate Bernoulli model this d i t is either 1 or 0 where 1 is when w t appears in d i and 0 when w t does not appear in d i right and so whenever we are using the multivariate Bernoulli model the value here how we calculate this essentially changes out here in the multivariate Bernoulli we can see we are taking the product over the vocabulary and we are using the Bernoulli distributions formula right p of x to the p of theta to the power of whatever say here the p to the power of x times 1 minus p to the power of 1 minus x is the probability of x or we can even write this as theta we are doing the same thing when we are wanting to use the multinomial distribution this has to be calculated differently and how is it calculated differently it's basically going to use this formula here which is what we have as the formula for the multinomial distribution but we are also multiplying by the probability probability of the length of the document and this probability of the length of the document we need to figure out what is the distribution going to be for this right so that is what changes in the naive base depending on what kind of vectorizer we are using here when we use a binary vectorizer it works pretty much the same way as the standard naive base when we choose a count vectorizer this part changes and we need to use instead this distribution which is the probability of bi given cj okay I know this has got a little heavy I'm aware of that so let's leave it here for now and let me go to some code and I'll be sharing this in your educo lab as well so here there are going to be two ipython notebooks that I share one is the text vectorizer and the other is toxic comment classification here we are using in sklearn a standard data set which consists of news groups 20 news groups so in the good old days people would have news groups where they would email into this news group email address and then all of the messages associated with a particular topic would get collated and would get shared with other people that were interested in that topic this was in the good old days when the internet was not as mature as it is today we didn't have social media so essentially this data set consists of a number of messages and we can see at the structure of the data set we have essentially a dictionary and one of the elements of the dictionary is data and that has as its value a list and in that list you basically have a set of messages this is one message and in that message you have who sent it what was the subject and where did they post it and so on and then it has some text so really what we want to do is we want to use this text we want to ignore these email addresses and all that kind of stuff we want to really use this text to be able to automatically classify it into whether it is about the Christian religion or whether it's about hockey or the Middle East or motorcycles so if you look at this the labels the class labels that we are hoping to use here are actually a taxonomy now what is a taxonomy a taxonomy is a tree structure so when Yahoo was the main search engine for the internet they had two ways in which you could navigate and find content one was they provided or created a taxonomy for the internet where you had everything here I can't remember the exact word but this was basically the root node and then you had different child nodes now in our case here we have social recreation as two of the child nodes that I can see here within social we have further splits and I can see one of them is religion and then within religion we have Christian I presume we will also have other nodes here like Islam or maybe Hindu Sikh and so on right and here in recreation we have sports and within sports we have hockey and again we would expect it to have cricket and various other things too we also have talk politics right so out here this would be talk this would be politics and then we have middle east and again we would expect other subtopics as well right so the dot here this is just a way of converting your taxonomy into a string where you use a dot as a separator to separate these out and basically what Yahoo did was they would have a bunch of documents hanging off not just the leaf nodes but you could also have documents hanging off here where they didn't know quite exactly how to split it further into its sub categories and so the idea here is that we want to classify every one of these messages into a leaf node and there are 20 such leaf nodes that we have provided and here they are right so alt dot atheism computer dot graphics operating systems and so on right so the first thing that we want to do here is get the data into a shape that we are interested in if we look at the data that we have been provided the data consists of five different keys they are dictionaries and their keys are data file names target names target and description we are only interested in actually the content and the target name the target is just one to one mapping of a string onto an integer from what I can take up right so we brought that in out here also we don't really and so the first thing we do typically like we said is we will lemmatize the data we will clean up some issues in the data like remove email addresses and stuff like this remove new line characters single quotes right so punctuation so that's happening here and then out here is where the lemmatization is happening using another library that you will now have to start to explore called SPACY and SPACY is one of the leading natural language processing libraries that is there so getting familiarity with that is going to be important now when we lemmatize the results of the lemmatization depend on another piece of information which are called POS tags right so the these POS tags are assigned to individual words POS stands for part of speech and so if we look up part of speech you will find part of speech essentially you are assigning tags to every word tags like nouns adjectives verb adverb right and knowing the POS tag improves the quality of lemmatization that is done to it right because you wouldn't want to lemmatize somebody's name for example even though it looks like a prime template for doing so so what we are doing here is we are loading a model a pre trained model in SPACY if we look at this EN stands for English so this is a model specifically for the English language and SM stands for small that is large models also and medium sized models that you can pull up and typically when we load this model this model has all sorts of processing capabilities and we are disabling named entity recognition and entity parsing or sentence parsing in here all we are interested in doing at this point is tokenization and post tags identification so when we run this what is happening here is we are getting our data from our data frame here and we are choosing just the values in the content column we are converting it into a list we are removing emails we are removing new line characters removing certain punctuation we are loading this model the NLP model and we are oh in fact before we load that NLP model we are executing on this here where we are removing punctuation and converting each element in our list which is one of these posts done to our groups and converting it into a list of words and we are then calling the lemmatization piece here that is using the spacey model and looking at only lemmatization of these postiles right so it's taking each token and lemmatizing it and providing that in our texts out which is another list which is being stored in data lemmatized and that is being returned so when we call our get lemmatized clean data passing the data frame here what we end up getting back is this list of words that appear within that message that has been lemmatized punctuation has been removed but we can see we still have some issues here right we have an apostrophe s out here so really we should be doing some further cleaning but this is the basic cleaning that we are doing here now at this point I'm going to take the data lemmatized and that is really the training data that we have and I'm passing it through in sklearn we have the feature extraction dot text count vector and I'm saying I want to tokenize based on words I only want words in my vocabulary that have a minimum of 10 documents that they appear in I want to use a list of stop words that is already embedded within this function this method they're the English stop words that I want to remove I want to make all of my words lower case and I only want to have in my vocabulary words that have greater than or equal to three characters within it and I do a fit transform on it and I end up with this fit transform creating a vocabulary where it's matched every unique token every word onto your unique ID and essentially what I'm seeing here is that the vector that we generate for every document the 7083rd element in that vector is going to be representing the word thing and the frequency within it because I'm calling the count things right so that's all of my vocabulary here of which we've seen we have a few thousand words we can see the vocabulary is actually 7846 right now when I give you this this ipython notebook you can play around with some of the parameters reduce the minimum df for example from 10 to a lower value what that's going to do is increase the vocabulary if you change the token pattern to be not three characters minimum but two characters minimum again you're going to increase the size of the vocabulary right and we can see that we have in our data vectors that have been returned 11,314 vectors and data vectorized is a sparse matrix right so it's saying that this is a sparse matrix the dimensionality of that sparse matrix is 11,314 by 7846 so every row is a document and every word in our vocabulary is a column in here and it says that out of all of these elements there are just 729489 elements that have a non-zero value so it's a sparse matrix why do are there so many zeros because every document only contains a small subset of words compared to the vocabulary size imagine we are looking at a vocabulary size of only 7800 words right now imagine when that's a million words or 2 million words how sparse that matrix is going to be right and this matrix is in what's called compressed sparse row format what I would highly recommend is you look at scipy and the different formats in which sparse matrices are represented and why one representation is better than another depending on what kind of operations you want to do on it right so I realize we are out of time so I'll stop here but we'll continue from here in the next lecture tomorrow and I'll share this so that you can go through it in your own time just after this lecture all right thanks for sticking there