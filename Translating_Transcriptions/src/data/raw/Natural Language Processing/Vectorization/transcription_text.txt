Thank you.Good morning, everyone.So we had started to discuss text analysis for natural language processing.and we have looked at a number of examples,  the kinds of things that we can do with text.And today we're gonna start with actually  some of the techniques that we useto convert documents to vectors.Now, hopefully, let's just recap very quickly.  what we have talked about up until now.We have as input a set of documents.  We are ignoring any hyperlinks now.We have talked about how we can model hyperlinks as graphs.And so if you ever had to analyze content  that actually had hyperlinks as well,then you would actually have to combine content  with what the graph-based algorithms were.So we're going to focus on the content itself.  So we have a corpus.What we want to do is convert that corpusinto a tabular representation wherewhere we have a set of variables that we have extracted from this text.And we call this process vectorization.We talked about the fact that the columns here, the features that we extract here,the simplest method, which is known as a syntactic approach to vectorization,creates a vocabularyand then uses that vocabulary, each of the individual elements of this vocabulary, as features.we talked about the fact that  the basic vocabulary is all unique words  that appear withinthe corpus  but then we also talked about key phraseslike the United Nations  like  the United States of Americalike the Republic of India  or in fact we even used some  examples likeDurga Puja. These were all key phrases  and these needs to be extracted from the textbecause we don't really know which sequence of words  creates a key phrase and in fact key phrasesare sometimes even dependent on what are these documents about.And then what we said was that actually some key phrases have a type,and we refer to those as named entities.Where we had seen examples like the Bharti-Ajantra Party being a named entity.the Congress would be another example  of a named entity.  The Ahmadinejad Partylike the Bhartia Jinta Party would be a key phrase  that has a type therefore it would be  a named entity.All three of these  would be of type  political party.And then the three examples  that we gave are what we refer to as instances of this type.So the unique words and the key phrases create our vocabulary, and we can then assign each one ofthese elements of the vocabulary as columns.  And then we can either put a 1 or a 0as the vector representation of a single document.Does this element of the vocabulary appear in this?  Does it not appear?  We also saw an alternative.And so in this case, this is a binary vector.We also looked at an alternative where we can put in the frequency with which words appear in the document.And in Python library vocabulary, we call that the count vectorizer.The vectorizer is what creates the vector.  You basically have a count vector when you produce it  through the count vectorizer.And then we also looked at the TFIDF vectorizerthat balanced the frequency with which a word appeared in a documentand the frequency with which it appeared across the corpus, across documents.So the more common the word that it appears in all documents within the corpus,the less excited we are about it as representing what an individual document is about.The higher the frequency of the word in the document, the more encouraged we are that thisdocument is something to do with this element of the vocabulary. Right? And we will look at thisin a little more detail soon in any case, but these are the three syntactic vectorization approaches.And then what we said was the big problem with this, there are a number of problems, but the biggest problem is that it is vocabulary dependent.And so what we really want is a representation of a document that is semantic in nature.Where these columns represent semantic elements.what is this document about conceptually?  So at a higher level of abstraction,we are wanting to have columns  that can represent the semantics.What is this document about?  And what we said there was,  if we can find topicswhere topics  drive the vocabulary that is used.So you could actually say  the author of a document  and the topic togetherdrive the words or vocabulary  used in a topic.And in fact, typically a document isn't related to a single topic.  It is a mixture of topics.And so based on that mixture of topics, the vocabulary within this document is generated.and we will look at or we will model these topics as hiddenvariables. And another word for them is latent.Okay. And so this is what we are, we have discussed to date. We haven't gotten into any  depth, now we're going to go into depth.So the basic idea here is that documents should be representedas vectors in what is referred to as the vector space model,where every point in this space, and of course,  I'm only drawing two dimensions or three dimensions,but every point in this space is now a document.and as long as we can have all documents represented in this space as vectors we cannow calculate similarity between them we can therefore apply a lot of the machine learningalgorithms now on documents as we had done on structured data so this idea of a vectorrepresentation is really all about creating a structured representation of documents.If we think of named entities, we could also instead say forget about the vocabulary thatdoes not belong to the set of named entities. I am only interested in any political party that ismentioned, any person that is mentioned, and so on and so forth.  Right?And so out here, if for a document I have multiple political parties that I've mentioned,now what I can do is I can store them as an array, for example.  Right?And similarly, people that I've mentioned, I could represent as an array.Now, of course, the array is not great for me because I need one value in each element.And so I could actually represent it by doing the equivalent of one hot encoding.  coding.And so I would generate a much larger dimensionalitywhere I would now have the BJP, the Congress, et cetera,  et cetera, as columns.And I would now have a 1 in each of the ones that appear  and 0s in the ones that don't appear.  Right?So out here, what am I doing?  If I limit myself to only using the named entities,I am ignoring all words that are not entities.  I am restricting my vocabulary.I am doing dimensionality reductionand focusing on just the different types of data that are represented in the text.Now, which one is the better approach to use? Ultimately, you're going to end up with thisvector representation. All that's happening is that out of the complete vocabulary,which is key phrases and unique words we are now coming up with a representation where wehave limited it to be only those key phrases and words that are instances of some entitiesnow if there's too much of loss of information by dropping  the rest of the vocabulary you're not going to get a very good model right but this is reallyagain, a feature engineering problem.So for now, we will assume that we  want to use all of the unique words and key phrases  in our vocabulary.And that would be our vector space model for our documents.Now, one of the downsides of doing this representation, which is also known as the bag of words representation, is the fact that we ignore the ordering of words.So, the poor man ate the food and the man ate the poor food have the same bag of words representation because we are ignoring the sequence in which the words are.Right?So, in a way, this is a simplification of the problem where we are saying, well, the sequence in which the words appear are not important.All that's important is that the word appeared.And so when we were talking about the Bayesian approach to text and we said, you know, they were really representing only statistical properties.And one of the simplifying assumptions they made was that the sequence was not important.We look at why that was the case, but you can see that the vector space model was kind of influenced by that thought process, that the order of words is not.So then, essentially, from a pre-processing of text is concerned, the basic steps are we tokenize the text,which means we break it down to a word representation.  Now, of course, you would have, for some of you who have played with JackGPT,you know that they also talk about the number of tokens.  They don't talk about words, right?And part of the reason why they talk about tokens rather than wordsis that they have actually got a subword tokenization method, right?And so while we have talked about taking words as the primitive object and finding key phrases and representing that as a vector representation, which is the traditional way of doing it, the more recent innovations have started to say, actually, we are better off looking at subword tokenization.So breaking it by the way.  Yes, Suman.In case of vector space model, not in case of tokenization,  in case of vector space model,are we taking the plurals of words as separate words  or using the plurals and the singulars as same?It depends on what you do, right?  Here, the second step is stemming and lemmatization, right?And this typically removes not only plurals and singularand k makes them one, but it actually goes to the root word, right? Like organize without the ereplaces organizing, organizes and organize. Okay. Okay. So again, the reason for doing that isto reduce the dimensionality. The curse of dimensionality is a real problem when it comesto text, right? Because the number of unique words and key phrases often goes into millions.so once we have done the tokenization again we may or may not want to remove the punctuationbecause in certain applications the punctuation may be important another thing that people say  you should do is toconvert every word to lowercase that may or may not be the best thing to doWhen you're looking for named entities, for example, there are features that say, well, you know, if you have some capital letters, it's likely to be a noun, which could be a person or a, you know, a location or a temp.but in general  what we would do is to minimize  the vocabulary size  after tokenizing and removing  punctuationwe would also  change the  case to lowercase  for all words and thenwe would do stemming and  lemmatization, two different approaches  to mapping  multiple words  to their root formand then we remove stop words, words like a, and, the, and, but.  So there is a list of stop words.So this is just based on a lookup to say, is this word in the stop words?  If it is, we throw it out and don't add it to our vector representation.Now, stemming and lemmatization.  lemmatization requires a thesaurusstemming algorithmically  changes words right so amwill become b in lemmatization  and remains am from stemming goingremains going and stemming but becomes go in  sorry, remains going in lemmatizationbut becomes go in stemming.  And having becomes have in lemmatizationbut becomes H-A-V in stemming.  Now stemming typically can resultin these kind of non-English words  while identifying the root.  And so it takes away a little bitfrom the understanding of  what is the word that is important within a documentonce you've stemmed it.  So just be aware of this,  but these are ways that were used in the past,and so it's less relevant in any case at this point  to reduce the vocabulary size.And I'll tell you why it's become less relevant later on.  Okay.  OK.Now, we talked about the syntactic representation  and we've talked about TF-IDF.  So once we have identified our vocabulary,whether this is a word or a key phrase,  we need to assign it a weight.  And that one way of assigning the weightis TF-IDF, other than just looking at the count  or a binary representation or one or zero.So let's look at a very simple example.  We have these eight documents here.  We've actually just got the titles of these documents.  And we can see by looking at thisthat the first few documents seem to be about  computer human machine interface.The last three documents here are about graph theory.We studied how to analyze graphs.  So these are related to that topic.Now, this is our corpus, all of it.  and so we can convert it into a vectorized form in this way here.Now, what I've been talking about is that every word becomes a column.  I have taken the transpose of that.  And so it doesn't really matter.What we are now saying is that a document has a vector representation that looks like this.Now to make it easier, I've taken a subset of the vocabulary.  We are taking the vocabulary to only be words, not key phrases.And so we have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12.  A vocabulary of size 12 here.And you can see that we have count vectorized each of our documents.Now, if we look at the word human in document 1, if we were calculating the TFIDF, we would say, okay, human has a frequency of 1.There are 1, 2, 3, 4, 5, 6 words.  This is a stock word.so we have removed it  and I can only see the three words herethe rest of the words I have kind of cut off because I didn't have enough space  to show it. So 1 out of 6is the frequency part of this. So this is the TF  the term frequency and here8 is the number of documentsand we can see human appears in document 1 and document 4.So its document frequency is 2.And so the way we calculate DFIDF is term frequency  inverse document frequency.So 2 by 8 is the document frequency.  We make it 8 by 2.  and we get this value of 0.23.So if we wanted to now instead represent our documents as vectors of TF-IDF scores,we can calculate them.  So the first step, really, of calculating the TF-IDF  is to really do a counter-vectorization of the documents.Now, I'm going to skip this because we looked at thisin principal component analysis, but what I'm going to move on to is what is singular valuedecomposition. Singular value decomposition is a linear algebra methodand is based around eigenvalue decomposition.  And this was the first semantic vectorization approachthat was proposed.  Now, singular value decompositionwas the method used for doing semantic vectorization.  When applied to textual documents,it was known as latent semantic analysis.So what was singular value decomposition, first of all?This matrix that I had, x, where we had word 1, word 2,to word m, and we had d1, d2, to dn.we are going to refer to this as x.  And so when we go x transpose,x transpose is going to be an n by m matrix  multiplied by an m by n matrix.So what are we going to end up with?  We're going to end up with a n by n matrix.When we were doing principal component analysis,what we did was we said we have x1, x2 to xn,and we had various objects, 1, 2, 0, m.and we first computed the covariance matrix.And the covariance matrix was an n by n matrix,which, given that we have these variables describing each object,it stored the variance along the diagonal of the matrix and the covariancein the non-diagonal elements here. Right? So this here was the variance of Xiand this, depending on which row and column it was,was the covariance of xi and xj.  So here also we've got an n by n matrix.So what can we kind of visualize this as?We can visualize it that we are treating this matrix xas each word being an objectthat is represented by its occurrence  within the end document.So the documents themselves  are now our variablesthat in some way are describing our word.And so this x transpose x, if we do an eigenvector  decomposition on it, which is essentiallythe same as what we do in principal component analysis,and we have r eigenvectors, right?What that means is for every one of these eigenvectors,x transpose x times vi is equal to lambda i vi. This is by just the definition ofan eigenvector and eigenvalue which is lambda i.Now, let us define r vectors u1 to ur as this here.So we are taking the x matrix, which is this matrix here.We are multiplying by vi, which is the eigenvector of x transpose x.and we are multiplying by 1 divided by the square root of lambda i.It turns out that these UIs are also orthonormal. Why am I saying also? Because these eigenvectors are orthonormal. So, these sets of vectors are orthonormal as well.So if I now say x and I essentially put v1, v2,to vr as columns within a matrix, and I call this matrix v,then what we have is x times v is equal to u lambda.Where what are u? u r, u 1, u 2, u r.And lambda here is lambda 1, lambda 2,to lambda r.Right?  And it's the square root of the lambda.  Sorry.  This should be square root of that.  So we've taken this lambda i  and multiplied it on this side.now if we multiply both sides by the inverse of v we get x v v inverse is equal tou lambda v inverseso this will become the identity matrix so we can ignore it so this is just xand because the vectors V are orthonormal,V inverse is actually the same as V transpose.  Right?Because if we are looking at a vector, a matrix V,the inverse is something that produces the identity matrix.Now each element in the product of these two, right?  So V, V inverse, each element here,any of these elements are essentially the dot product  of a vector in V, which is a row,and a vector in V inverse which is a coliform.  So if we take the transpose,what are we going to end up with?  This element here is going to be V1 dot V1which is the length of the vector which is 1  because it's orthonormal.And any non-diagonal element is going to be V1 dot Vj.And, of course, that is equal to 0 because they are appendicular to each other.  That's the orthogonal part of this.And so V V transpose is going to be the identity matrix.  Oops, apologies.And so the inverse of a matrix where the columns areperpendicular to each other is going to be the transpose.And so what we are getting here is an interesting behavior.That if we take the eigenvectors of x transpose x,We stack them up into columns and take the transpose of that,which is the equivalent of stacking them as rows.And we define a new set of vectors, as we have done here.What we are getting is actually a decomposition,  or matrix factorization.This is called matrix factorization, where we have a matrix X that can be represented as a product of two or more matrices.Where have we heard factorization before?In school, we were told factorize 12, and we would write 2 plus 2, 2 times 2 times 3.These are the prime factors, right?So we are doing the same here, but we are doing it at a matrix level.Now, this here, like we said, is a diagonal matrix with the diagonal elements being thesquare root of lambda i. Now, if we look to calculate x, x transpose now, we know that xcan be written as u lambda v transpose.  So we plug those in here.  And what do we get?We get u lambda squared u transpose.Because what happens when we open this up,it becomes u transpose lambda transpose v transpose oftranspose, which is V. Oops, the wrong way. It becomes V lambda U transpose. That's this parthere. We multiply it with U lambda V transpose. V transpose V is the identity matrix. So itdisappears and we end up with u lambda squared u transpose.The square of a diagonal matrix is really the square of the diagonal elements, and so youend up with essentially this representation here, which is x x transpose is equal to uthe matrix with all lambda 1 to lambda r u transpose.And if we now take this to this side by multiplying by u,we are basically going to get x x transpose u is equal to 2 with lambda 1.lambda n, which is actually what we were seeing here.So while the vi's are the eigenvectors of x transpose x, these ui's are the eigenvectorsof x x transpose and both have the same eigenvalues. Now let's look at what x x transpose is.x x transpose x is m by n n transpose would be n by m and so this would be an m by m matrix.So what we are doing here is just as we said that X transpose X was like calculating the covariancematrix where we treated our documents as our variables. Out here, XX transpose,O's, on the other hand, is the covariance matrixfor the representation of documents as words.  There are m words in our vocabulary here.We are representing this as an m by m matrix.  So we are treating every word as a variable instead.And so, in a way, what we are doing here, when we do singular value decomposition and do a matrix factorization of x into a product of three matrices, what we end up with is a parallel eigenvalue decomposition.of x transpose x and x x transpose.  So we have the eigenvectors.And remember what we did here, right?  When we did a principal component analysis  and we chose the k eigenvectors, what did we end up with?we ended up with an R dimensional representation of each of our objects.So what has happened here is that we have taken R X transpose, sorry, X X transposewhere we are saying our words are  variables. And as a resultwe have got an R dimensional representation.  So the U is an R dimensional representationof  documents.and our V is our R dimensional representationof words.  Right? These were our variables.Therefore, we got an R dimensional representation of words.in the other case we were treating each of these as our observations and treating words as our variablesso we got our dimensional representation of documents right soSo we get not just a representation of objects  in R-dimensional space as we get with principal componentanalysis with singular value decomposition appliedto a vectorization of documents where the matrix is represented  as shown here.We end up with an R-dimensional representation for words  and an R-dimensional representation of documents.And that is what is referred to as latent semantic analysis.  So let's look at what happens here.We have our X matrix, which is this one here.Documents as columns.and if we look at sorry let's yeah okay so we are saying x is equal to u s v transposex here was something by eight how many words were there 12 words12 times 8.  This now is going to be 12 times R  and becomes our word representation in R dimensional space.Our S here becomes R by Rand our V transpose will be R by 8.which means V itself is going to be 8 by R, right?  And there were 8 documents.And there were 12 words.So we have words as represented in R-dimensional spaceand we have documents represented in R-dimensional space.Now, just as we did in principal component analysis, these diagonal elements are representing the variance.But remember that the variance is the eigenvalues.This matrix here is the square root of the eigenvalues, right?And so these are referred to as singular values.And just as we had a descending order to the eigenvalues after we didprinciple component analysis, we have aThe more of the variance is being captured. But the variance itself being captured is the square of these.Right? So in this case, if I want to just visualize my words and documents, I will choose the two top  eigenvalues.As a result of choosing  this as a 2 by 2,  so I'm saying r is 2,  therefore I must look at  each word nowas being  two-dimensional  with these coordinates.  And every document,  which is a column,is being represented as a two-dimensional  representation.So now, of course, I can visualize those like this.  So each dot here is a word.And each dot here is a document.  And what we can see is there are these three documents that are close to each other.These are quite separated from these documents.  And then you have this one document that's sitting out here.which document is sitting right out here?  It is document number 5012345.I think I messed up here.  I've added a new document.  But essentially, if we look at what's happening here,not sure why I have nine documents there.  I'll have to look at it.maybe I by mistake incorporated this but if we look at these documents the last three documentswere supposed to be graph related and what we are saying is we have one two three fourat least these documents potentially are the documents that are also close to each otherand this one may be the one that is a little further away from we'll have to I'll have to  can come back to you or why I have nine problems there.Right?  But here we can see that these are all graph theory.These are all human computer interaction.And maybe this here is a little more on response time  rather than human-computer interaction.So that's why it is separated.We also see that some words here are coming out as similar to each other.These are words that have their second axis.Okay, so this word here and this word here are close to each other.  so what were those words  umgraph and my  trees and miners right  and so trees graphs and  miners these are definitely to do  with graph theory that'swhy they are appearing close to each  other out here on the other  hand if we look at  human and userwe would expect these to be close to each other  right a user of a computer  is, as of now at least, a human.  So that's the first and fourth.So first and second, third, fourth.  It's not coming up that close, right?But what we would expect is that the coordinates of these would come close.  Now, of course, take into account that these are,this is a very small corpus that we are using.  Look at this.  These two are very similar, exactly the same.So what are these? 1, 2, 3, 4, 5, 6, 7. 1, 2, 3, 4, 5, 6, 7. Response and time are identical in their representation, right?So we can see that something interesting is happening here.Now, when we get this 12 by 2 matrix here, multiplied by a 2 by 2 matrix, multiplied by a 2 by, in this case, 9, which should be 8 matrix.by multiplying just these red portions  we get a reconstruction of our X.So this is what our X looked like  where we can seethat clearly the vocabulary that was being used in these documentswere different from the vocabulary being used in these documents.Right?  But the sparseness here, the zeros here,made it look like maybe these weren't as similar to each other.But when we reconstruct x by taking u, s, v transpose,we are now getting not a sparse matrix but a dense matrix.And words that previously had the value 0 in here  now don't have a 0 value.Because even though they didn't mention the value,  the word system, they did mention computer.And computer and system are really synonyms  in the context of what we are talking about.And so we are now filling in these missing values with numbers other than zero.If there are other words in that document that really are suggesting that instead of using the word system, we could have used computer.right and we are seeing that a lot of this area now has larger positive values across the boardand similarly here we have larger positive values  across the board compared to what we have in this oneRight?  So this shashi tharoor versus my lecture problem,  to some extent, is being resolved  by latent semantic analysis.But critics of this method said, what  do you mean by a negative value?By looking at a positive value, we can say, yeah, OK.So, you know, this word has some importance in the second document.But when we have a negative value, what does that really mean?Does it mean that this word appearing takes away from the meaning of this document?Or what is that negative value representing?So this was the, what was represented as a rebuttal by the Bayesians.Who then went on to propose a probabilistic latent semantic analysis.where essentially you ended up with probabilities in here.  And so when you looked at the coordinates out here,where how do we interpret this?  These are called topic 1 and topic 2.And what was being said here is that word 1  has a membership of this topic of point 2.2.So this word here is much stronger in its connection to topic one compared to this word here.And this word here was much more related to topic two compared to this word.here. And we can see again, right, that these last three words are associated with topictwo. The first bunch of words are much more related to topic one. And this word here seemsto have an equal representation across those topics.  And what was that word?  Survey.Because survey appeared once here in a document that had more of this vocabulary and alsoappeared here once in a set of documents that actually had this vocabulary.right? So the jury is out or maybe this word transcends both topics and that is what isrepresented out here. Right? So you can see that what we are getting is a partitioning of thevocabulary out here. But the criticism was y minus. What does this minus represent?And you can see how if you instead had probabilities where word 1 was looking like this, word 2 and so on.And then the latter words, word 10, word 11, and word 12, word 9.Word 9 looked more like 4 and 0.6, whereas these were more 0.8, 0.9, 0.92.The interpretation is so much easier, right?  What we are saying is that if a topic, topic one,is what is generating a word,  or if we are seeing a word in a document,the likelihood is that topic one generated this word  much more than the likelihood of topic two generated.It's four times more likely that topic one would have generated this word.And so when we look across all the words that appear in a document,  we can then come up with a document representation,saying that the document has maybe 60% of its vocabulary from topic one  and 40% of its vocabulary from topic two.And that would become the document representation.Here again, what we can see is that these are related to topic 1 and these are relatedmore to topic 2.  Right?  So the partitioning is happening of documents.but what these actually mean especially the negative values became a question that was askedso let's take a break from the theory of it uh oh actually maybe not i was going to go intosome text but let me now do this next topic before i get on to some of the codeSo what we have seen is three methods for syntactic vectorization.And as of now, we have seen one method for semantic representation or semantic vectorization.Where each document now is represented in this two-dimensional topic space.Either way, what we now want to do is apply this to something.And when we started this topic, we had talked about text categorization.Where we have documents that have a particular class associated.  Is this a sports document? Is this a politics document? Is it a whatever?So now that we have been able to vectorize documents,  this essentially becomes a standard machine learning  problem.But before we even vectorize the documents,or maybe we could look at this as justifying why we are doing  what we are doing in the vectorization,especially the syntactic vectorization.We talked about the vector space model for documents.  Let's look at some probabilistic models for documents instead.So the idea here is we are talking about a...We are talking about a probabilistic model, therefore we must be talking about generative models.And what we are saying is that we have a corpus. A corpus consists of a number of documents.And each document is generated from a mixture model, parameterized by some parameter vector.Now, we've come across mixture models before when we were looking at the EM algorithm forclustering. And essentially, a mixture model is a probability distribution defined by a linearcombination of individual probability distribution, also known as components.  and when we were studying the EM algorithm,we had assumed that these components are what?They are Gaussian.  Right?  And each Gaussian had its own parametersand there was another parameter  which was the probability of a component being called upon to generate data.So similarly, we can say that a document DIis generated by a number of components.and each of those components has a probability ofbeing asked to generate a part of this document.  So when we think of a document,we are really saying that a document is a bag of words.why are we using the word bag we're saying it's a bag not a set because a bag can havemultiple copies of the same word a set card a set has only unique elementsright so what we are saying is that this document has  words in it and those words are generatedby calling upon these components.  So the question is,what kind of probability distribution are these components?And there are two models, probabilistic models of documents.  One is the multivariate Bern-Rowli,and the second is the multinomial distribution model.so we're going to look at both of thosethe multivariate per nauli model what does that tell you there are two really important wordsmultivariate what does multivariate  chair kitchenany idea what it is multivariate we say multivariate statistics alsono sir okay sumit any idea what multivariate stands for this wordmultiple variables. It's as simple as that. Multivariate means multiple variables.So, we are saying we have x1, x2, x3 till xn. What are these x1, 2, xn in our case? Becausewe're looking at documents, these are elements of our vocabulary.So they could be words, they could be key phrases.And of course, Bernoulli, we understand in that they either occur or don't occur.  right so this is just a binaryvariant  random variantand so the model of generating a document  which is multivariateis that we are saying that  a document you can think of a document consisting ofn slots one for every word or key phrase can you hear me hello my video seems to have got stuckhello can you hear meyes sir you're audible okay thank you so essentially we are saying this documenthas n slots, one slot for every word or keyphrase.  And what we want to decide is, does this appear or notappear in the column?  So this is the equivalent of our binary vectorization.What are the parameters of this model?For each element, we have a separate Barnaoli distribution.The probability of x1 being a 1 is going to be some probability theta 1.The probability of x2 equal to 1 is going to have a different probability.right? And so what we are saying here is that for each one of these words,we have its own parameter, theta n.So there are n parameters here that we need to learn.  But why are we saying components?think of components as topics.Or actually, let's not even confuse it with topics right now.  Think of each of these as classes.  We are talking aboutthe corpus as a whole,  which is a set of documents.being generated from a mixture model.  So whenever we are generating a document,we first decide which component are we going to call,  which class are we going to generate a document call.  And depending on the class,we will generate the words that appear.  And to generate the words that appear,  we need n parameters.So the total number of parameters that we need here  is if we have k classes and we have n parameters for each class,we have n times k parameters.  But each of the classes themselvesalso have a probability of being asked to be generated.And so we have the probability of C1 is equal to theta C1, probability of C2 is equal to theta C2till the probability of Ck which is equal to 1 minus the sum of probability of Ck minuscj minus 1 where j goes from 1 to k.Actually, 1 minus sigma of theta cj where j goes from 1 to k minus 1.So there are k minus 1 more parameters.  So the total number of parameters we would need is this.So that is the multivariate Bernoulli model.  What about the multinomial distribution?  The multinomial distributionis where we have a categorical variable.and so what we are doing here is we are saying for every element of the vocabulary we call them v1v2 till vm we have a probability of it being chosen as a word or key phrase in the documentSo we have a document.  We now decide on a length of the document.And so if we say the length is K,  let's not call it K because we've already,let's call it capital N.  We have capital N slots that need to be filled.Again, we choose the class to which the document belongs, and then we choose from the probability of words given the class that has been chosen.And what is that? That is this categorical distribution.and what we are saying now is we are choosing  from all of our vocabulary based on these probabilitieswe are choosing words at random  and filling our slots with those wordsso what are we going to end up withwe are going to have n selections from m possible values,which are each of the elements of a vocab.So what is the probability of a document going to be,given that it has been generated from a particular class.  When we see a document,it is of length n,  and there are m values here  from which we can choose these n.What we are going to end up with  is essentially this here.This capital N  is the number of trials  and these arethe occurrences.  the number of times each of our words are occurring.So what we had said in terms of our notation here  is we have n trials.We are going to say n divided by x1 factorial x2 factorialtill xm factorial.where each of these x1, x2, and xm are the frequency with which each of these elements appear in our ntrials that we have. So this is the number of ways in which n slots can be filled with these frequency ofm vocabulary words and we then multiply by the probabilities right so p1 to the power of x1times p2 to the power of x2 all the way till pm to the power of x3where p1 to pm are these probabilities right so that's what we are representing  Okay.So the multinomial distribution model of a document is different from a multivariateBernoulli model where we are saying that the words that appear in the document, they canhave a frequency associated with them. They fill up  N slots and therefore we have to choose the length of a documentalso. But once  we've chosen the length of the document, we are  sixSelecting what to fill these documents by a probability distribution.Okay, and that's the multinomial distribution model.Now, given these two models, we end up with, if we want to do a classification model, we end up withtwo versions of the naive Bayes algorithm.Remember that the naive Bayes algorithm  made a simplifying assumption.If we had n input variables and we had the class C and we wanted to know what's the probabilityof a class given a set of input values, evidence E.we can now represent it as the product of the probability of each xi given Cwhere i goes from 1 to n multiplied by the probability of C divided by the probability ofthis is what we had done.  This was the naive Bayes model.Now, why do we have two different versions  of the naive Bayes algorithm?  Depending on whether this is a binary vectoror whether this is a count vector.A count vector is really required for the multinomium document model.Why?Because we need to know the number of times, NIT is the number of times the t-th elementof the vocabulary appears in the document dI.And that is what is represented within the count vectorizer,  the number of times that word appears.And that's why we need this count vector  when we want to calculate the probability of a document  given a class.When we want to use instead the multivariate Bernoulli model,this dit is either 1 or 0, where 1 is when wt appears in di.and 0 when WT does not appear in DI.Right?  And so whenever we are using the multivariate Bernoulli model,the value here, how we calculate this, essentially changes.Out here in the multivariate Bernoulli,  we can see we are taking the product over the vocabularyand we are using the Bernoulli  distributions formula.Right?  P of x to the,  P of theta  to the power of,whatever I say here,  the P to the power of x times  1 minus p to the power of 1 minus x is the probability of this.Or we can even write this as theta.  We're doing the same thing.When we are wanting to use the multinomial distribution,this has to be calculated differently.  And how is it calculated differently?it's basically going to use this formula here,which is what we have as the formula for the multinomial distribution,but we are also multiplying by the probability of the length of the document.And this probability of the length of the document,  we need to figure out what is the distribution going to be for this.Right?  So that is what changes in the naive Bayes algorithmdepending on what kind of vectorizer we are using here.  When we use a binary vectorizerit works pretty much the same way as the standard naive Bayes.  when we choose a count vectorizer, this part changes.And we need to use instead this distribution,  which is the probability of bi given cj.OK, I know this has got a little heavy.  I'm aware of that.  So let's leave it here for now.And let me go to some code.and I'll be sharing this in your EducoLab as well.  So here, there are going to be twoIPython notebooks that I share.  One is the text vectorizer, and the other is toxic comment classification.here we are using in sklearn a standard data set which consists of news groups 20 news groupsso in the good old days people would have news groups where they would email into this newsgroup email address and then all of the messages associated with a particulartopic would get collated and would get shared with other people that were interested in that  topic. This was in the good old days when the internet was not as mature as it is today.We didn't have social media and so on. So essentially this data set consists of a numberof messages and we can see that the structure of the data set, we have essentially a dictionaryand one of the elements in the dictionary is data  and that has as its value a list.And in that list, you basically have a set of messages.  This is one message.And in that message, you have who sent it,  what was the subject and where did they post it and so on.  And then it has some text.So really what we want to do is we want to use this text.  We want to ignore these email addresses and all that kind of stuff.We want to really use this text to be able to automatically classify it into whether it is about the Christian religion or whether it's about hockey or the Middle East or motorcycles.So if you look at this, the labels, the class labels that we are hoping to use here are actually a taxonomy.Now, what is a taxonomy?  A taxonomy is a tree structure.so when yahoo was the main search engine for the internetthey had two ways in which you could navigate  and find content.One was they provided or created a taxonomy  for the internetwhere you had everything here.  I can't remember the exact word,  but this was basically the root node.And then you had differentchild nodes. Now in our case here, we have social, recreation,as two of the child nodes that I can see here. Within social, we have further splits,and I can see one of them is  religion  and then within religion we have  ChristianI presume we will also have other  notes here  like Islam  or maybe  HinduSikh and so on  right  and here in recreation  we have sportsand within sports we have hockey.  And again, we would expect it to have cricket  and various other things too.We also have talk, politics, right?  So out here, this would be talk,  this would be politics,and then we have Middle East.and again we would expect other subtopics as well right so the dot here this is just a wayof converting your taxonomy into a string where you use a dot as a separator to separate theseout and basically what yahoo did was they would have a bunch of documents hanging offnot just the leaf nodes, but you could also have documents hanging off here where they didn't knowquite exactly how to split it further into its subcategories. And so the idea here is that wewant to classify every one of these messages into a leaf node. And there are 20 such leaf nodes thatwe have provided. And here they are, right? So alt.atheism, computer.graphics,operating systems, and so on, right? So the first thing that we want to do here is get the data intoa shape that we are interested in. If we look at the data that we have been provided,the data consists of five different keys.  They are dictionaries, and their keys are data, file names,target names, target, and description.  We are only interested in actually the content and the target name.The target is just a one-to-one mapping of a string  onto an integer from what I can take up.right so we brought that in out here also we don't reallyand so the first thing we do typically like we said is we will lemmatize the data we willclean up some issues in the data like remove email addresses and stuff like thisremove new line characters, single quotes, right?  So punctuation.  So that's happening here.And then out here is where the lemmatization is happeningusing another library that you will now have to start to explore  called SPACY.And SPACY is one of the leading natural language processing libraries  that is there. So getting familiarity with that is going to be important.Now, when we lemmatize, the results of the lemmatizationdepend on another piece of information, which are called POS tags.Right? So these POS tags are assigned to individual words. POS stands for part of speech.and so if we look up part of speech you will findpart of speech. Essentially you are assigning tags to every wordtags like nouns adjectives verb adverb right and knowing the post tagImproves the quality of lemmatization that is done to it.Right? Because you wouldn't want to lemmatize somebody's name, for example, even though it looks like a prime candidate for doing so.So what we are doing here is we are loading a model, a pre-trained model in SPACEY.If we look at this, EN stands for English.  So this is a model specifically for the English language.  And SM stands for small.There is large models also, and medium-sized models  that you can pull up.And typically, when we load this model,  this model has all sorts of processing capabilities.and we are disabling named entity recognition  and entity parsing or sentence parsing in here.All we are interested in doing at this point  is tokenization and post tags identification.So when we run this,what is happening here is we are getting our data from our data frame here and we are choosingjust the values in the content column we are converting it into a list we are removing emailswe are removing new line characters removing certain punctuationwe are loading this model the nlp model and we areoh in fact before we load that nlp model we are executing on this here where we are removingpunctuation and converting each element in our list, which is one of these posts done to ourgroups, and converting it into a list of words.and we are then calling the lemmatization piece herethat is using the spacey modeland looking at only lemmatization of these cost types.right so it's taking each token and it's lemmatizing itand providing that in our texts out which is another listwhich is being stored in data lemmatized and that is being returned so when we call our  get lemmatized clean data,passing the data frame here,  what we end up getting backis this list of words that appear within that message  that has been lemmatized,punctuation's been removed,  but we can see we still have some issues here, right?  We have an apostrophe S out here.  So really we should be doing some further cleaning,but this is the basic cleaning that we are doing here.Now, at this point, I'm going to take the data lemmatized,  and that is really the training data that we have.And I'm passing it through.  In sklearn, we have the feature extraction.textCountVectorizer.And I'm saying I want to tokenize based on words.I only want words in my vocabulary that have a minimum of 10 documents that they appear inI want to use a list of stock words that is already embedded within this function this methodthey're the English stock words that I want to remove I want to make all of my words lowercaseand I only want to have in my vocabulary wordsthat have greater than or equal to three characters within it.And I do a fit transform on it,and I end up with this fit transform creating a vocabularywhere it's matched every unique token, every word onto your unique ID.And essentially what I'm seeing here is that the vector that we generate for every document,the 7083rd element in that vector is going to be representing the word thing and the frequency within itbecause I'm calling the count.So that's all of my vocabulary here, of which we've seen we have a few thousand words.We can see the vocabulary is actually 7,846.Now, when I give you this IPython notebook, you can play around with some of the parameters,reduce the minimum df, for example, from 10 to a lower value,  what that's going to do is increase the vocabulary.If you change the token pattern to be not three characters minimum,but two characters minimum, again, you're going to increase the size of the vocabulary.  Right.And we can see that we have in our data vectors that have been returned 11,314 vectors.And data vectorized is a sparse matrix.  Right?  So it's saying that this is a sparse matrix.The dimensionality of that sparse matrix is 11,314 by 7,846.So every row is a document and every word in our vocabulary is a column in here.And it says that out of all of these elements, there are just 729489 elements that have a non-zero value in it.So it's a sparse matrix.  Why are there so many zeros?  Because every document only contains a small subset of wordscompared to the vocabulary size.  Imagine we are looking at a vocabulary size of only 7,800 words right now.Imagine when that's a million words or two million words,  how sparse that matrix is going to be.  Right?And this matrix is in what's called compressed sparse row format.And what I would highly recommend is you look at scipy and the different formats in which sparse matrices are represented.And why one representation is better than another, depending on what kind of operations you want to do on it.So I realize we are out of time, so I'll stop here.  But we'll continue from here in the next lecture tomorrow.and I'll share this so that you can go through it in your own time just after this lecture.  All right.  Thanks for staying there.